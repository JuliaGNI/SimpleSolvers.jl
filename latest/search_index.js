var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"C. T. Kelley. Iterative methods for linear and nonlinear equations (SIAM, 1995).\n\n\n\nM. Bierlaire. Optimization: principles and algorithms (EPFL press, 2015).\n\n\n\nJ. Nocedal and S. J. Wright. Numerical optimization (Springer, New York, NY, 2006). Second Edition.\n\n\n\nM. J. Kochenderfer and T. A. Wheeler. Algorithms for optimization (Mit Press, 2019).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\n","category":"page"},{"location":"linesearch/bierlaire_quadratic/#Bierlaire-Quadratic-Line-Search","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic Line Search","text":"","category":"section"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"In [2] quadratic line search is defined as an interpolation between three points. For this consider","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"using SimpleSolvers\nusing SimpleSolvers: compute_jacobian!, factorize!, linearsolver, jacobian, cache, linesearch_objective, direction # hide\nusing LinearAlgebra: rmul!, ldiv! # hide\nusing Random # hide\nRandom.seed!(1234) # hide\n\nf(x::T) where {T<:Number} = exp(x) * (T(.5) * x ^ 3 - 5x ^ 2 + 2x) + 2one(T)\nf(x::AbstractArray{T}) where {T<:Number} = exp.(x) .* (T(.5) * (x .^ 3) - 5 * (x .^ 2) + 2x) .+ 2one(T)\nf!(y::AbstractVector{T}, x::AbstractVector{T}) where {T} = y .= f.(x)\nj!(j::AbstractMatrix{T}, x::AbstractVector{T}) where {T} = SimpleSolvers.ForwardDiff.jacobian!(j, f!, similar(x), x)\nx = -10 * rand(1)\nsolver = NewtonSolver(x, f.(x); F = f)\nupdate!(solver, x)\ncompute_jacobian!(solver, x, j!; mode = :function)\n\n# compute rhs\nf!(cache(solver).rhs, x)\nrmul!(cache(solver).rhs, -1)\n\n# multiply rhs with jacobian\nfactorize!(linearsolver(solver), jacobian(solver))\nldiv!(direction(cache(solver)), linearsolver(solver), cache(solver).rhs)\n\nnls = NonlinearSystem(f, x)\nls_obj = linesearch_objective(nls, cache(solver))\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"For the Bierlaire quadratic line search we need three points: a, b and c:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"a, b, c = -2., 0.5, 2.5\nnothing # hide","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"In the figure above we already plotted three points a, b and c on whose basis a second-order polynomial will be built that should approximate f^mathrmls.[1] The polynomial is built with the ansatz:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"[1]: These points further need to satisfy f^mathrmls(a)  f^mathrmls(b)  f^mathrmls(c).","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"p(alpha) = beta_1(alpha - a)(x - b) + beta_2(alpha - a) + beta_3(alpha - b)","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"and by identifying ","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"beginaligned\np(a)  = f^mathrmls(a) \np(b)  = f^mathrmls(b) \np(c)  = f^mathrmls(c) \nendaligned","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"we get","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"beginaligned\nbeta_1  = frac(b - c)f^mathrmls(a) + (c - a)f^mathrmls(b) + (a - b)f^mathrmls(c)(a - b)(c - a)(c - b)  \nbeta_2  = fracf^mathrmls(b)b - a \nbeta_3  = fracf^mathrmls(a)a - b\nendaligned","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We can plot this polynomial:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We can now easily determine the minimum of the polynomial p. It is:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"chi = frac12 frac f^mathrmls(a) (b^2 - c^2) + f^mathrmls(b) (c^2 - a^2) + f^mathrmls(c) (a^2 - b^2) f^mathrmls(a) (b - c) + f^mathrmls(b) (c - a) + f^mathrmls(c) (a - b)","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We now use this chi to either replace a, b or c and distinguish between the following four scenarios:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"chi  b and f^mathrmls(chi)  f^mathrmls(b) implies we replace c gets chi,\nchi  b and f^mathrmls(chi) leq f^mathrmls(b) implies we replace a b gets b chi,\nchi leq b and f^mathrmls(chi)  f^mathrmls(b) implies we replace a gets chi,\nchi leq b and f^mathrmls(chi) leq f^mathrmls(b) implies we replace b c gets chi b.","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"In our example we have the second case: chi is to the right of b and f^mathrmls(chi) is smaller than f(b). We therefore replace a with b and b with chi. The new approximation is the following one:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We again observe the second case. By replacing a b gets b chi we get:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We now observe the first case: chi is to the left of b and f^mathrmls(chi) is above f(b). Hence we replace b c gets chi b A successive iteration yields:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"info: Info\nAfter having computed chi we further either shift it to the left or right depending on whether (c - b) or (b - a) is bigger respectively. The shift is made by either adding or subtracting the constant varepsilon.","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"Also see SimpleSolvers.DEFAULT_BIERLAIRE_ε.","category":"page"},{"location":"linesearch/curvature_condition/#The-Curvature-Condition","page":"The Curvature Condition","title":"The Curvature Condition","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"The curvature condition is used together with the sufficient decrease condition and ensures that step sizes are not chosen too small (which might happen if we only use the sufficient decrease condition). The sufficient decrease condition and the curvature condition together are called the Wolfe conditions.","category":"page"},{"location":"linesearch/curvature_condition/#Standard-Curvature-Condition","page":"The Curvature Condition","title":"Standard Curvature Condition","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"For the standard curvature condition (see SimpleSolvers.CurvatureCondition) we have:","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"    fracpartialpartialalphaBigg_alpha=alpha_kf(R_x_k(alphap_k)) = g(mathrmgrad_R_x_k(alpha_kp_k)f p_k) geq c_2g(mathrmgrad_x_kf p_k) = c_2fracpartialpartialalphaBigg_alpha=0f(R_x_k(alphap_k))","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"for c_2in(c_1 1) In words this means that the derivative with respect to alpha_k should be bigger at the new iterate x_k+1 than at the old iterate x_k. ","category":"page"},{"location":"linesearch/curvature_condition/#Strong-Curvature-Condition","page":"The Curvature Condition","title":"Strong Curvature Condition","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"For the strong curvature condition[1] we replace the curvature condition by:","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"[1]: We consequently also speak of the strong Wolfe conditions when taking the strong curvature condition and the sufficient decrease condition together.","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"    g(mathrmgrad_R_x_k(alpha_kp_k)f p_k)  c_2g(mathrmgrad_x_kf p_k)","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"Note the sign change here. This is because the term g(mathrmgrad_x_kf p_k) is negative if p_k is a search direction. Both the standard curvature condition and the strong curvature condition are implemented under SimpleSolvers.CurvatureCondition.","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"info: Info\nIn order to use the corresponding condition you have to either pass mode = :Standard or mode = :Strong to the constructor of CurvatureCondition.","category":"page"},{"location":"linesearch/curvature_condition/#Example","page":"The Curvature Condition","title":"Example","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"We use the same example that we had when we explained the sufficient decrease condition:","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"using SimpleSolvers # hide\nusing SimpleSolvers: CurvatureCondition, NewtonOptimizerCache, update!, gradient!, linesearch_objective, ldiv! # hide\n\nx = [3., 1.3]\nf = x -> 10 * sum(x .^ 3 / 6 - x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\nhes = Hessian(obj, x; mode = :autodiff)\nupdate!(hes, x)\n\nc₂ = .9\ng = gradient(obj, x)\nrhs = -g\n# the search direction is determined by multiplying the right hand side with the inverse of the Hessian from the left.\np = similar(rhs)\nldiv!(p, hes, rhs)\ncc = CurvatureCondition(c₂, x, g, p, obj, obj.G)\n\n# check different values\nα₁, α₂, α₃, α₄, α₅ = .09, .4, 0.7, 1., 1.3\n(cc(α₁), cc(α₂), cc(α₃), cc(α₄), cc(α₅))","category":"page"},{"location":"jacobians/#Jacobians","page":"Jacobians","title":"Jacobians","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"The supertype Jacobian comprises different ways of taking Jacobians:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"JacobianFunction,\nJacobianAutodiff,\nJacobianFiniteDifferences.","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"We first start by showing JacobianAutodiff:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"using SimpleSolvers, Random; using SimpleSolvers: JacobianAutodiff, Jacobian, JacobianFunction, JacobianFiniteDifferences; Random.seed!(123) # hide\n# the input and output dimensions of this function are the same\nF(y::AbstractArray, x::AbstractArray) = y .= tanh.(x)\ndim = 3\nx = rand(dim)\njac = JacobianAutodiff{eltype(x)}(F, dim)","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"Instead of calling JacobianAutodiff(f, x) we can equivalently do:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"jac = Jacobian{eltype(x)}(F, dim; mode = :autodiff)","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"When calling an instance of Jacobian we can use the function [compute_jacobian!]:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"j = zeros(dim, dim)\ncompute_jacobian!(j, x, jac)","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"This is equivalent to calling:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"jac(j, x)","category":"page"},{"location":"linear/linear_solvers/#Linear-Solvers","page":"Linear Solvers","title":"Linear Solvers","text":"","category":"section"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Objects of type LinearSolver are used to solve LinearSystems, i.e. we want to find x for given A and y such that","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"    Ax = y","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"is satisfied. ","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"A linear system can be called with[1]:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"[1]: Here we also have to update the LinearSystem by calling update!. For more information see the page on the initialize! function.","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"using SimpleSolvers\n\nA = [(0. + 1e-6) 1. 2.; 3. 4. 5.; 6. 7. 8.]\ny = [1., 2., 3.]\nls = LinearSystem(A, y)\nupdate!(ls, A, y)\nnothing # hide","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Note that we here use the matrix:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"A = beginpmatrix 0 + varepsilon  1  2  3  4  5  6  7  8 endpmatrix","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"This matrix would be singular if we had varepsilon = 0 because 2cdotbeginpmatrix 3  4  5 endpmatrix - beginpmatrix 6  7  8 endpmatrix = beginpmatrix 0  1  2 endpmatrix So by choosing varepsilon = 10^-6 the matrix is ill-conditioned.","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We first solve LinearSystem with an lu solver (using LU and solve) in double precision and without pivoting:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"lu = LU(; pivot = false)\ny¹ = solve(lu, ls)","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We check the result:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"A * y¹","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We now do the same in single precision:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Aˢ = Float32.(A)\nyˢ = Float32.(y)\nlsˢ = LinearSystem(Aˢ, yˢ)\nupdate!(lsˢ, Aˢ, yˢ)\ny² = solve(lu, lsˢ)","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"and again check the result:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Aˢ * y²","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"As we can see the computation of the factorization returns a wrong solution. If we use pivoting however, the problem can also be solved with single precision:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"lu = LU(; pivot = true)\ny³ = solve(lu, lsˢ)","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Aˢ * y³","category":"page"},{"location":"linear/linear_solvers/#Solving-the-System-with-Built-In-Functionality-from-the-LinearAlgebra-Package","page":"Linear Solvers","title":"Solving the System with Built-In Functionality from the LinearAlgebra Package","text":"","category":"section"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We further try to solve the system with the inv operator from the LinearAlgebra package. First in double precision:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"inv(A) * y","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"And also in single precision","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"inv(Aˢ) * yˢ","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"In single precision the result is completely wrong as can also be seen by computing:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"inv(Aˢ) * Aˢ","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"If we however write:","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Aˢ \\ yˢ","category":"page"},{"location":"linear/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"we again obtain a correct-looking result, as LinearAlgebra.\\ uses an algorithm very similar to factorize! in SimpleSolvers.","category":"page"},{"location":"linesearch/static/#Static-Line-Search","page":"Static","title":"Static Line Search","text":"","category":"section"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"Static line search is the simplest form of line search in which the guess for alpha is always just a fixed value. In the following we demonstrate how to use this line search.","category":"page"},{"location":"linesearch/static/#static_example","page":"Static","title":"Example","text":"","category":"section"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"We show how to use linesearches in SimpleSolvers to solve a simple toy problem:","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"using SimpleSolvers # hide\n\nx = [1., 0., 0.]\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\n\nα = .1\nsl = Static(α)\nnothing # hide","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"SimpleSolvers contains a function SimpleSolvers.linesearch_objective that allocates a SimpleSolvers.TemporaryUnivariateObjective that only depends on alpha:","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"using SimpleSolvers: linesearch_objective, NewtonOptimizerCache, LinesearchState, update! # hide\ncache = NewtonOptimizerCache(x)\n\nupdate!(cache, x)\nx₂ = [.9, 0., 0.]\nupdate!(cache, x₂)\nvalue!(obj, x₂)\ngradient!(obj, x₂)\nls_obj = linesearch_objective(obj, cache)\nnothing # hide","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"We now use this to compute a static line search[1]:","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"[1]: We also note the use of the SimpleSolvers.LinesearchState constructor here, which has to be used together with a SimpleSolvers.LinesearchMethod.","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"ls = LinesearchState(sl)\nls(ls_obj, α)","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"info: Info\nWe note that for the static line search we always just return alpha.","category":"page"},{"location":"optimizers/optimizers/#Optimizers","page":"Optimizers","title":"Optimizers","text":"","category":"section"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"An Optimizer stores an OptimizationAlgorithm, a MultivariateObjective, the SimpleSolvers.OptimizerResult and a SimpleSolvers.NonlinearMethod. Its purposes are:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers\nusing LinearAlgebra: norm\nimport Random # hide\nRandom.seed!(123) # hide\n\nx = rand(3)\nobj = MultivariateObjective(x -> sum((x - [0., 0., 1.]) .^ 2), x)\nbt = Backtracking()\nalg = Newton()\nopt = Optimizer(x, obj; algorithm = alg, linesearch = bt)","category":"page"},{"location":"optimizers/optimizers/#Optimizer-Constructor","page":"Optimizers","title":"Optimizer Constructor","text":"","category":"section"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Internally the constructor for Optimizer calls SimpleSolvers.OptimizerResult and SimpleSolvers.NewtonOptimizerState and Hessian. We can also allocate these objects manually and then call a different constructor for Optimizer:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers: NewtonOptimizerState, OptimizerResult, initialize!\n\nresult = OptimizerResult(x, value!(obj, x))\ninitialize!(result, x)\nstate = NewtonOptimizerState(x; linesearch = bt)\nhes = Hessian(alg, obj, x)\nopt₂ = Optimizer(alg, obj, hes, result, state)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"If we want to solve the problem, we can call solve! on the Optimizer instance:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"x₀ = copy(x)\n\nsolve!(opt, x₀)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Internally SimpleSolvers.solve! repeatedly calls SimpleSolvers.solver_step! until SimpleSolvers.meets_stopping_criteria is satisfied.","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers: solver_step!\n\nx = rand(3)\nsolver_step!(opt, x)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"The function SimpleSolvers.solver_step! in turn does the following:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"# update objective, hessian, state and result\nupdate!(opt, x)\n# solve H δx = - ∇f\nldiv!(direction(opt), hessian(opt), rhs(opt))\n# apply line search\nα = linesearch(state(opt))(linesearch_objective(objective(opt), cache(opt)))\n# compute new minimizer\nx .= compute_new_iterate(x, α, direction(opt))","category":"page"},{"location":"optimizers/optimizers/#Solving-the-Line-Search-Problem-with-Backtracking","page":"Optimizers","title":"Solving the Line Search Problem with Backtracking","text":"","category":"section"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Calling an instance of SimpleSolvers.LinesearchState (in this case SimpleSolvers.BacktrackingState) on an SimpleSolvers.AbstractUnivariateObjective in turn does:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"α *= ls.p","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"as long as the SimpleSolvers.SufficientDecreaseCondition isn't satisfied. This condition checks the following:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"fₖ₊₁ ≤ sdc.fₖ + sdc.c₁ * αₖ * sdc.pₖ' * sdc.gradₖ","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"sdc is first allocated as:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers: SufficientDecreaseCondition, linesearch, linesearch_objective, objective, cache # hide\nls = linesearch(opt)\nα = ls.α₀\nx₀ = zero(α)\nlso = linesearch_objective(objective(opt), cache(opt))\ny₀ = value!(lso, x₀)\nd₀ = derivative!(lso, x₀)\n\nsdc = SufficientDecreaseCondition(ls.ϵ, x₀, y₀, d₀, d₀, obj)","category":"page"},{"location":"update/#Updates","page":"Updates","title":"Updates","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"One of the most central objects in SimpleSolvers are update! routines. They can be used together with many different types and structs:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"SimpleSolvers.update!(::Hessian, ::AbstractVector): this routine exists for most Hessians, i.e. for HessianFunction, HessianAutodiff, HessianBFGS and HessianDFP,\nSimpleSolvers.update!(::SimpleSolvers.NewtonSolverCache, ::AbstractVector),\nSimpleSolvers.update!(::SimpleSolvers.NonlinearSolverStatus, ::AbstractVector, ::Base.Callable),\nSimpleSolvers.update!(::SimpleSolvers.NewtonOptimizerCache, ::AbstractVector, ::AbstractVector, ::Hessian),\nSimpleSolvers.update!(::SimpleSolvers.NewtonOptimizerState, ::AbstractVector).\nSimpleSolvers.update!(::SimpleSolvers.OptimizerResult, ::AbstractVector, ::AbstractVector, ::AbstractVector).","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"So update! always takes an object that has to be updated and a single vector in the simplest case. For some methods more arguments need to be provided. ","category":"page"},{"location":"update/#Examples","page":"Updates","title":"Examples","text":"","category":"section"},{"location":"update/#Hessian","page":"Updates","title":"Hessian","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"If we look at the case of the Hessian, we store a matrix H that has to be updated in every iteration. We first initialize the matrix[1]:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"[1]: The constructor uses the function SimpleSolvers.initialize!.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers # hide\nusing LinearAlgebra: norm # hide\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nx = [1., 0., 0.]\nhes = Hessian(f, x; mode = :autodiff)\nhes.H","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"And then update:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"update!(hes, x)\n\nhes.H","category":"page"},{"location":"update/#NewtonOptimizerCache","page":"Updates","title":"NewtonOptimizerCache","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"In order to update an instance of SimpleSolvers.NewtonOptimizerCache we have to supply a value of the Gradient and the Hessian in addition to x:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers: initialize!, NewtonOptimizerCache # hide\ngrad = Gradient(f, x; mode = :autodiff)\ncache = NewtonOptimizerCache(x)\nupdate!(cache, x, grad, hes)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"info: Info\nWe note that when calling update! on the NewtonOptimizerCache, the Hessian hes is not automatically updated! This has to be done manually.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"info: Info\nCalling update! on the NewtonOptimizerCache updates everything except x as this in general requires another line search!","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"In order that we do not have to update the Hessian and the SimpleSolvers.NewtonOptimizerCache separately we can use SimpleSolvers.NewtonOptimizerState:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers: NewtonOptimizerState # hide\nobj = MultivariateObjective(f, x)\nstate = NewtonOptimizerState(x)\nupdate!(state, x, Gradient(obj), hes)","category":"page"},{"location":"update/#OptimizerResult","page":"Updates","title":"OptimizerResult","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"We also show how to update an instance of SimpleSolvers.OptimizerResult:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers: OptimizerResult # hide\n\nresult = OptimizerResult(x, obj)\n\nupdate!(result, x, obj, grad)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"Note that the residuals are still NaNs here. In order to get proper values for these we have to perform two updating steps:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"x₂ = [.9, 0., 0.]\nupdate!(result, x₂, obj, grad)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"warn: Warn\nNewtonOptimizerCache, OptimizerResult and NewtonOptimizerState (through MultivariateObjective) all store things that are somewhat similar, for example x. This may make it somewhat difficult to keep track of all the things that happen during optimization.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"An Optimizer stores a MultivariateObjective, an SimpleSolvers.OptimizerResult and an OptimizationAlgorithm (and therefore the MultivariateObjective again). We also give an example:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"opt = Optimizer(x, obj)\n\nupdate!(opt, x)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"Equivalent to calling update! on SimpleSolvers.OptimizerResult, the diagnostics cannot be computed with only one iterations; we have to compute a second one:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"x₂ = [.9, 0., 0.]\nupdate!(opt, x₂)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"We note that simply calling update! on an instance of SimpleSolvers.Optimizer is not enough to perform a complete iteration since the computation of a new x requires a line search procedure in general.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"We also note that update! always returns the first input argument.","category":"page"},{"location":"linesearch/bisections/#Bisections","page":"Bisections","title":"Bisections","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"Bisections work by moving an interval until we observe one in which the sign of the derivative of the function changes. ","category":"page"},{"location":"linesearch/bisections/#Example","page":"Bisections","title":"Example","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"We consider the same example as we had when demonstrating backtracking line search:","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"ls_obj = linesearch_objective(obj, cache)\nnothing # hide","category":"page"},{"location":"linesearch/bisections/#Bracketing","page":"Bisections","title":"Bracketing","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"For bracketing [4] we move an interval successively and simultaneously increase it in the hope that we observe a local minimum (see bracket_minimum).","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"α₀ = 0.0\n(a, c) = bracket_minimum(Function(ls_obj), α₀)","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"(Image: )","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"We then use this interval to start the bisection algorithm.","category":"page"},{"location":"linesearch/bisections/#Potential-Problem-with-Backtracking","page":"Bisections","title":"Potential Problem with Backtracking","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"We here illustrate a potential issue with backtracking. For this consider the following function:","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"using SimpleSolvers: bracket_root\nf2(α::T) where {T <: Number} = α^2 - one(T)\nα₀ = -3.0\n(a, c) = bracket_root(f2, α₀)","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"And when we plot this we find:","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"(Image: )","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"And we see that the interval now contains two roots, r_1 and r_2.","category":"page"},{"location":"linesearch/backtracking/#Backtracking-Line-Search","page":"Backtracking","title":"Backtracking Line Search","text":"","category":"section"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"A backtracking line search method determines the amount to move in a given search direction by iteratively decreasing a step size alpha until an acceptable level is reached. In SimpleSolvers we can use the sufficient decrease condition and the curvature condition to quantify this acceptable level. The sufficient decrease condition is also referred to as the Armijo condition and the sufficient decrease condition and the curvature condition are referred to as the Wolfe conditions[1] [3]. ","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[1]: If we use the strong curvature condition instead of the standard curvature condition we conversely also say that we use the strong Wolfe conditions.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"info: Info\nWe note that for the static line search we always just return alpha.","category":"page"},{"location":"linesearch/backtracking/#Backtracking-Line-Search-for-a-Line-Search-Objective","page":"Backtracking","title":"Backtracking Line Search for a Line Search Objective","text":"","category":"section"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We note that when performing backtracking on a line search objective care needs to be taken. This is because we need to find equivalent quantities for mathrmgrad_x_kf and p. We first look at the derivative of the line search objective:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"fracddalphaf^mathrmls(alpha) = fracddalphaf(mathcalR_x_k(alphap)) = langle d_mathcalR_x_k(alphap)f alphap rangle","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"because the tangent map of a retraction is the identity at zero [5], i.e. T_0_xmathcalR = mathrmid_T_xmathcalM. In the equation above d_mathcalR_x_k(alphap)finT^*mathcalM indicates the exterior derivative of f evaluated at mathcalR_x_k(alphap) and langle cdot cdot rangle T^*mathcalMtimesTmathcalMtomathbbR is the natural pairing between tangent and cotangent space[2] [6].","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[2]: If we are not dealing with general Riemannian manifolds but only vector spaces then d_mathcalR_x_k(alphap)f simply becomes nabla_mathcalR_x_k(alphap)f and we further have langle A Brangle = A^T B.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We again look at the example introduced when talking about the sufficient decrease condition and cast it in the form of a line search objective:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"ls_obj = linesearch_objective(obj, cache)\nnothing # hide","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"This objective only depends on the parameter alpha. We plot it:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"(Image: )","category":"page"},{"location":"linesearch/backtracking/#sdc_example","page":"Backtracking","title":"Example","text":"","category":"section"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We show how to use line searches in SimpleSolvers to solve a simple toy problem[3]:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[3]: Also compare this to the case of the static line search.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"using SimpleSolvers # hide\n\nsl = Backtracking()\nnothing # hide","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"SimpleSolvers contains a function SimpleSolvers.linesearch_objective that allocates a SimpleSolvers.TemporaryUnivariateObjective that only depends on alpha:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We now use this to compute a backtracking line search[4]:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[4]: We also note the use of the SimpleSolvers.LinesearchState constructor here, which has to be used together with a SimpleSolvers.LinesearchMethod.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"ls = LinesearchState(sl)\nα = 50.\nαₜ = ls(ls_obj, α)","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"And we check whether the SimpleSolvers.SufficientDecreaseCondition is satisfied:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"sdc = SufficientDecreaseCondition(c₁, x, f(x), g, p, obj)\nsdc(αₜ)","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"Similarly for the SimpleSolvers.CurvatureCondition:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"using SimpleSolvers: CurvatureCondition # hide\nc₂ = .9\ncc = CurvatureCondition(c₂, x, g, p, obj, obj.G)\ncc(αₜ)","category":"page"},{"location":"linesearch/quadratic/#Quadratic-Line-Search","page":"Quadratic","title":"Quadratic Line Search","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"Quadratic line search is based on making a quadratic approximation to an objective and then pick the minimum of this quadratic approximation as the next iteration of alpha.","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"The quadratic polynomial is built the following way[1]:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"[1]: This is different from the Bierlaire quadratic polynomial described in [2].","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p(alpha) = f^mathrmls(0) + (f^mathrmls)(0)alpha + p_2alpha^2","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"and we also call p_0=f^mathrmls(0) and p_1=(f^mathrmls)(0). The coefficient p_2 is then determined the following way:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"take a value alpha (typically initialized as SimpleSolvers.DEFAULT_ARMIJO_α₀) and compute y = f^mathrmls(alpha),\nset p_2 gets frac(y^2 - p_0 - p_1alpha)alpha^2","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"After the polynomial is found we then take its minimum (analogously to the Bierlaire quadratic line search) and check if it satisfies the sufficient decrease condition. If it does not satisfy this condition we repeat the process, but with the current alph as the starting point for the line search (instead of the initial SimpleSolvers.DEFAULT_ARMIJO_α₀).","category":"page"},{"location":"linesearch/quadratic/#Example","page":"Quadratic","title":"Example","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"Here we treat the following problem:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"f(x::Union{T, Vector{T}}) where {T<:Number} = exp.(x) .* (x .^ 3 - 5x + 2x) .+ 2one(T)\nf!(y::AbstractVector{T}, x::AbstractVector{T}) where {T} = y .= f.(x)\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now want to use quadratic line search to find the root of this function starting at x = 0. We compute the Jacobian of f and initialize a line search objective:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers\nusing SimpleSolvers: compute_jacobian!, factorize!, update!, linearsolver, jacobian, cache, linesearch_objective, direction, determine_initial_α # hide\nusing LinearAlgebra: rmul!, ldiv! # hide\nusing Random # hide\nRandom.seed!(123) # hide\n\nj!(j::AbstractMatrix{T}, x::AbstractVector{T}) where {T} = SimpleSolvers.ForwardDiff.jacobian!(j, f, x)\nx = [0.]\n# allocate solver\nsolver = NewtonSolver(x, f(x); F = f)\n# initialize solver\nupdate!(solver, x)\ncompute_jacobian!(solver, x, j!; mode = :function)\n\n# compute rhs\nf!(cache(solver).rhs, x)\nrmul!(cache(solver).rhs, -1)\n\n# multiply rhs with jacobian\nfactorize!(linearsolver(solver), jacobian(solver))\nldiv!(direction(cache(solver)), linearsolver(solver), cache(solver).rhs)\nnls = NonlinearSystem(f, x)\nls_obj = linesearch_objective(nls, cache(solver))\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"The first two coefficient of the polynomial p (i.e. p_1 and p_2) are easy to compute:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/#Initializing-\\alpha","page":"Quadratic","title":"Initializing alpha","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"In order to compute p_2 we first have to initialize alpha. We start by guessing an initial alpha as SimpleSolvers.DEFAULT_ARMIJO_α₀. If this initial alpha does not satisfy the SimpleSolvers.BracketMinimumCriterion, i.e. it holds that f^mathrmls(alpha_0)  f^mathrmls(0), we call SimpleSolvers.bracket_minimum_with_fixed_point (similarly to calling SimpleSolvers.bracket_minimum for standard bracketing). ","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"Looking at SimpleSolvers.DEFAULT_ARMIJO_α₀, we see that the SimpleSolvers.BracketMinimumCriterion is not satisfied:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We therefore see that calling SimpleSolvers.determine_initial_α returns a different alpha (the result of calling SimpleSolvers.bracket_minimum_with_fixed_point):","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"α₀ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We can now finally compute p_2 and determine the minimum of the polynomial:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"y = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"When using SimpleSolvers.QuadraticState we in addition call SimpleSolvers.adjust_α:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: adjust_α # hide\nα₁ = adjust_α(αₜ, α₀)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now check wether alpha_1 satisfies the sufficient decrease condition:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: DEFAULT_WOLFE_c₁, SufficientDecreaseCondition # hide\nsdc = SufficientDecreaseCondition(DEFAULT_WOLFE_c₁, 0., fˡˢ(0.), derivative(ls_obj, 0.), 1., ls_obj)\n@assert sdc(α₁) # hide\nsdc(α₁)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now move the original x in the Newton direction with step length alpha_1 by using SimpleSolvers.compute_new_iterate:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: compute_new_iterate # hide\nx .= compute_new_iterate(x, α₁, direction(cache(solver)))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"And we see that we already very close to the root.","category":"page"},{"location":"linesearch/quadratic/#Example-for-Optimization","page":"Quadratic","title":"Example for Optimization","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We look again at the same example as before, but this time we want to find a minimum and not a root. We hence use SimpleSolvers.linesearch_objective not for a NewtonSolver, but for an Optimizer:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: NewtonOptimizerCache, initialize!, gradient\n\nx₀, x₁ = [0.], x\nobj = MultivariateObjective(sum∘f, x₀)\ngradient!(obj, x₀)\nvalue!(obj, x₀)\n_cache = NewtonOptimizerCache(x₀)\nhess = Hessian(obj, x₀; mode = :autodiff)\nupdate!(hess, x₀)\nupdate!(_cache, x₀, gradient(obj), hess)\ngradient!(obj, x₁)\nvalue!(obj, x₁)\nupdate!(hess, x₁)\nupdate!(_cache, x₁, gradient(obj), hess)\nls_obj = linesearch_objective(obj, _cache)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"info: Info\nNote the different shape of the line search objective in the case of the optimizer, especially that the line search objective can take negative values in this case!","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now again want to find the minimum with quadratic line search and repeat the procedure above:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₀ = fˡˢ(0.)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₁ = ∂fˡˢ∂α(0.)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"α₀ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)\nα₁ = adjust_α(αₜ, α₀)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"What we see here is that we do not use alpha_t = -p_1  (2p_2) as SimpleSolvers.adjust_α instead picks the left point in the interval sigma_0alpha_0 sigma_1alpha_0 as the change computed with alpha_t would be too small.","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now again move the original x in the Newton direction with step length alpha_1:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"x .= compute_new_iterate(x, α₁, direction(_cache))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We make another iteration:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"gradient!(obj, x)\nvalue!(obj, x)\nupdate!(hess, x)\nupdate!(_cache, x, gradient(obj), hess)\nls_obj = linesearch_objective(obj, _cache)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\np₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nα₀⁽²⁾ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀⁽²⁾) / α₀⁽²⁾^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"α₂ = adjust_α(αₜ, α₀⁽²⁾)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We see that for alpha_2 (as opposed to alpha_1) we have alpha_2 = alpha_t as alpha_t is in (this is what SimpleSolvers.adjust_α checks for):","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: DEFAULT_ARMIJO_σ₀, DEFAULT_ARMIJO_σ₁ # hide\n(DEFAULT_ARMIJO_σ₀ * α₀⁽²⁾, DEFAULT_ARMIJO_σ₁ * α₀⁽²⁾)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"x .= compute_new_iterate(x, α₂, direction(_cache))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We finally compute a third iterate:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"gradient!(obj, x)\nvalue!(obj, x)\nupdate!(hess, x)\nupdate!(_cache, x, gradient(obj), hess)\nls_obj = linesearch_objective(obj, _cache)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\np₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nα₀⁽³⁾ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀⁽³⁾) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)\nα₃ = adjust_α(αₜ, α₀⁽³⁾)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"x .= compute_new_iterate(x, α₃, direction(_cache))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/#Example-II","page":"Quadratic","title":"Example II","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"Here we consider the same example as when discussing the Bierlaire quadratic line search.","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"ls_obj = linesearch_objective(nls, cache(solver))\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now try to find a minimum of f^mathrmls with quadratic line search. For this we first need to find a bracket; we again do this with SimpleSolvers.bracket_minimum_with_fixed_point[2]:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"[2]: Here we use SimpleSolvers.bracket_minimum_with_fixed_point directly instead of using SimpleSolvers.determine_initial_α.","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(a, b) = SimpleSolvers.bracket_minimum_with_fixed_point(fˡˢ, 0.)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We plot the bracket:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using CairoMakie\nmred = RGBf(214 / 256, 39 / 256, 40 / 256)\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256)\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256)\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256)\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256)\n\nfig = Figure()\nax = Axis(fig[1, 1])\nalpha = -2.5:.01:3.\nlines!(ax, alpha, fˡˢ.(alpha); label = L\"f^\\mathrm{ls}(\\alpha)\")\nscatter!(ax, a, fˡˢ(a); color = mred, label = L\"a\")\nscatter!(ax, b, fˡˢ(b); color = mpurple, label = L\"b\")\nylims!(ax, (-1., 6.))\naxislegend(ax)\nsave(\"f_ls_1.png\", fig)\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now build the polynomial:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₀ = fˡˢ(a)\np₁ = ∂fˡˢ∂α(a)\ny = fˡˢ(b)\np₂ = (y - p₀ - p₁*b) / b^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"and compute its minimum:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"αₜ = -p₁ / (2p₂)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"lines!(ax, alpha, p.(alpha); label = L\"p(\\alpha)\")\nscatter!(ax, αₜ, p(αₜ); label = L\"\\alpha_t\")\nylims!(ax, (-1., 6.))\naxislegend(ax)\nsave(\"f_ls_2.png\", fig)\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now set a gets alpha_t and perform another iteration:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(a, b) = SimpleSolvers.bracket_minimum_with_fixed_point(fˡˢ, αₜ)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We again build the polynomial:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₀ = fˡˢ(a)\np₁ = ∂fˡˢ∂α(a)\ny = fˡˢ(b)\np₂ = (y - p₀ - p₁*(b-a)) / (b-a)^2\np(α) = p₀ + p₁ * (α-a) + p₂ * (α-a)^2\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"and compute its minimum:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"αₜ = -p₁ / (2p₂) + a","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/sufficient_decrease_condition/#The-Sufficient-Decrease-Condition","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"","category":"section"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"The Armijo condition or sufficient decrease condition states:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"    f(R_x_k(alpha_kp_k)) leq f(x_k) + c_1g_x_k(alpha_kp_k mathrmgrad^g_x_kf)  ","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"for some constant c_1in(0 1) (see SimpleSolvers.DEFAULT_WOLFE_c₁).","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"The sufficient decrease condition can also be written as ","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"    fracf(R_x_k(alpha_kp_k)) - f(x_k)alpha_k leq g_x_k(c_1p_k mathrmgrad^g_x_kf)","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"As we assume that f(R_x_k(alpha_kp_k)) leq f(x_k) and g_x_k(c_1p_k mathrmgrad^g_x_kf)  0, we can rewrite this as:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"    fracf(R_x_k(alpha_kp_k)) - f(x_k)alpha_k geq g_x_k(c_1p_k mathrmgrad^g_x_kf)","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"making clear why this is called the sufficient decrease condition. The parameter c_1 is typically chosen very small, around 10^-4. This is implemented as SimpleSolvers.SufficientDecreaseCondition.","category":"page"},{"location":"linesearch/sufficient_decrease_condition/#sdc_example_full","page":"The Sufficient Decrease Condition","title":"Example","text":"","category":"section"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"We can visualize the sufficient decrease condition with an example:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"using SimpleSolvers # hide\nusing SimpleSolvers: SufficientDecreaseCondition, NewtonOptimizerCache, update!, gradient!, linesearch_objective, ldiv! # hide\n\nx = [3., 1.3]\nf = x -> 10 * sum(x .^ 3 / 6 - x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\nhes = Hessian(obj, x; mode = :autodiff)\nupdate!(hes, x)\n\nc₁ = 1e-4\ng = gradient!(obj, x)\nrhs = -g\n# the search direction is determined by multiplying the right hand side with the inverse of the Hessian from the left.\np = similar(rhs)\nldiv!(p, hes, rhs)\nsdc = SufficientDecreaseCondition(c₁, x, f(x), g, p, obj)\n\n# check different values\nα₁, α₂, α₃, α₄, α₅ = .09, .4, 0.7, 1., 1.3\n(sdc(α₁), sdc(α₂), sdc(α₃), sdc(α₄), sdc(α₅))","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"We can also illustrate this:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"(Image: )","category":"page"},{"location":"objectives/#Objectives","page":"Objectives","title":"Objectives","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"A central object in SimpleSolvers are objectives (see SimpleSolvers.AbstractObjective). They are either SimpleSolvers.AbstractUnivariateObjectives or MultivariateObjectives. The goal of a solver (both LinearSolvers and NonlinearSolvers) is to make the objective have value zero. The goal of an Optimizer is to minimize a MultivariateObjective.","category":"page"},{"location":"objectives/#Examples","page":"Objectives","title":"Examples","text":"","category":"section"},{"location":"objectives/#Univariate-Objectives","page":"Objectives","title":"Univariate Objectives","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"We can allocate a univariate objective with:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"using SimpleSolvers\nusing Random; Random.seed!(123) # hide\n\nf(x::Number) = x ^ 2\nx = rand()\nobj = UnivariateObjective(f, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Associated to UnivariateObjective are the following functions (amongst others):","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"value\nvalue!\nvalue!!\nderivative\nderivative!\nderivative!!","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The function value evaluates the objective at the provided input and increases the counter by 1:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value(obj, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"We can check how the function call changed obj:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"obj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The stored value for f has not been updated. In order to so we can call the in-place function value!:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value!(obj, x)\nobj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"We further note that SimpleSolvers contains another function value!! that forces evaluation. This is opposed to value! which does not always force evaluation:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value!(obj, x)\nobj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"So value! first checks if the objective obj has already been called on x. In order to force another evaluation we can write:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value!!(obj, x)\nobj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The function value can also be called without additional input arguments:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"value(obj)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"But then the associated function is not called again (calling value this way does not increase the counter):","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"obj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"An equivalent relationship exists between the functions derivative, derivative! and derivative!!.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"In addition to UnivariateObjective, SimpleSolvers also contains a TemporaryUnivariateObjective[1]:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"[1]: To be used together with SimpleSolvers.linesearch_objective.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"t_obj = TemporaryUnivariateObjective(obj.F, obj.D)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"info: Why are there two types of univariate objectives?\nThere are two types of univariate objectives in SimpleSolvers: UnivariateObjectives and TemporaryUnivariateObjectives. The latter is only used for allocating line search objectives and contains less functionality.","category":"page"},{"location":"objectives/#Multivariate-Objectives","page":"Objectives","title":"Multivariate Objectives","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"MultivariateObjectives are used in a way similar to UnivariateObjectives, the difference is that the derivative functions are replaced by gradient functions, i.e.:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"derivative implies gradient,\nderivative! implies gradient!,\nderivative!! implies gradient!!.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Random.seed!(123) # hide\nf(x::AbstractArray) = sum(x .^ 2)\nx = rand(3)\n\nobj = MultivariateObjective(f, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Every instance of MultivariateObjective stores an instance of Gradient to which we similarly can apply the functions gradient or gradient!:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"gradient(obj, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The difference to Gradient is that we also store the value for the evaluated gradient, which can be accessed by calling:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"gradient(obj)","category":"page"},{"location":"in_place_out_of_place/#What-is-In-Place-and-what-is-Out-Of-Place","page":"In-place vs out-of-place","title":"What is In-Place and what is Out-Of-Place","text":"","category":"section"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"In SimpleSolvers we almost always use in-place functions internally for performance, but let the user deal with out-of-place functions for ease of use.","category":"page"},{"location":"in_place_out_of_place/#Example","page":"In-place vs out-of-place","title":"Example","text":"","category":"section"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"using SimpleSolvers\n\nf(x) = sum(x.^2 .* exp.(-abs.(x)) + 2 * cos.(x) .* exp.(-x.^2))\nnothing # hide","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"(Image: )","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"If we now allocate a MultivariateObjective based on this, we get a series of in-place functions based on this. For example value![1]:","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"[1]: See the section on objectives for an explanation of how to use value! and value.","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"x = [0.]\nobj = MultivariateObjective(f, x)\ny = [0.]\nvalue!(obj, x)\n@assert value(obj) == f(x) # hide\nvalue(obj) == f(x)","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"To compute the derivative we can use gradient![2]:","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"[2]: Note that we are using a MultivariateObjective and therefore gradient!. A UnivariateObjective has to be used together with derivative.","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"x = [[x] for x in -7.:.1:7.]\ny = Vector{Float64}[]\nfor x_sing in x\n    gradient!(obj, x_sing)\n    push!(y, copy(gradient(obj)))\nend\nnothing # hide","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"(Image: )","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"The idea is however that the user almost never used the in-place versions of these routines directly, but instead functions like solve! and value, gradient etc. as a possible diagnostic.","category":"page"},{"location":"gradients/#Gradients","page":"Gradients","title":"Gradients","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"The supertype Gradient comprises different ways of taking gradients:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"GradientFunction,\nGradientAutodiff,\nGradientFiniteDifferences.","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"We first start by showing GradientAutodiff:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"using SimpleSolvers, Random; using SimpleSolvers: GradientAutodiff, Gradient, GradientFunction, GradientFiniteDifferences; Random.seed!(123) # hide\nf(x::AbstractArray) = sum(x .^ 2)\nx = rand(3)\ngrad = GradientAutodiff(f, x)","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"Instead of calling GradientAutodiff(f, x) we can equivalently do:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"grad = Gradient(f, x; mode = :autodiff)","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"When calling an instance of Gradient we can use the functions gradient and gradient![1]:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"[1]: Internally these functions call functors that are implemented for the individual structs derived from Gradient, but for consistency (especially with regards to MultivariateObjectives) we recommend using the functions gradient and gradient!.","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"gradient(x, grad)","category":"page"},{"location":"nonlinear_solver_status/#Solver-Status","page":"Solver Status","title":"Solver Status","text":"","category":"section"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"In SimpleSolvers we can use the SimpleSolvers.NonlinearSolverStatus to provide a diagnostic tool for a NonlinearSolver. We first make an instance of NonlinearSystem:","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"using SimpleSolvers # hide\nusing SimpleSolvers: SufficientDecreaseCondition, NewtonOptimizerCache, update!, gradient!, linesearch_objective, ldiv! # hide\n\nx = [3., 1.3]\nf = x -> tanh.(x)\nnls = NonlinearSystem(f, x)","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"We now create an instance of NewtonSolver which also allocates a SimpleSolvers.NonlinearSolverStatus:","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"solver = NewtonSolver(x, f)","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"Note that all variables are initialized with NaNs.","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"For the first step we therefore have to call update![1]:","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"[1]: Also see the page on the update! function.","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"update!(solver, x)","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"Note that the residuals are still NaNs however as we need to perform at least two updates in order to compute them. As a next step we write:","category":"page"},{"location":"nonlinear_solver_status/","page":"Solver Status","title":"Solver Status","text":"x = [2., 1.2]\nupdate!(solver, x)","category":"page"},{"location":"linesearch/linesearch/#Line-Search","page":"Line Searches","title":"Line Search","text":"","category":"section"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"This page is largely a summary of [3, Chapter 3]. We summarize this reference by omitting proofs, but also aim to extend it to manifolds.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"A line search method has the goal of minimizing an objective (either a UnivariateObjective or a MultivariateObjective) approximately, based on a search direction[1].","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"[1]: in [3] (and other references) a search direction is called a descent direction.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"info: Definition\nFor an objective fmathcalMtomathbbR on a manifold mathcalM a search direction at point x_kinmathcalM is a vector p_kinT_x_kmathcalM for which we have    g_x_k(p_k mathrmgrad^g_x_kf)  0where g_x_kT_x_kmathcalMtimesT_x_kmathcalMtomathbbR is a Riemannian metric.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"A line search is therefore a sub-optimization problem in a nonlinear optimizer (or solver) in which we want to find an alpha that minimizes:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"    min_alphaf^mathrmls(alpha) = min_alphaf(mathcalR_x_k(alpha_kp_k))","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"where p_k is the search direction.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"For line search methods we have to (i) find a search direction p_k and (ii) find an appropriate step size alpha_k = mathrmargmin_alphaf(alpha). We then update x_k based on these quantities:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"    x_k+1 gets mathcalR_x_k(alpha_kp_k)","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"where mathcalR_x_kT_x_kmathcalMtomathcalM is a retraction at x_k","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"In practice we will not be able to find the ideal alpha at every step, but only an approximation thereof. Examples of line search algorithms that aim at finding this alpha are the static line search and the backtracking line search.","category":"page"},{"location":"linesearch/linesearch/#Line-Search-Objective","page":"Line Searches","title":"Line Search Objective","text":"","category":"section"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"SimpleSolvers contains a function SimpleSolvers.linesearch_objective that allocates a TemporaryUnivariateObjective that realizes the function f^mathrmls described above.","category":"page"},{"location":"linesearch/linesearch/#Search-Directions-for-Optimizers","page":"Line Searches","title":"Search Directions for Optimizers","text":"","category":"section"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"In SimpleSolvers we typically build the search direction by multiplying the gradient with a Hessian. When starting at x_k we take:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"    p_k = H_x_k^-1(nabla_x_kf)","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"where H_x_k_ij = partial^2fpartialx_ipartialx_j_x_k is the Hessian. Note that we often use approximations of this Hessian in practice (such as the HessianBFGS).","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"For manifolds [5] defining a Hessian, equivalently to defining a gradient, requires a Riemannian metric and the associated Levi-Civita connection nabla:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"mathrmHess(f) = nablanablaf = nabladf in Gamma(T^*mathcalMotimesT^*mathcalM)","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"For specific vector fields xi eta in Gamma(TmathcalM) we can write this as:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"langle mathrmHess(f)xi eta  rangle = xi(etaf) - (nabla_xieta)f","category":"page"},{"location":"hessians/#Hessians","page":"Hessians","title":"Hessians","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Hessians are a crucial ingredient in NewtonSolvers and SimpleSolvers.NewtonOptimizerStates.","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"using SimpleSolvers\nusing LinearAlgebra: norm\n\nx = rand(3)\nobj = MultivariateObjective(x -> norm(x - vcat(0., 0., 1.))  ^ 2, x)\nhes = HessianAutodiff(obj, x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"An instance of HessianAutodiff stores a Hessian matrix:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"hes.H","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"The instance of HessianAutodiff can be called:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"hes(x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Or equivalently with:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"update!(hes, x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"This updates hes.H:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"hes.H","category":"page"},{"location":"hessians/#BFGS-Hessian","page":"Hessians","title":"BFGS Hessian","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"using SimpleSolvers: initialize!\nhes = HessianBFGS(obj, x)\ninitialize!(hes, x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"For computational reasons we save the inverse of the Hessian, it can be accessed by calling inv:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"inv(hes)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Similarly to HessianAutodiff we can call update!:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"update!(hes, x)","category":"page"},{"location":"#SimpleSolvers","page":"Home","title":"SimpleSolvers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_p","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_p","text":"const DEFAULT_ARMIJO_p\n\nConstant used in BacktrackingState. Its value is 0.5\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_α₀","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_α₀","text":"const DEFAULT_ARMIJO_α₀\n\nThe default starting value for alpha used in SufficientDecreaseCondition (also see BacktrackingState and QuadraticState). Its value is 1.0\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_σ₀","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_σ₀","text":"const DEFAULT_ARMIJO_σ₀\n\nConstant used in QuadraticState. Also see DEFAULT_ARMIJO_σ₁.\n\nIt is meant to safeguard against stagnation when performing line searches (see [1]).\n\nIts value is 0.1\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_σ₁","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_σ₁","text":"const DEFAULT_ARMIJO_σ₁\n\nConstant used in QuadraticState. Also see DEFAULT_ARMIJO_σ₀. Its value is 0.5\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BIERLAIRE_ε","page":"Home","title":"SimpleSolvers.DEFAULT_BIERLAIRE_ε","text":"DEFAULT_BIERLAIRE_ε\n\nA constant that determines the precision in BierlaireQuadraticState. The constant recommended in [2] is 1E-3.\n\nNote that this constant may also depend on whether we deal with optimizers or solvers.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BIERLAIRE_ξ","page":"Home","title":"SimpleSolvers.DEFAULT_BIERLAIRE_ξ","text":"DEFAULT_BIERLAIRE_ξ\n\nA constant on basis of which the b in BierlaireQuadraticState is perturbed in order \"to avoid stalling\" (see [2, Chapter 11.2.1]; in this reference the author recommends 10^-7 as a value). Its value is 1.1920928955078125e-7.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_k","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_k","text":"const DEFAULT_BRACKETING_k\n\nGives the default ratio by which the bracket is increased if bracketing was not successful. See bracket_minimum.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_nmax","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_nmax","text":"Default constant\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_s","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_s","text":"const DEFAULT_BRACKETING_s\n\nGives the default width of the interval (the bracket). See bracket_minimum.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_GRADIENT_ϵ","page":"Home","title":"SimpleSolvers.DEFAULT_GRADIENT_ϵ","text":"DEFAULT_GRADIENT_ϵ\n\nA constant on whose basis finite differences are computed.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER","page":"Home","title":"SimpleSolvers.DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER","text":"The default number of iterations before the Jacobian is refactored in the QuasiNewtonSolver\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_JACOBIAN_ϵ","page":"Home","title":"SimpleSolvers.DEFAULT_JACOBIAN_ϵ","text":"DEFAULT_JACOBIAN_ϵ\n\nA constant used for computing the finite difference Jacobian.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_WOLFE_c₁","page":"Home","title":"SimpleSolvers.DEFAULT_WOLFE_c₁","text":"const DEFAULT_WOLFE_c₁\n\nA constant epsilon on which a finite difference approximation of the derivative of the objective is computed. This is then used in the following stopping criterion:\n\nfracf(alpha) - f(alpha_0)epsilon  alphacdotf(alpha_0)\n\nExtended help\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_s_REDUCTION","page":"Home","title":"SimpleSolvers.DEFAULT_s_REDUCTION","text":"A factor by which s is reduced in each bracketing iteration (see bracket_minimum_with_fixed_point).\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.MAX_NUMBER_OF_ITERATIONS_FOR_QUADRATIC_LINESEARCH","page":"Home","title":"SimpleSolvers.MAX_NUMBER_OF_ITERATIONS_FOR_QUADRATIC_LINESEARCH","text":"This constant is used for QuadraticState and BierlaireQuadraticState.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.N_STATIC_THRESHOLD","page":"Home","title":"SimpleSolvers.N_STATIC_THRESHOLD","text":"Threshold for the maximum size a static matrix should have.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.AbstractObjective","page":"Home","title":"SimpleSolvers.AbstractObjective","text":"AbstractObjective\n\nAn objective is a quantity to has to be made zero by a solver or minimized by an optimizer.\n\nSee AbstractUnivariateObjective and MultivariateObjective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.AbstractUnivariateObjective","page":"Home","title":"SimpleSolvers.AbstractUnivariateObjective","text":"AbstractUnivariateObjective <: AbstractObjective\n\nA subtype of AbstractObjective that only depends on one variable. See UnivariateObjective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Backtracking","page":"Home","title":"SimpleSolvers.Backtracking","text":"Backtracking <: LinesearchMethod\n\nThe backtracking method.\n\nConstructors\n\nBacktracking()\n\nExtended help\n\nThe backtracking algorithm starts by setting y_0 gets f(0) and d_0 gets nabla_0f.\n\nThe algorithm is executed by calling the functor of BacktrackingState.\n\nThe following is then repeated until the stopping criterion is satisfied or config.max_iterations (1000 by default) is reached:\n\nif value!(obj, α) ≥ y₀ + ls.ϵ * α * d₀\n    α *= ls.p\nelse\n    break\nend\n\nThe stopping criterion as an equation can be written as:\n\nf(lpha)  y_0 + psilon lpha \nabla_0f = y_0 + psilon (lpha - 0)\nabla_0f\n\nNote that if the stopping criterion is not reached, lpha is multiplied with p and the process continues.\n\nSometimes the parameters p and psilon have different names such as au and c.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BacktrackingCondition","page":"Home","title":"SimpleSolvers.BacktrackingCondition","text":"BacktrackingCondition\n\nAbstract type comprising the conditions that are used for checking step sizes for the backtracking line search (see BacktrackingState).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BacktrackingState","page":"Home","title":"SimpleSolvers.BacktrackingState","text":"BacktrackingState <: LinesearchState\n\nCorresponding LinesearchState to Backtracking.\n\nKeys\n\nThe keys are:\n\nconfig::Options\nα₀: \nϵ=$(DEFAULT_WOLFE_c₁): a default step size on whose basis we compute a finite difference approximation of the derivative of the objective. Also see DEFAULT_WOLFE_c₁.\np=$(DEFAULT_ARMIJO_p): a parameter with which alpha is decreased in every step until the stopping criterion is satisfied.\n\nFunctor\n\nThe functor is used the following way:\n\nls(obj, α = ls.α₀)\n\nImplementation\n\nThe algorithm starts by setting:\n\nx_0 gets 0\ny_0 gets f(x_0)\nd_0 gets f(x_0)\nalpha gets alpha_0\n\nwhere f is the univariate objective (of type AbstractUnivariateObjective) and alpha_0 is stored in ls. It then repeatedly does alpha gets alphacdotp until either (i) the maximum number of iterations is reached (the max_iterations keyword in Options) or (ii) the following holds:\n\n    f(alpha)  y_0 + epsilon cdot alpha cdot d_0\n\nwhere epsilon is stored in ls.\n\ninfo: Info\nThe algorithm allocates an instance of SufficientDecreaseCondition by calling SufficientDecreaseCondition(ls.ϵ, x₀, y₀, d₀, one(α), obj), here we take the value one for the search direction p, this is because we already have the search direction encoded into the line search objective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BierlaireQuadratic","page":"Home","title":"SimpleSolvers.BierlaireQuadratic","text":"BierlaireQuadratic <: LinesearchMethod\n\nAlgorithm taken from [2].\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BierlaireQuadraticState","page":"Home","title":"SimpleSolvers.BierlaireQuadraticState","text":"BierlaireQuadraticState <: LinesearchState\n\nExtended help\n\nNote that the performance of BierlaireQuadratic may heavily depend on the choice of DEFAULT_BIERLAIRE_ε (i.e. the precision) and DEFAULT_BIERLAIRE_ξ.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Bisection","page":"Home","title":"SimpleSolvers.Bisection","text":"Bisection <: LinesearchMethod\n\nThe bisection method.\n\nConstructors\n\nBisection()\n\nExtended help\n\nThe bisection algorithm starts with an interval and successively bisects it into smaller intervals until a root is found. See bisection.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BisectionState","page":"Home","title":"SimpleSolvers.BisectionState","text":"BisectionState <: LinesearchState\n\nCorresponding LinesearchState to Bisection.\n\nSee bisection for the implementation of the algorithm.\n\nConstructors\n\nBisectionState(options)\nBisectionState(; options)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BracketMinimumCriterion","page":"Home","title":"SimpleSolvers.BracketMinimumCriterion","text":"BracketMinimumCriterion <: BracketingCriterion\n\nThe criterion used for bracket_minimum.\n\nFunctor\n\nbc(yb, yc)\n\nThis checks whether yc is bigger than yb, i.e. whether c is past the minimum.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.CurvatureCondition","page":"Home","title":"SimpleSolvers.CurvatureCondition","text":"CurvatureCondition <: LinesearchCondition\n\nThe second of the Wolfe conditions [3]. The first one is the SufficientDecreaseCondition.\n\nThis encompasses the standard curvature condition and the strong curvature condition.\n\nConstructor\n\nCurvatureCondition(c, xₖ, gradₖ, pₖ, obj, grad; mode)\n\nHere grad has to be a Gradient and obj an AbstractObjective. The other inputs are either arrays or numbers.\n\nImplementation\n\nFor computational reasons CurvatureCondition also has a field gradₖ₊₁ in which the temporary new gradient is saved.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Gradient","page":"Home","title":"SimpleSolvers.Gradient","text":"Gradient\n\nAbstract type. strcuts that are derived from this need an assoicated functor that computes the gradient of a function (in-place).\n\nImplementation\n\nWhen a custom Gradient is implemented, a functor is needed:\n\nfunction (grad::Gradient)(g::AbstractVector, x::AbstractVector) end\n\nThis functor can also be called with gradient!.\n\nExamples\n\nExamples include:\n\nGradientFunction\nGradientAutodiff\nGradientFiniteDifferences\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientAutodiff","page":"Home","title":"SimpleSolvers.GradientAutodiff","text":"GradientAutodiff <: Gradient\n\nA struct that realizes Gradient by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\n∇config: result of applying ForwardDiff.GradientConfig.\n\nConstructors\n\nGradientAutodiff(F, x::AbstractVector)\nGradientAutodiff(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\ngrad(g, x) = ForwardDiff.gradient!(g, grad.F, x, grad.∇config)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientFiniteDifferences","page":"Home","title":"SimpleSolvers.GradientFiniteDifferences","text":"GradientFiniteDifferences <: Gradient\n\nA struct that realizes Gradient by using finite differences.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nϵ: small constant on whose basis the finite differences are computed.\ne: auxiliary vector used for computing finite differences. It's of the form e_1 = beginbmatrix 1  0  cdots  0 endbmatrix.\ntx: auxiliary vector used for computing finite differences. It stores the offset in the x vector.\n\nConstructor(s)\n\nGradientFiniteDifferences{T}(F, nx::Integer; ϵ)\n\nBy default for ϵ is DEFAULT_GRADIENT_ϵ.\n\nFunctor\n\nThe functor does:\n\nfor j in eachindex(x,g)\n    ϵⱼ = grad.ϵ * x[j] + grad.ϵ\n    fill!(grad.e, 0)\n    grad.e[j] = 1\n    grad.tx .= x .- ϵⱼ .* grad.e\n    f1 = grad.F(grad.tx)\n    grad.tx .= x .+ ϵⱼ .* grad.e\n    f2 = grad.F(grad.tx)\n    g[j] = (f2 - f1) / (2ϵⱼ)\nend\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientFunction","page":"Home","title":"SimpleSolvers.GradientFunction","text":"GradientFunction <: Gradient\n\nA struct that realizes a Gradient by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\n∇F!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\ngrad(g, x) = grad.∇F!(g, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Hessian","page":"Home","title":"SimpleSolvers.Hessian","text":"Hessian\n\nAbstract type. structs derived from this need an associated functor that computes the Hessian of a function (in-place).\n\nAlso see Gradient.\n\nImplementation\n\nWhen a custom Hessian is implemented, a functor is needed:\n\nfunction (hessian::Hessian)(h::AbstractMatrix, x::AbstractVector) end\n\nThis functor can also be called with compute_hessian!.\n\nExamples\n\nExamples include:\n\nHessianFunction\nHessianAutodiff\nHessianBFGS\nHessianDFP\n\nThese examples can also be called with e.g. Hessian(x; mode = :autodiff).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianAutodiff","page":"Home","title":"SimpleSolvers.HessianAutodiff","text":"HessianAutodiff <: Hessian\n\nA struct that realizes Hessian by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nH: a matrix in which the (updated) Hessian is stored. \nHconfig: result of applying ForwardDiff.HessianConfig.\n\nConstructors\n\nHessianAutodiff(F, x::AbstractVector)\nHessianAutodiff(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\nhes(g, x) = ForwardDiff.hessian!(hes.H, hes.F, x, grad.Hconfig)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianBFGS","page":"Home","title":"SimpleSolvers.HessianBFGS","text":"HessianBFGS <: Hessian\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianDFP","page":"Home","title":"SimpleSolvers.HessianDFP","text":"HessianDFP <: Hessian\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianFunction","page":"Home","title":"SimpleSolvers.HessianFunction","text":"HessianFunction <: Hessian\n\nA struct that realizes a Hessian by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\nH!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\nhes(H, x) = hes.H!(H, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Jacobian","page":"Home","title":"SimpleSolvers.Jacobian","text":"Jacobian\n\nAbstract type. structs that are derived from this need an associated functor that computes the Jacobian of a function (in-place).\n\nImplementation\n\nWhen a custom Jacobian is implemented, a functor is needed:\n\nfunction (j::Jacobian)(g::AbstractMatrix, x::AbstractVector) end\n\nThis functor can also be called with compute_jacobian!.\n\nExamples\n\nExamples include:\n\nJacobianFunction\nJacobianAutodiff\nJacobianFiniteDifferences\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Jacobian-Tuple{NewtonSolver}","page":"Home","title":"SimpleSolvers.Jacobian","text":"Jacobian(solver::NewtonSolver)\n\nReturn the Jacobian stored in the NonlinearSystem of solver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.Jacobian-Tuple{NonlinearSystem}","page":"Home","title":"SimpleSolvers.Jacobian","text":"Jacobian(x, nls::NonlinearSystem)\n\nReturn the Jacobian stored in nls. Also see jacobian(::NonlinearSystem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.JacobianAutodiff","page":"Home","title":"SimpleSolvers.JacobianAutodiff","text":"JacobianAutodiff <: Jacobian\n\nA struct that realizes Jacobian by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nJconfig: result of applying ForwardDiff.JacobianConfig.\nty: vector that is used for evaluating ForwardDiff.jacobian!\n\nConstructors\n\nJacobianAutodiff(F, y::AbstractVector)\nJacobianAutodiff(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\njac(J, x) = ForwardDiff.jacobian!(J, jac.ty, x, grad.Jconfig)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianFiniteDifferences","page":"Home","title":"SimpleSolvers.JacobianFiniteDifferences","text":"JacobianFiniteDifferences <: Jacobian\n\nA struct that realizes Jacobian by using finite differences.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nϵ: small constant on whose basis the finite differences are computed.\nf1:\nf2:\ne1: auxiliary vector used for computing finite differences. It's of the form e_1 = beginbmatrix 1  0  cdots  0 endbmatrix.\ne2:\ntx: auxiliary vector used for computing finite differences. It stores the offset in the x vector.\n\nConstructor(s)\n\nJacobianFiniteDifferences{T}(F, nx::Integer, ny::Integer; ϵ)\n\nBy default for ϵ is DEFAULT_JACOBIAN_ϵ.\n\nFunctor\n\nThe functor does:\n\nfor j in eachindex(x)\n    ϵⱼ = jac.ϵ * x[j] + jac.ϵ\n    fill!(jac.e, 0)\n    jac.e[j] = 1\n    jac.tx .= x .- ϵⱼ .* jac.e\n    f(jac.f1, jac.tx)\n    jac.tx .= x .+ ϵⱼ .* jac.e\n    f(jac.f2, jac.tx)\n    for i in eachindex(x)\n        J[i,j] = (jac.f2[i] - jac.f1[i]) / (2ϵⱼ)\n    end\nend\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianFunction","page":"Home","title":"SimpleSolvers.JacobianFunction","text":"JacobianFunction <: Jacobian\n\nA struct that realizes a Jacobian by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\nDF!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\njac(g, x) = jac.DF!(g, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LU","page":"Home","title":"SimpleSolvers.LU","text":"struct LU <: LinearSolverMethod\n\nA custom implementation of an LU solver, meant to solve a LinearSystem.\n\nRoutines that use the LU solver include factorize!, ldiv! and solve!. In practice the LU solver is used by calling the LinearSolver constructor and ldiv! or solve!, or with an instance of LU as an argument directly, as shown in the Example section of this docstring.\n\nconstructor\n\nThe constructor is called with either no argument:\n\nLU()\n\n# output\n\nLU{Missing}(missing, true)\n\nor with pivot and static as optional booleans:\n\nLU(; pivot=true, static=true)\n\n# output\n\nLU{Bool}(true, true)\n\nNote that if we do not supply an explicit keyword static, the corresponding field is missing (as in the first case). Also see _static.\n\nExample\n\nWe use the LU together with solve to solve a linear system:\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\nv = rand(3)\nls = LinearSystem(A, v)\nupdate!(ls, A, v)\n\nlu = LU()\n\nsolve(lu, ls) ≈ inv(A) * v\n\n# output\n\ntrue\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LUSolverCache","page":"Home","title":"SimpleSolvers.LUSolverCache","text":"LUSolverCache <: LinearSolverCache\n\nKeys\n\nA: the factorized matrix A,\npivots:\nperms:\ninfo\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LUSolverLAPACK","page":"Home","title":"SimpleSolvers.LUSolverLAPACK","text":"LUSolverLAPACK <: LinearSolver\n\nThe LU Solver taken from LinearAlgebra.BLAS.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSolver","page":"Home","title":"SimpleSolvers.LinearSolver","text":"LinearSolver <: AbstractSolver\n\nA struct that stores LinearSolverMethods and LinearSolverCaches. LinearSolvers are used to solve LinearSystems.\n\nConstructors\n\nLinearSolver(method, cache)\nLinearSolver(method, A)\nLinearSolver(method, ls::LinearSystem)\nLinearSolver(method, x)\n\ninfo: Info\nWe note that the constructors do not call the function factorize, so only allocate a new matrix. The factorization needs to be done manually.\n\nYou can manually factorize by either calling factorize! or solve!.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSolverCache","page":"Home","title":"SimpleSolvers.LinearSolverCache","text":"LinearSolverCache\n\nAn abstract type that summarizes all the caches used for LinearSolvers. See e.g. LUSolverCache.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSolverMethod","page":"Home","title":"SimpleSolvers.LinearSolverMethod","text":"LinearSolverMethod <: SolverMethod\n\nSummarizes all the methods used for solving linear systems of equations such as the LU method.\n\nExtended help\n\nThe abstract type SolverMethod was imported from GeometricBase.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSystem","page":"Home","title":"SimpleSolvers.LinearSystem","text":"LinearSystem\n\nA LinearSystem describes Ax = y, where we want to solve for x.\n\nKeys\n\nA\ny\n\nConstructors\n\nA LinearSystem can be allocated by calling:\n\nLinearSystem(A, y)\nLinearSystem)(A)\nLinearSystem(y)\nLinearSystem{T}(n, m)\nLinearSystem{T}(n)\n\nNote that in any case the allocated system is initialized with NaNs:\n\nA = [1. 2. 3.; 4. 5. 6.; 7. 8. 9.]\ny = [1., 2., 3.]\nls = LinearSystem(A, y)\n\n# output\n\nLinearSystem{Float64, Vector{Float64}, Matrix{Float64}}([NaN NaN NaN; NaN NaN NaN; NaN NaN NaN], [NaN, NaN, NaN])\n\nIn order to initialize the system with values, we have to call update!:\n\nupdate!(ls, A, y)\n\n# output\n\nLinearSystem{Float64, Vector{Float64}, Matrix{Float64}}([1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0], [1.0, 2.0, 3.0])\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Linesearch","page":"Home","title":"SimpleSolvers.Linesearch","text":"Linesearch\n\nA struct that stores the LinesearchMethod, the LinesearchState and Options.\n\nKeys\n\nalgorithm::LinesearchMethod\nconfig::Options\nstate::LinesearchState\n\nConstructors\n\nThe following constructors can be used:\n\nLinesearch(alg, config, state)\nLinesearch(; algorithm, config, kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinesearchMethod","page":"Home","title":"SimpleSolvers.LinesearchMethod","text":"LinesearchMethod\n\nExamples include StaticState, Backtracking, Bisection and Quadratic. See these examples for specific information on linesearch algorithms.\n\nExtended help\n\nA LinesearchMethod always goes together with a LinesearchState and each of those LinesearchStates has a functor implemented:\n\nls(obj, α)\n\nwhere obj is a AbstractUnivariateObjective and α is an initial step size. The output of this functor is then a final step size that is used for updating the parameters.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinesearchState","page":"Home","title":"SimpleSolvers.LinesearchState","text":"LinesearchState\n\nAbstract type. \n\nExamples include StaticState, BacktrackingState, BisectionState and QuadraticState.\n\nImplementation\n\nA struct that is subtyped from LinesearchState needs to implement the functors:\n\nls(x; kwargs...)\nls(obj::AbstractUnivariateObjective, x; kwargs...)\n\nAdditionaly the following function needs to be extended:\n\nLinesearchState(algorithm::LinesearchMethod; kwargs...)\n\nConstructors\n\nThe following is used to construct a specific line search state based on a LinesearchMethod:\n\nLinesearchState(algorithm::LinesearchMethod; T::DataType=Float64, kwargs...)\n\nwhere the data type should be specified each time the constructor is called. This is done automatically when calling the constructor of NewtonSolver for example.\n\nFunctors\n\nThe following functors are auxiliary helper functions:\n\nls(f::Callable; kwargs...) = ls(TemporaryUnivariateObjective(f, missing); kwargs...)\nls(f::Callable, x::Number; kwargs...) = ls(TemporaryUnivariateObjective(f, missing), x; kwargs...)\nls(f::Callable, g::Callable; kwargs...) = ls(TemporaryUnivariateObjective(f, g); kwargs...)\nls(f::Callable, g::Callable, x::Number; kwargs...) = ls(TemporaryUnivariateObjective(f, g), x; kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.MultivariateObjective","page":"Home","title":"SimpleSolvers.MultivariateObjective","text":"MultivariateObjective <: AbstractObjective\n\nLike UnivariateObjective, but stores gradients instead of derivatives. Also compare this to NonlinearSystem.\n\nThe type of the stored gradient has to be a subtype of Gradient.\n\nFunctor\n\nIf MultivariateObjective is called on a single function, the gradient is generated with GradientAutodiff.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonOptimizerCache","page":"Home","title":"SimpleSolvers.NewtonOptimizerCache","text":"NewtonOptimizerCache\n\nKeys\n\nx̄: the previous iterate,\nx: current iterate (this stores the guess called by the functions generated with linesearch_objective),\nδ: direction of optimization step (difference between x and x̄); this is obtained by multiplying rhs with the inverse of the Hessian,\ng: gradient value (this stores the gradient associated with x called by the derivative part of linesearch_objective),\nrhs: the right hand side used to compute the update.\n\nTo understand how these are used in practice see e.g. linesearch_objective.\n\nAlso compare this to NewtonSolverCache.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonOptimizerState","page":"Home","title":"SimpleSolvers.NewtonOptimizerState","text":"NewtonOptimizerState <: OptimizationAlgorithm\n\nThe optimizer state is needed to update the Optimizer. This is different to OptimizerStatus and OptimizerResult which serve as diagnostic tools. It stores a LinesearchState and a NewtonOptimizerCache which is used to compute the line search objective at each iteration.\n\nKeys\n\nlinesearch::LinesearchState\ncache::NewtonOptimizerCache\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonSolver","page":"Home","title":"SimpleSolvers.NewtonSolver","text":"NewtonSolver\n\nA struct that comprises all Newton solvers. Those typically differ in the way the Jacobian is computed.\n\nConstructors\n\nThe NewtonSolver can be called with an NonlinearSystem or with a Callable. Note however that the latter will probably be deprecated in the future.\n\nlinesearch = Quadratic()\nF(x) = tanh.(x)\nx = [.5, .5]\nNewtonSolver(x, F(x); F = F, linesearch = linesearch)\n\n# output\n\ni=   0,\nx= NaN,\nf= NaN,\nrxₐ= NaN,\nrfₐ= NaN\n\nWhat is shown here is the status of the NewtonSolver, i.e. an instance of NonlinearSolverStatus.\n\nKeywords\n\nnonlinearsystem::NonlinearSystem: the system that has to be solved. This can be accessed by calling nonlinearsystem,\njacobian::Jacobian\nlinear::LinearSolver: the linear solver is used to compute the direction of the solver step (see solver_step!). This can be accessed by calling linearsolver,\nlinesearch::LinesearchState\nrefactorize::Int: determines after how many steps the Jacobian is updated and refactored (see factorize!). If we have refactorize > 1, then we speak of a QuasiNewtonSolver,\ncache::NewtonSolverCache\nconfig::Options\nstatus::NonlinearSolverStatus:\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonSolver-Union{Tuple{AT}, Tuple{T}, Tuple{AT, Union{Function, Type}}, Tuple{AT, Union{Function, Type}, AT}} where {T, AT<:AbstractVector{T}}","page":"Home","title":"SimpleSolvers.NewtonSolver","text":"NewtonSolver(x, F, y)\n\nKeywords\n\nlinear_solver_method\nDF!\nlinesearch\nmode\noptions_kwargs: see Options\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.NewtonSolverCache","page":"Home","title":"SimpleSolvers.NewtonSolverCache","text":"NewtonSolverCache\n\nStores x̄, x, δx, rhs, y and J.\n\nCompare this to NewtonOptimizerCache.\n\nKeys\n\nx̄: the previous iterate,\nx: the next iterate (or guess thereof). The guess is computed when calling the functions created by linesearch_objective,\nδx: search direction. This is updated when calling solver_step! via the LinearSolver stored in the NewtonSolver,\nrhs: the right-hand-side (this can be accessed by calling rhs), \ny: the objective evaluated at x. This is used in linesearch_objective,\nJ::AbstractMatrix: the Jacobian evaluated at x. This is used in linesearch_objective. Note that this is not of type Jacobian!\n\nConstructor\n\nNewtonSolverCache(x, y)\n\nImplementation\n\nJ is allocated by calling alloc_j.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearMethod","page":"Home","title":"SimpleSolvers.NonlinearMethod","text":"A supertype collecting all nonlinear methods, including NewtonMethods.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSolver","page":"Home","title":"SimpleSolvers.NonlinearSolver","text":"NonlinearSolver <: AbstractSolver\n\nA supertype that comprises e.g. NewtonSolver.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSolverStatus","page":"Home","title":"SimpleSolvers.NonlinearSolverStatus","text":"NonlinearSolverStatus\n\nStores absolute, relative and successive residuals for x and f. It is used as a diagnostic tool in NewtonSolver.\n\nKeys\n\ni::Int: iteration number,\nrxₐ: absolute residual in x,\nrxₛ: successive residual in x,\nrfₐ: absolute residual in f,\nrfₛ: successive residual in f,\nx: the current solution (can also be accessed by calling solution),\nx̄: previous solution\nδ: change in solution (see direction). This is updated by calling update!(::NonlinearSolverStatus, ::AbstractVector, ::NonlinearSystem),\nx̃: a variable that gives the component-wise change via deltax,\nf₀: initial function value,\nf: current function value,\nf̄: previous function value,\nγ: records change in f. This is updated by calling update!(::NonlinearSolverStatus, ::AbstractVector, ::NonlinearSystem),\nx_converged::Bool\nf_converged::Bool\ng_converged::Bool\nf_increased::Bool\n\nExamples\n\nNonlinearSolverStatus{Float64}(3)\n\n# output\n\ni=   0,\nx= NaN,\nf= NaN,\nrxₐ= NaN,\nrfₐ= NaN\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSystem","page":"Home","title":"SimpleSolvers.NonlinearSystem","text":"NonlinearSystem\n\nA NonlinearSystem describes F(x) = y, where we want to solve for x and F is in nonlinear in general (also compare this to LinearSystem and MultivariateObjective).\n\ninfo: Info\nNonlinearSystems are used for solvers whereas MultivariateObjectives are their equivalent for optimizers.\n\nKeys\n\nF: accessed by calling Function(nls),\nJ::Jacobian: accessed by calling Jacobian(nls),\nf: accessed by calling value(nls),\nj: accessed by calling jacobian(nls),\nx_f: accessed by calling f_argument(nls),\nx_j: accessed by calling j_argument(nls),\nf_calls: accessed by calling f_calls(nls),\nj_calls: accessed by calling j_calls(nls).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSystem-Tuple{Union{Function, Type}, AbstractArray}","page":"Home","title":"SimpleSolvers.NonlinearSystem","text":"NonlinearSystem(F, x)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.NonlinearSystem-Union{Tuple{T}, Tuple{Union{Function, Type}, Union{Function, Type}, AbstractVector{T}}} where T<:Number","page":"Home","title":"SimpleSolvers.NonlinearSystem","text":"NonlinearSystem(F, J!, x)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.OptimizationAlgorithm","page":"Home","title":"SimpleSolvers.OptimizationAlgorithm","text":"An OptimizationAlgorithm is a data structure that is used to dispatch on different algorithms.\n\nIt needs to implement three methods,\n\ninitialize!(alg::OptimizationAlgorithm, ::AbstractVector)\nupdate!(alg::OptimizationAlgorithm, ::AbstractVector)\nsolver_step!(::AbstractVector, alg::OptimizationAlgorithm)\n\nthat initialize and update the state of the algorithm and perform an actual optimization step.\n\nFurther the following convenience methods should be implemented,\n\nobjective(alg::OptimizationAlgorithm)\ngradient(alg::OptimizationAlgorithm)\nhessian(alg::OptimizationAlgorithm)\nlinesearch(alg::OptimizationAlgorithm)\n\nwhich return the objective to optimize, its gradient and (approximate) Hessian as well as the linesearch algorithm used in conjunction with the optimization algorithm if any.\n\nSee NewtonOptimizerState for a struct that was derived from OptimizationAlgorithm.\n\ninfo: Info\nNote that a OptimizationAlgorithm is not necessarily a NewtonOptimizerState as we can also have other optimizers, Adam for example.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Optimizer","page":"Home","title":"SimpleSolvers.Optimizer","text":"Optimizer\n\nThe optimizer that stores all the information needed for an optimization problem. This problem can be solved by calling solve!(::AbstractVector, ::Optimizer).\n\nKeys\n\nalgorithm::OptimizationAlgorithm,\nobjective::MultivariateObjective,\nconfig::Options,\nresult::OptimizerResult,\nstate::OptimizationAlgorithm.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerResult","page":"Home","title":"SimpleSolvers.OptimizerResult","text":"OptimizerResult\n\nStores an OptimizerStatus as well as x, f and g (as keys). OptimizerStatus stores all other information (apart form x ,f and g); i.e. residuals etc.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerStatus","page":"Home","title":"SimpleSolvers.OptimizerStatus","text":"OptimizerStatus\n\nStores residuals (relative and absolute) and various convergence properties.\n\nSee OptimizerResult.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Options","page":"Home","title":"SimpleSolvers.Options","text":"Options\n\nKeys\n\nConfigurable options with defaults (values 0 and NaN indicate unlimited):\n\nx_abstol = 2eps(T): absolute tolerance for x (the function argument). Used in e.g. assess_convergence! and bisection,\nx_reltol = 2eps(T): relative tolerance for x (the function argument). Used in e.g. assess_convergence!,\nx_suctol = 2eps(T): succesive tolerance for x. Used in e.g. assess_convergence!,\nf_abstol = zero(T): absolute tolerance for how close the function value should be to zero. See absolute_tolerance. Used in e.g. bisection and assess_convergence!,\nf_reltol = 2eps(T): relative tolerance for the function value. Used in e.g. assess_convergence!,\nf_suctol = 2eps(T): succesive tolerance for the function value. Used in e.g. assess_convergence!,\nf_mindec = T(10)^-4: minimum value by which the function has to decrease (also see minimum_decrease_threshold),\ng_restol = 2eps(T): tolerance for the residual (?) of the gradient,\nx_abstol_break = -Inf: see meets_stopping_criteria,\nx_reltol_break = Inf: see meets_stopping_criteria,\nf_abstol_break = Inf: see meets_stopping_criteria,\nf_reltol_break = Inf: see meets_stopping_criteria.,\ng_restol_break = Inf,\nf_calls_limit = 0,\ng_calls_limit = 0,\nh_calls_limit = 0,\nallow_f_increases = true,\nmin_iterations = 0,\nmax_iterations = 1000: the maximum number of iterations used in an alorithm, e.g. bisection and the functor for BacktrackingState,\nwarn_iterations = 1000,\nshow_trace = false,\nstore_trace = false,\nextended_trace = false,\nshow_every = 1,\nverbosity = 1\n\nSome of the constants are defined by the functions default_tolerance and absolute_tolerance.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Quadratic","page":"Home","title":"SimpleSolvers.Quadratic","text":"Quadratic <: LinesearchMethod\n\nThe quadratic method. Compare this to BierlaireQuadratic. The algorithm is taken from [1].\n\nConstructors\n\nQuadratic()\n\nExtended help\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Quadratic2","page":"Home","title":"SimpleSolvers.Quadratic2","text":"Quadratic2 <: LinesearchMethod\n\nThe second quadratic method. Compare this to Quadratic.\n\nExtended help\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.QuadraticState","page":"Home","title":"SimpleSolvers.QuadraticState","text":"QuadraticState <: LinesearchState\n\nQuadratic Polynomial line search.\n\nQuadratic line search works by fitting a polynomial to a univariate objective (see AbstractUnivariateObjective) and then finding the minimum of that polynomial. Also compare this to BierlaireQuadraticState. The algorithm is taken from [1].\n\nKeywords\n\nconfig::Options\nα₀: by default DEFAULT_ARMIJO_α₀\nσ₀: by default DEFAULT_ARMIJO_σ₀\nσ₁: by default DEFAULT_ARMIJO_σ₁\nc: by default DEFAULT_WOLFE_c₁\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.QuadraticState2","page":"Home","title":"SimpleSolvers.QuadraticState2","text":"QuadraticState2 <: LinesearchState\n\nQuadratic Polynomial line search. This is similar to QuadraticState, but performs multiple iterations in which all parameters p_0, p_1 and p_2 are changed. This is different from QuadraticState (taken from [1]), where only p_2 is changed. We further do not check the SufficientDecreaseCondition but rather whether the derivative is small enough.\n\nThis algorithm repeatedly builds new quadratic polynomials until a minimum is found (to sufficient accuracy). The iteration may also stop after we reaches the maximum number of iterations (see MAX_NUMBER_OF_ITERATIONS_FOR_QUADRATIC_LINESEARCH).\n\nKeywords\n\nconfig::Options\nε: A constant that checks the precision/tolerance.\ns: A constant that determines the initial interval for bracketing. By default this is DEFAULT_BRACKETING_s.\ns_reduction: A constant that determines the factor by which s is decreased in each new bracketing iteration.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Static","page":"Home","title":"SimpleSolvers.Static","text":"Static <: LinesearchMethod\n\nThe static method.\n\nConstructors\n\nStatic(α)\n\nKeys\n\nKeys include: -α: equivalent to a step size. The default is 1.\n\nExtended help\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.StaticState","page":"Home","title":"SimpleSolvers.StaticState","text":"StaticState <: LinesearchState\n\nThe state for Static.\n\nFunctors\n\nFor a Number a and an AbstractUnivariateObjective obj we have the following functors:\n\nls.(a) = ls.α\nls.(obj, a) = ls.α\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.SufficientDecreaseCondition","page":"Home","title":"SimpleSolvers.SufficientDecreaseCondition","text":"SufficientDecreaseCondition <: LinesearchCondition\n\nThe condition that determines if alpha_k is big enough.\n\nConstructor\n\nSufficientDecreaseCondition(c₁, xₖ, fₖ, gradₖ, pₖ, obj)\n\nFunctors\n\nsdc(xₖ₊₁, αₖ)\nsdc(αₖ)\n\nThe second functor is shorthand for sdc(compute_new_iterate(sdc.xₖ, αₖ, sdc.pₖ), T(αₖ)), also see compute_new_iterate.\n\nExtended help\n\nWe call the constant that pertains to the sufficient decrease condition c. This is typically called c_1 in the literature [3]. See DEFAULT_WOLFE_c₁ for the relevant constant\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.TemporaryUnivariateObjective","page":"Home","title":"SimpleSolvers.TemporaryUnivariateObjective","text":"TemporaryUnivariateObjective <: AbstractUnivariateObjective\n\nLike UnivariateObjective but doesn't store f, d, x_f and x_d as well as f_calls and d_calls.\n\nIn practice TemporaryUnivariateObjectives are allocated by calling linesearch_objective.\n\nConstructors\n\nwarn: Calling line search objectives\nBelow we show a few constructors that can be used to allocate TemporaryUnivariateObjectives. Note however that in practice one probably should not do that and instead call linesearch_objective.\n\nf(x) = x^2 - 1\ng(x) = 2x\nδx(x) = - g(x) / 2\nx₀ = 3.\n_f(α) = f(compute_new_iterate(x₀, α, δx(x₀)))\n_d(α) = g(compute_new_iterate(x₀, α, δx(x₀)))\nls_obj = TemporaryUnivariateObjective{typeof(x₀)}(_f, _d)\n\n# output\n\nTemporaryUnivariateObjective{Float64, typeof(_f), typeof(_d)}(_f, _d)\n\nAlternatively one can also do:\n\nls_obj = TemporaryUnivariateObjective(_f, _d, x₀)\n\n# output\n\nTemporaryUnivariateObjective{Float64, typeof(_f), typeof(_d)}(_f, _d)\n\nHere we wrote ls_obj to mean line search objective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.UnivariateObjective","page":"Home","title":"SimpleSolvers.UnivariateObjective","text":"UnivariateObjective <: AbstractUnivariateObjective\n\nKeywords\n\nIt stores the following:\n\nF: objective\nD: derivative of objective\nf: cache for function output\nd: cache for derivative output\nx_f: x used to evaluate F (stored in f)\nx_d: x used to evaluate D (stored in d)\nf_calls: number of times F has been called\nd_calls: number of times D has been called\n\nConstructor\n\nThere are several constructors, the most generic (besides the default one) is:\n\nUnivariateObjective(F, D, x; f, d)\n\nWhere no keys are inferred, except x_f and x_d (via alloc_f and alloc_d). f_calls and d_calls are set to zero.\n\nThe most general constructor (i.e. the one the needs the least specification) is:\n\nf(x::Number) = x ^ 2\nUnivariateObjective(f, 1.)\n\n# output\n\nUnivariateObjective:\n\n    f(x)              = NaN\n    d(x)              = NaN\n    x_f               = NaN\n    x_d               = NaN\n    number of f calls = 0\n    number of d calls = 0\n\nwhere ForwardDiff is used to generate the derivative of the (anonymous) function.\n\nFunctor\n\nThe functor calls value!.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricBase.update!-Tuple{HessianAutodiff, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(H, x)\n\nUpdate a HessianAutodiff object H.\n\nThis is identical to initialize!.\n\nImplementation\n\nInternally this is calling the HessianAutodiff functor and therefore also ForwardDiff.hessian!.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{MultivariateObjective, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(obj, x)\n\nCall value! and gradient! on obj.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{NewtonSolver, AbstractArray}","page":"Home","title":"GeometricBase.update!","text":"update!(solver, x)\n\nUpdate the solver::NewtonSolver based on x. This updates the cache (instance of type NewtonSolverCache) and the status (instance of type NonlinearSolverStatus). In course of updating the latter, we also update the nonlinear stored in solver (and status(solver)).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{Optimizer, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(opt, x)\n\nCompute objective and gradient at new solution and update result.\n\nThis first calls update!(::OptimizerResult, ::AbstractVector, ::AbstractVector, ::AbstractVector) and then update!(::NewtonOptimizerState, ::AbstractVector). We note that the OptimizerStatus (unlike the NewtonOptimizerState) is updated when calling update!(::OptimizerResult, ::AbstractVector, ::AbstractVector, ::AbstractVector).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{SimpleSolvers.NewtonOptimizerCache, AbstractVector, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(cache, x, g)\n\nUpdate the NewtonOptimizerCache based on x and g.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{SimpleSolvers.NewtonOptimizerCache, AbstractVector, Union{Gradient, AbstractVector}, Hessian}","page":"Home","title":"GeometricBase.update!","text":"update!(cache::NewtonOptimizerCache, x, g, hes)\n\nUpdate an instance of NewtonOptimizerCache based on x.\n\nThis is used in update!(::NewtonOptimizerState, ::AbstractVector).\n\nThis sets:\n\nbarx^mathttcache gets x\nx^mathttcache gets x\ng^mathttcache gets g\nmathrmrhs^mathttcache gets -g\ndelta^mathttcache gets H^-1mathrmrhs^mathttcache\n\nwhere we wrote H for the Hessian (i.e. the input argument hes). \n\nAlso see update!(::NewtonSolverCache, ::AbstractVector). \n\nwarn: Warn\nNote that this is not updating the Hessian hes. For this call update! on the Hessian.\n\nImplementation\n\nThe multiplication by the inverse of H is done with LinearAlgebra.ldiv!.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{SimpleSolvers.NewtonOptimizerState, AbstractVector, Union{Gradient, AbstractVector}, Hessian}","page":"Home","title":"GeometricBase.update!","text":"update!(state::NewtonOptimizerState, x, g, hes)\n\nUpdate an instance of NewtonOptimizerState based on x, g and hes, where g can either be an AbstractVector or a Gradient and hes is a Hessian. This updates the NewtonOptimizerCache contained in the NewtonOptimizerState by calling update!(::NewtonOptimizerCache, ::AbstractVector, ::Union{AbstractVector, Gradient}, ::Hessian).\n\ninfo: Info\nAn instance of NewtonOptimizerState stores the NewtonOptimizerCache as well as a LinesearchState. The LinesearchState stays the same at every iteration, which is why only the NewtonOptimizerState is updated.\n\nExamples\n\nWe show that after initializing, update has to be called together with a Gradient and a Hessian:\n\nIf we only call update! once there are still NaNs in the ...\n\nf(x) = sum(x.^2)\nx = [1., 2.]\nstate = NewtonOptimizerState(x)\nobj = MultivariateObjective(f, x)\ng = gradient!(obj, x)\nhes = HessianAutodiff(obj, x)\nupdate!(hes, x)\nupdate!(state, x, g, hes)\n\n# output\n\nNewtonOptimizerState{Float64, SimpleSolvers.BacktrackingState{Float64}, SimpleSolvers.NewtonOptimizerCache{Float64, Vector{Float64}}}(Backtracking with α₀ = 1.0, ϵ = 0.0001and p = 0.5., SimpleSolvers.NewtonOptimizerCache{Float64, Vector{Float64}}([1.0, 2.0], [1.0, 2.0], [-1.0, -2.0], [2.0, 4.0], [-2.0, -4.0]))\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{SimpleSolvers.NonlinearSolverStatus, AbstractVector, NonlinearSystem}","page":"Home","title":"GeometricBase.update!","text":"update!(status, x, nls)\n\nUpdate the status::NonlinearSolverStatus based on x for the NonlinearSystem obj. Internally this calls next_iteration!.\n\ninfo: Info\n\n\nThis also updates the objective nls!\n\nThe new x and x̄ stored in status are used to compute δ. The new f and f̄ stored in status are used to compute γ. See NonlinearSolverStatus for an explanation of those variables.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{SimpleSolvers.OptimizerResult, AbstractVector, Number, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(result, x, f, g)\n\nUpdate the OptimizerResult based on x, f and g (all vectors). This involves updating the OptimizerStatus stored in result (by calling residual!).\n\nThis also calls increase_iteration_number!(::OptimizerResult)\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Union{Tuple{HT}, Tuple{HT, AbstractVector}} where HT<:Hessian","page":"Home","title":"GeometricBase.update!","text":"update!(hessian, x)\n\nUpdate the Hessian based on the vector x. For an explicit example see e.g. update!(::HessianAutodiff).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Union{Tuple{T}, Tuple{LinearSystem{T, VT, AT} where {VT<:AbstractVector{T}, AT<:AbstractMatrix{T}}, AbstractMatrix{T}, AbstractVector{T}}} where T","page":"Home","title":"GeometricBase.update!","text":"update!(ls, A, y)\n\nSet the rhs vector to y and the matrix stored in ls to A.\n\ninfo: Info\nCalling update! doesn't solve the LinearSystem, you still have to call solve! in combination with a LinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Union{Tuple{T}, Tuple{SimpleSolvers.NewtonSolverCache{T, AT} where AT<:AbstractVector{T}, AbstractVector{T}}} where T","page":"Home","title":"GeometricBase.update!","text":"update!(cache, x)\n\nUpdate the NewtonSolverCache based on x, i.e.:\n\ncache.x̄ gets x,\ncache.x gets x,\ncache.δx gets 0.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.value-Tuple{SimpleSolvers.AbstractObjective, Union{Number, AbstractArray{<:Number}}}","page":"Home","title":"GeometricBase.value","text":"value(obj::AbstractObjective, x)\n\nEvaluates the objective value at x (i.e. computes obj.F(x)).\n\nExamples\n\nusing SimpleSolvers\n\nobj = UnivariateObjective(x::Number -> x^2, 1.)\nvalue(obj, 2.)\nobj.f_calls\n\n# output\n\n1\n\nNote that the f_calls counter increased by one!\n\nIf value is called on obj (an AbstractObjective) without supplying x than the output of the last obj.F call is returned:\n\nusing SimpleSolvers\n\nobj = UnivariateObjective(x::Number -> x^2, 1.)\nvalue(obj)\n\n# output\n\nNaN\n\nIn this example this is NaN since the function hasn't been called yet.\n\n\n\n\n\n","category":"method"},{"location":"#LinearAlgebra.ldiv!-Union{Tuple{LUT}, Tuple{T}, Tuple{AbstractVector{T}, LinearSolver{T, LUT}, AbstractVector{T}}} where {T, LUT<:LU}","page":"Home","title":"LinearAlgebra.ldiv!","text":"ldiv!(x, lu, b)\n\nCompute inv(cache(lsolver).A) * b by utilizing the factorization of the lu solver (see LU and LinearSolver) and store the result in x.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.QuasiNewtonSolver-Tuple","page":"Home","title":"SimpleSolvers.QuasiNewtonSolver","text":"QuasiNewtonSolver\n\nA convenience constructor for NewtonSolver. Also see DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER.\n\nCalling QuasiNewtonSolver hence produces an instance of NewtonSolver with the difference that refactorize ≠ 1. The Jacobian is thus not evaluated and refactored in every step.\n\nImplementation\n\nIt does:\n\nQuasiNewtonSolver(args...; kwargs...) = NewtonSolver(args...; refactorize=DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER, kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers._static-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers._static","text":"_static(A)\n\nDetermine whether to allocate a StaticArray or simply copy the input array. This is used when calling LinearSolverCache on LU. Every matrix that is smaller or equal to N_STATIC_THRESHOLD is turned into a StaticArray as a consequence.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.absolute_tolerance-Union{Tuple{Type{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"SimpleSolvers.absolute_tolerance","text":"absolute_tolerance(T)\n\nDetermine the absolute tolerance for a specific data type. This is used in the constructor of Options.\n\nIn comparison to default_tolerance, this should return a very small number, close to zero (i.e. not just machine precision).\n\nExamples\n\nabsolute_tolerance(Float64)\n\n# output\n\n0.0\n\nabsolute_tolerance(Float32)\n\n# output\n\n0.0f0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.adjust_α-Union{Tuple{T}, Tuple{SimpleSolvers.QuadraticState{T}, T, T}} where T","page":"Home","title":"SimpleSolvers.adjust_α","text":"adjust_α(ls, αₜ, α)\n\nCheck which conditions the new αₜ is in sigma_0alpha_0 simga_1alpha_0 and return the updated α accordingly (it is updated if it does not lie in the interval).\n\nWe first check the following:\n\n    alpha_t   alpha_0alpha\n\nwhere sigma_0 is stored in ls (i.e. in an instance of QuadraticState). If this is not true we check:\n\n    alpha_t  sigma_1alpha\n\nwhere sigma_1 is again stored in ls. If this second condition is also not true we simply return the unchanged alpha_t. So if \\alpha_t does not lie in the interval (sigma_0alpha sigma_1alpha) the interval is made bigger by either multiplying with sigma_0 (default DEFAULT_ARMIJO_σ₀) or sigma_1 (default DEFAULT_ARMIJO_σ₁).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.adjust_α-Union{Tuple{T}, Tuple{T, T}, Tuple{T, T, T}, NTuple{4, T}} where T","page":"Home","title":"SimpleSolvers.adjust_α","text":"adjust_α(αₜ, α)\n\nAdjust αₜ based on the previous α. Also see adjust_α(::QuadraticState{T}, ::T, ::T) where {T}.\n\nThe check that alpha in sigma_0alpha_mathrmold sigma_1alpha_mathrmold should safeguard against stagnation in the iterates as well as checking that alpha decreases at least by a factor sigma_1. The defaults for σ₀ and σ₁ are DEFAULT_ARMIJO_σ₀ and DEFAULT_ARMIJO_σ₁ respectively.\n\nImplementation\n\nWee use defaults DEFAULT_ARMIJO_σ₀ and DEFAULT_ARMIJO_σ₁.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.alloc_d","page":"Home","title":"SimpleSolvers.alloc_d","text":"alloc_d(x)\n\nAllocate NaNs of the size of the derivative of f (with respect to x).\n\nThis is used in combination with a AbstractUnivariateObjective.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_f","page":"Home","title":"SimpleSolvers.alloc_f","text":"alloc_f(x)\n\nAllocate NaNs of the size the size of f (evaluated at x).\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_g","page":"Home","title":"SimpleSolvers.alloc_g","text":"alloc_g(x)\n\nAllocate NaNs of the size of the gradient of f (with respect to x).\n\nThis is used in combination with a MultivariateObjective.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_h","page":"Home","title":"SimpleSolvers.alloc_h","text":"alloc_h(x)\n\nAllocate NaNs of the size of the Hessian of f (with respect to x).\n\nThis is used in combination with a MultivariateObjective.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_j","page":"Home","title":"SimpleSolvers.alloc_j","text":"alloc_j(x, f)\n\nAllocate NaNs of the size of the Jacobian of f (with respect to x).\n\nThis is used in combination with a MultivariateObjective.\n\nExamples\n\nx = rand(3)\nfₓ = rand(2)\nalloc_j(x, fₓ)\n\n# output\n\n2×3 Matrix{Float64}:\n NaN  NaN  NaN\n NaN  NaN  NaN\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_x","page":"Home","title":"SimpleSolvers.alloc_x","text":"alloc_x(x)\n\nAllocate NaNs of the size of x.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.assess_convergence!-Tuple{SimpleSolvers.NonlinearSolverStatus, Options}","page":"Home","title":"SimpleSolvers.assess_convergence!","text":"assess_convergence!(status, config)\n\nCheck if one of the following is true for status::NonlinearSolverStatus:\n\nstatus.rxₐ ≤ config.x_abstol,\nstatus.rxₛ ≤ config.x_suctol,\nstatus.rfₐ ≤ config.f_abstol,\nstatus.rfₛ ≤ config.f_suctol.\n\nAlso see meets_stopping_criteria. The tolerances are by default determined with default_tolerance.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.assess_convergence!-Tuple{SimpleSolvers.OptimizerStatus, Options}","page":"Home","title":"SimpleSolvers.assess_convergence!","text":"assess_convergence!(status, config)\n\nChecks if the optimizer converged.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bisection-Tuple{Any, Number}","page":"Home","title":"SimpleSolvers.bisection","text":"bisection(f, x)\n\nUse bracket_minimum to find a starting interval and then do bisections.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bisection-Union{Tuple{T}, Tuple{Union{Function, Type}, T, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bisection","text":"bisection(f, xmin, xmax; config)\n\nPerform bisection of f in the interval [xmin, xmax] with Options config.\n\nThe algorithm is repeated until a root is found (up to tolerance config.f_abstol which is determined by default_tolerance by default).\n\nimplementation\n\nWhen calling bisection it first checks if x_mathrmmin  x_mathrmmax and else flips the two entries.\n\nExtended help\n\nThe bisection algorithm divides an interval into equal halves until a root is found (up to a desired accuracy).\n\nWe first initialize:\n\nbeginaligned\nx_0 gets  x_mathrmmin\nx_1 gets  x_mathrmmax\nendaligned\n\nand then repeat:\n\nbeginaligned\n x gets fracx_0 + x_12 \n textif f(x_0)f(x)  0 \n qquad x_0 gets x \n textelse \n qquad x_1 gets x \n textend\nendaligned\n\nSo the algorithm checks in each step where the sign change occurred and moves the x_0 or x_1 accordingly. The loop is terminated (and errors) if config.max_iterations is reached (by default1000 and the Options struct).\n\nwarning: Warning\nThe obvious danger with using bisections is that the supplied interval can have multiple roots (or no roots). One should be careful to avoid this when fixing the interval.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_minimum-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_minimum","text":"bracket_minimum(f, x)\n\nMove a bracket successively in the search direction (starting at x) and increase its size until a local minimum of f is found.  This is used for performing Bisections when only one x is given (and not an entire interval).  This bracketing algorithm is taken from [4]. Also compare it to bracket_minimum_with_fixed_point.\n\nKeyword arguments\n\ns::DEFAULT_BRACKETING_s\nk::DEFAULT_BRACKETING_k\nnmax::DEFAULT_BRACKETING_nmax\n\nExtended help\n\nFor bracketing we need two constants s and k (see DEFAULT_BRACKETING_s and DEFAULT_BRACKETING_k). \n\nBefore we start the algorithm we initialize it, i.e. we check that we indeed have a descent direction:\n\nbeginaligned\n a gets x \n b gets a + s \n mathrmif quad f(b)  f(a)\n qquadtextFlip a and b and set sgets-s\n mathrmend\nendaligned\n\nThe algorithm then successively computes:\n\nc gets b + s\n\nand then checks whether f(c)  f(b). If this is true it returns (a c) or (c a), depending on whether ac or ca respectively. If this is not satisfied a b and s are updated:\n\nbeginaligned\na gets  b \nb gets  c \ns gets  sk \nendaligned\n\nand the algorithm is continued. If we have not found a sign chance after n_mathrmmax iterations (see DEFAULT_BRACKETING_nmax) the algorithm is terminated and returns an error. The interval that is returned by bracket_minimum is then typically used as a starting point for bisection.\n\ninfo: Info\nThe function bracket_root is equivalent to bracket_minimum with the only difference that the criterion we check for is:f(c)f(b)  0i.e. that a sign change in the function occurs.\n\nSee bracket_root.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_minimum_with_fixed_point-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_minimum_with_fixed_point","text":"bracket_minimum_with_fixed_point(f, x)\n\nFind a bracket while keeping the left side (i.e. x) fixed.  The algorithm is similar to bracket_minimum (also based on DEFAULT_BRACKETING_s and DEFAULT_BRACKETING_k) with the difference that for the latter the left side is also moving.\n\nThe function bracket_minimum_with_fixed_point is used as a starting point for Quadratic (taken from [1]), as the minimum of the polynomial approximation is:\n\np_2 = fracf(b) - f(a) - f(0)bb^2\n\nwhere b = mathttbracket_minimum_with_fixed_point(a). We check that f(b)  f(a) in order to ensure that the curvature of the polynomial (i.e. p_2 is positive) and we have a minimum.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_root-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_root","text":"bracket_root(f, x)\n\nMake a bracket for the function based on x (for root finding).\n\nThis is largely equivalent to bracket_minimum. See the end of that docstring for more information.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.cache-Tuple{LinearSolver}","page":"Home","title":"SimpleSolvers.cache","text":"cache(ls)\n\nReturn the cache (of type LinearSolverCache) of the LinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_gradient-Tuple{AbstractVector}","page":"Home","title":"SimpleSolvers.check_gradient","text":"check_gradient(g)\n\nCheck norm, maximum value and minimum value of a vector.\n\nExamples\n\nusing SimpleSolvers\n\ng = [1., 1., 1., 2., 0.9, 3.]\nSimpleSolvers.check_gradient(g; digits=3)\n\n# output\n\nnorm(Gradient):               4.1\nminimum(|Gradient|):          0.9\nmaximum(|Gradient|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_hessian-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers.check_hessian","text":"check_hessian(H)\n\nCheck the condition number, determinant, max and min value of the Hessian H.\n\nusing SimpleSolvers\n\nH = [1. √2.; √2. 3.]\nSimpleSolvers.check_hessian(H)\n\n# output\n\nCondition Number of Hessian: 13.9282\nDeterminant of Hessian:      1.0\nminimum(|Hessian|):          1.0\nmaximum(|Hessian|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_jacobian-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers.check_jacobian","text":"check_jacobian(J)\n\nCheck the condition number, determinant, max and min value of the Jacobian J.\n\nusing SimpleSolvers\n\nJ = [1. √2.; √2. 3.]\nSimpleSolvers.check_jacobian(J)\n\n# output\n\nCondition Number of Jacobian: 13.9282\nDeterminant of Jacobian:      1.0\nminimum(|Jacobian|):          1.0\nmaximum(|Jacobian|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Tuple{MultivariateObjective}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(obj)\n\nSimilar to initialize!, but with only one input argument.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Tuple{NonlinearSystem}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(nls::NonlinearSystem)\n\nSimilar to initialize!, but with only one input argument.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Tuple{SimpleSolvers.AbstractUnivariateObjective}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(obj)\n\nSimilar to initialize!, but with only one input argument.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Union{Tuple{LinearSystem{T, VT, AT} where {VT<:AbstractVector{T}, AT<:AbstractMatrix{T}}}, Tuple{T}} where T","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(ls)\n\nWrite NaNs into Matrix(ls) and Vector(ls).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Union{Tuple{SimpleSolvers.OptimizerResult{XT, YT, VT, OST} where {VT<:(AbstractArray{XT}), OST<:SimpleSolvers.OptimizerStatus{XT, YT}}}, Tuple{YT}, Tuple{XT}} where {XT, YT}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(result)\n\nClear all the information contained in result::OptimizerResult. This also calls clear!(::OptimizerStatus).\n\ninfo: Info\n\n\nCalling initialize! on an OptimizerResult calls clear! internally.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Union{Tuple{SimpleSolvers.OptimizerStatus{XT, YT}}, Tuple{YT}, Tuple{XT}} where {XT, YT}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(obj)\n\nSimilar to initialize!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_gradient!","page":"Home","title":"SimpleSolvers.compute_gradient!","text":"compute_gradient!\n\nAlias for gradient!. Will probably be deprecated.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.compute_hessian!-Tuple{AbstractMatrix, AbstractVector, Any}","page":"Home","title":"SimpleSolvers.compute_hessian!","text":"compute_hessian!(h, x, ForH)\n\nCompute the hessian of function ForH at x and store it in h.\n\nImplementation\n\nInternally this allocates a Hessian object.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_hessian!-Tuple{AbstractMatrix, AbstractVector, Hessian}","page":"Home","title":"SimpleSolvers.compute_hessian!","text":"compute_hessian!(h, x, hessian)\n\nCompute the Hessian and store it in h.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_hessian-Tuple{Any, Hessian}","page":"Home","title":"SimpleSolvers.compute_hessian","text":"compute_hessian(x, hessian)\n\nCompute the Hessian at point x and return the result.\n\nInternally this calls compute_hessian!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_hessian_ad!-Union{Tuple{FT}, Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, FT}} where {T, FT}","page":"Home","title":"SimpleSolvers.compute_hessian_ad!","text":"compute_hessian_ad!(g, x, F)\n\nBuild a HessianAutodiff object based on F and apply it to x. The result is stored in H.\n\nAlso see gradient_ad! for the Gradient version.\n\nImplementation\n\nThis is using compute_hessian! with the keyword mode set to autodiff.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_jacobian!-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, Jacobian{T}}} where T","page":"Home","title":"SimpleSolvers.compute_jacobian!","text":"compute_jacobian!(j, x, jacobian::Jacobian)\n\nApply the Jacobian and store the result in j.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_jacobian!-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, Union{Function, Type}}} where T","page":"Home","title":"SimpleSolvers.compute_jacobian!","text":"compute_jacobian!(j, x, ForJ)\n\nAllocate a Jacobian object, apply it to x, and store the result in j.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_new_iterate-Union{Tuple{TVT}, Tuple{VT}, Tuple{T}, Tuple{VT, T, TVT}} where {T, VT, TVT}","page":"Home","title":"SimpleSolvers.compute_new_iterate","text":"compute_new_iterate(xₖ, αₖ, pₖ)\n\nCompute xₖ₊₁ based on a direction pₖ and a step length αₖ.\n\nExtended help\n\nIn the case of vector spaces this function simply does:\n\nxₖ + αₖ * pₖ\n\nFor manifolds we instead perform a retraction [5].\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.default_tolerance-Union{Tuple{Type{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"SimpleSolvers.default_tolerance","text":"default_tolerance(T)\n\nDetermine the default tolerance for a specific data type. This is used in the constructor of Options.\n\nExamples\n\ndefault_tolerance(Float64)\n\n# output\n\n4.440892098500626e-16\n\ndefault_tolerance(Float32)\n\n# output\n\n2.3841858f-7\n\ndefault_tolerance(Float16)\n\n# output\n\nFloat16(0.001953)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.derivative!!-Tuple{UnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.derivative!!","text":"derivative!!(obj::AbstractObjective, x)\n\nSimilar to value!!, but fo the derivative part (see UnivariateObjective and TemporaryUnivariateObjective).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.derivative!-Tuple{UnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.derivative!","text":"derivative!(obj, x)\n\nSimilar to value!, but fo the derivative part (see UnivariateObjective).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.derivative-Tuple{UnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.derivative","text":"derivative(obj::AbstractObjective, x)\n\nSimilar to value, but for the derivative part (see UnivariateObjective).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.determine_initial_α-Union{Tuple{T}, Tuple{SimpleSolvers.AbstractUnivariateObjective, T}, Tuple{SimpleSolvers.AbstractUnivariateObjective, T, T}, Tuple{SimpleSolvers.AbstractUnivariateObjective, T, T, T}} where T","page":"Home","title":"SimpleSolvers.determine_initial_α","text":"determine_initial_α(y₀, obj, α₀)\n\nCheck whether α₀ satisfies the BracketMinimumCriterion for obj. If the criterion is not satisfied we call bracket_minimum_with_fixed_point. This is used as a starting point for using the functor of QuadraticState and makes sure that α describes a point past the minimum.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.direction-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.direction","text":"direction(::NewtonOptimizerCache)\n\nReturn the direction of the gradient step (i.e. δ) of an instance of NewtonOptimizerCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.f_argument-Tuple{NonlinearSystem}","page":"Home","title":"SimpleSolvers.f_argument","text":"f_argument(nls)\n\nReturn the argument that was last used for evaluating value! for the NonlinearSystem nls.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.f_calls-Tuple{NonlinearSystem}","page":"Home","title":"SimpleSolvers.f_calls","text":"f_calls(nls)\n\nTell how many times Function(nls) has been called.\n\nExamples\n\nF(x) = tanh.(x)\nx = [1., 2., 3.]\nnls = NonlinearSystem(F, x)\n\nf_calls(nls)\n\n# output\n\n0\n\nAfter calling value once we get:\n\nvalue!(nls, x)\n\nf_calls(nls)\n\n# output\n\n1\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.factorize!-Tuple{LinearSolver}","page":"Home","title":"SimpleSolvers.factorize!","text":"factorize!(lsolver)\n\nFactorize the matrix stored in the LinearSolverCache in lsolver.\n\nSee factorize!(::LinearSolver{T, LUT}) where {T, LUT <: LU} for a concrete example.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.factorize!-Union{Tuple{LinearSolver{T, LUT}}, Tuple{LUT}, Tuple{T}} where {T, LUT<:LU}","page":"Home","title":"SimpleSolvers.factorize!","text":"factorize!(lsolver::LinearSolver, A)\n\nFactorize the matrix A and store the result in cache(lsolver).A. Note that calling cache on lsolver returns the instance of LUSolverCache stored in lsolver.\n\nExamples\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\ny = [1., 0., 0.]\nx = similar(y)\n\nlsolver = LinearSolver(LU(; static=false), x)\nfactorize!(lsolver, A)\ncache(lsolver).A\n\n# output\n\n3×3 Matrix{Float64}:\n 13.0        17.0       19.0\n  0.0769231   0.692308   1.53846\n  0.384615    0.666667   2.66667\n\nHere cache(lsolver).A stores the factorized matrix. If we call factorize! with two input arguments as above, the method first copies the matrix A into the LUSolverCache. We can equivalently also do:\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\ny = [1., 0., 0.]\n\nlsolver = LinearSolver(LU(), A)\nfactorize!(lsolver)\ncache(lsolver).A\n\n# output\n\n3×3 StaticArraysCore.MMatrix{3, 3, Float64, 9} with indices SOneTo(3)×SOneTo(3):\n 13.0        17.0       19.0\n  0.0769231   0.692308   1.53846\n  0.384615    0.666667   2.66667\n\nAlso note the difference between the output types of the two refactorized matrices. This is because we set the keyword static to false when calling LU. Also see _static.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.find_maximum_value-Union{Tuple{T}, Tuple{AbstractVector{T}, Integer}} where T<:Number","page":"Home","title":"SimpleSolvers.find_maximum_value","text":"find_maximum_value(v, k)\n\nFind the maximum value of vector v starting from the index k.  This is used for pivoting in factorize!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient!!-Tuple{MultivariateObjective, AbstractArray{<:Number}}","page":"Home","title":"SimpleSolvers.gradient!!","text":"gradient!!(obj::MultivariateObjective, x)\n\nLike derivative!!, but for MultivariateObjective, not UnivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient!-Tuple{AbstractVector, AbstractVector, Gradient}","page":"Home","title":"SimpleSolvers.gradient!","text":"gradient!(g, x, grad)\n\nApply the Gradient grad to x and store the result in g.\n\nImplementation\n\nThis is equivalent to doing\n\ngrad(g, x)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient!-Tuple{MultivariateObjective, AbstractArray{<:Number}}","page":"Home","title":"SimpleSolvers.gradient!","text":"gradient!(obj::MultivariateObjective, x)\n\nLike derivative!, but for MultivariateObjective, not UnivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{Any, Gradient}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(x, grad)\n\nApply grad to x and return the result. \n\nImplementation\n\nInternally this is using gradient!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{MultivariateObjective}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(x, obj::MultivariateObjective)\n\nLike derivative, but for MultivariateObjective, not UnivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(::NewtonOptimizerCache)\n\nReturn the stored gradient (array) of an instance of NewtonOptimizerCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient_ad!-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}, Union{Function, Type}}} where T<:Number","page":"Home","title":"SimpleSolvers.gradient_ad!","text":"gradient_ad!(g, x, F)\n\nBuild a GradientAutodiff object based on F and apply it to x. The result is stored in g.\n\nAlso see gradient_fd! for the finite differences version.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient_fd!-Union{Tuple{FT}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}, FT}} where {T, FT}","page":"Home","title":"SimpleSolvers.gradient_fd!","text":"gradient_fd!(g, x, F)\n\nBuild a GradientFiniteDifferences object based on F and apply it to x. The result is stored in g.\n\nAlso see gradient_ad! for the autodiff version.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.increase_iteration_number!-Tuple{SimpleSolvers.NonlinearSolverStatus}","page":"Home","title":"SimpleSolvers.increase_iteration_number!","text":"increase_iteration_number!(status)\n\nIncrease iteration number of status.\n\nExamples\n\nstatus = NonlinearSolverStatus{Float64}(5)\nincrease_iteration_number!(status)\niteration_number(status)\n\n# output\n\n1\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.increase_iteration_number!-Tuple{SimpleSolvers.OptimizerResult}","page":"Home","title":"SimpleSolvers.increase_iteration_number!","text":"increase_iteration_number!(result)\n\nIncrease the iteration number of resultOptimizerResult. This calls increase_iteration_number!(::OptimizerStatus).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.increase_iteration_number!-Tuple{SimpleSolvers.OptimizerStatus}","page":"Home","title":"SimpleSolvers.increase_iteration_number!","text":"increase_iteration_number!(status)\n\nIncrease the iteration number of a statusOptimizerStatus. See increase_iteration_number!(::NonlinearSolverStatus).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{Hessian, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(hessian, x)\n\nSee e.g. initialize!(::HessianAutodiff, ::AbstractVector).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{HessianAutodiff, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(H, x)\n\nInitialize a HessianAutodiff object H.\n\nImplementation\n\nInternally this is calling the HessianAutodiff functor and therefore also ForwardDiff.hessian!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{LinearSystem, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(ls, x)\n\nInitialize the LinearSystem ls. See clear!(::LinearSystem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{SimpleSolvers.NonlinearSolverStatus, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(status, x)\n\nClear status::NonlinearSolverStatus (via the function clear!).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Union{Tuple{T}, Tuple{SimpleSolvers.NewtonSolverCache{T, AT} where AT<:AbstractVector{T}, AbstractVector{T}}} where T","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(cache, x)\n\nInitialize the NewtonSolverCache based on x.\n\nImplementation\n\nThis calls alloc_x to do all the initialization.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.isaOptimizationAlgorithm-Tuple{Any}","page":"Home","title":"SimpleSolvers.isaOptimizationAlgorithm","text":"isaOptimizationAlgorithm(alg)\n\nVerify if an object implements the OptimizationAlgorithm interface.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.j_argument-Tuple{NonlinearSystem}","page":"Home","title":"SimpleSolvers.j_argument","text":"j_argument(nls)\n\nReturn the argument that was last used for evaluating jacobian! for the NonlinearSystem nls.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.j_calls-Tuple{NonlinearSystem}","page":"Home","title":"SimpleSolvers.j_calls","text":"j_calls(nls)\n\nLike f_calls in relation to a NonlinearSystem nls, but for jacobian (or jacobian!).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.jacobian!!-Union{Tuple{T}, Tuple{NonlinearSystem{T, TF, TJ, Tx, Tf, Tj} where {TF<:Union{Function, Type}, TJ<:Jacobian{T}, Tx<:AbstractVector{T}, Tf<:AbstractVector{T}, Tj<:AbstractMatrix{T}}, AbstractArray{T}}} where T","page":"Home","title":"SimpleSolvers.jacobian!!","text":"jacobian!!(nls::NonlinearSystem, x)\n\nForce the evaluation of the jacobian for a NonlinearSystem. Like derivative!! for UnivariateObjective or gradient!! for MultivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.jacobian!-Union{Tuple{T}, Tuple{NonlinearSystem{T, TF, TJ, Tx, Tf, Tj} where {TF<:Union{Function, Type}, TJ<:Jacobian{T}, Tx<:AbstractVector{T}, Tf<:AbstractVector{T}, Tj<:AbstractMatrix{T}}, AbstractArray{T}}} where T<:Number","page":"Home","title":"SimpleSolvers.jacobian!","text":"jacobian!(nls::NonlinearSystem, x)\n\nCompute the Jacobian of nls at x and store it in jacobian(nls). Note that the evaluation of the Jacobian is not necessarily enforced here (unlike calling jacobian!!). Like derivative! for MultivariateObjective and gradient! for UnivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.jacobian-Tuple{NewtonSolver}","page":"Home","title":"SimpleSolvers.jacobian","text":"jacobian(solver::NewtonSolver)\n\nReturn the evaluated Jacobian (a Matrix) stored in the NonlinearSystem of solver.\n\nAlso see jacobian(::NonlinearSystem) and Jacobian(::NonlinearSystem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.jacobian-Tuple{NonlinearSystem}","page":"Home","title":"SimpleSolvers.jacobian","text":"jacobian(nls::NonlinearSystem)\n\nReturn the value of the jacobian stored in nls (instance of NonlinearSystem). Like derivative for UnivariateObjective or gradient for MultivariateObjective.\n\nAlso see Jacobian(::NonlinearSystem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linearsolver-Tuple{NewtonSolver}","page":"Home","title":"SimpleSolvers.linearsolver","text":"linearsolver(solver)\n\nReturn the linear part (i.e. a LinearSolver) of an NewtonSolver.\n\nExamples\n\nx = rand(3)\ny = rand(3)\nF(x) = tanh.(x)\ns = NewtonSolver(x, y; F = F)\nlinearsolver(s)\n\n# output\n\nLinearSolver{Float64, LU{Missing}, SimpleSolvers.LUSolverCache{Float64, StaticArraysCore.MMatrix{3, 3, Float64, 9}}}(LU{Missing}(missing, true), SimpleSolvers.LUSolverCache{Float64, StaticArraysCore.MMatrix{3, 3, Float64, 9}}([0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0], [0, 0, 0], [0, 0, 0], 0))\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_objective-Tuple{NonlinearSolver}","page":"Home","title":"SimpleSolvers.linesearch_objective","text":"linesearch_objective(nl::NonlinearSolver)\n\nBuild a line search objective based on a NonlinearSolver (almost always a NewtonSolver in practice).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_objective-Union{Tuple{T}, Tuple{MultivariateObjective{T, Tx, TF, TG} where {Tx<:AbstractVector{T}, TF<:Union{Function, Type}, TG<:Gradient{T}}, SimpleSolvers.NewtonOptimizerCache{T, AT} where AT<:(AbstractArray{T})}} where T","page":"Home","title":"SimpleSolvers.linesearch_objective","text":"linesearch_objective(objective, cache)\n\nCreate TemporaryUnivariateObjective for linesearch algorithm. The variable on which this objective depends is alpha.\n\nExample\n\nx = [1, 0., 0.]\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\ngradient!(obj, x)\nvalue!(obj, x)\ncache = NewtonOptimizerCache(x)\nhess = Hessian(obj, x; mode = :autodiff)\nupdate!(hess, x)\nupdate!(cache, x, obj.g, hess)\nx₂ = [.9, 0., 0.]\ngradient!(obj, x₂)\nvalue!(obj, x₂)\nupdate!(hess, x₂)\nupdate!(cache, x₂, obj.g, hess)\nls_obj = linesearch_objective(obj, cache)\nα = .1\n(ls_obj.F(α), ls_obj.D(α))\n\n# output\n\n(0.4412947468016475, -0.8083161485821551)\n\nIn the example above we have to apply update! twice on the instance of NewtonOptimizerCache because it needs to store the current and the previous iterate.\n\nImplementation\n\nCalling the function and derivative stored in the TemporaryUnivariateObjective created with linesearch_objective does not allocate a new array, but uses the one stored in the instance of NewtonOptimizerCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_objective-Union{Tuple{T}, Tuple{NonlinearSystem{T, TF, TJ, Tx, Tf, Tj} where {TF<:Union{Function, Type}, TJ<:Jacobian{T}, Tx<:AbstractVector{T}, Tf<:AbstractVector{T}, Tj<:AbstractMatrix{T}}, SimpleSolvers.NewtonSolverCache{T, AT} where AT<:AbstractVector{T}}} where T","page":"Home","title":"SimpleSolvers.linesearch_objective","text":"linesearch_objective(nls, cache)\n\nMake a line search objective for a Newton solver (the cache here is an instance of NewtonSolverCache).\n\nImplementation\n\ninfo: Producing a single-valued output\nDifferent from the linesearch_objective for NewtonOptimizerCaches, we apply l2norm to the output of objective!. This is because the solver operates on an objective with multiple outputs from which we have to find roots, whereas an optimizer operates on an objective with a single output of which we should find a minimum.\n\nAlso see linesearch_objective(::MultivariateObjective{T}, ::NewtonOptimizerCache{T}) where {T}.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.meets_stopping_criteria-Tuple{SimpleSolvers.NonlinearSolverStatus, Options}","page":"Home","title":"SimpleSolvers.meets_stopping_criteria","text":"meets_stopping_criteria(status, config)\n\nDetermines whether the iteration stops based on the current NonlinearSolverStatus.\n\nwarn: Warn\nThe function meets_stopping_criteria may return true even if the solver has not converged. To check convergence, call assess_convergence! (with the same input arguments).\n\nThe function meets_stopping_criteria returns true if one of the following is satisfied:\n\nthe status::NonlinearSolverStatus is converged (checked with assess_convergence!) and iteration_number(status) ≥ config.min_iterations,\nstatus.f_increased and config.allow_f_increases = false (i.e. f increased even though we do not allow it),\niteration_number(status) ≥ config.max_iterations, \nif any component in solution(status) is NaN,\nif any component in status.f is NaN,\nstatus.rxₐ > config.x_abstol_break (by default Inf. In theory this returns true if the residual gets too big,\nstatus.rfₐ > config.f_abstol_break (by default Inf. In theory this returns true if the residual gets too big,\n\nSo convergence is only one possible criterion for which meets_stopping_criteria. We may also satisfy a stopping criterion without having convergence!\n\nExamples\n\nIn the following example we show that meets_stopping_criteria evaluates to true when used on a freshly allocated NonlinearSolverStatus:\n\nstatus = NonlinearSolverStatus{Float64}(5)\nconfig = Options()\nmeets_stopping_criteria(status, config)\n\n# output\n\ntrue\n\nThis obviously has not converged. To check convergence we can use assess_convergence!:\n\nstatus = NonlinearSolverStatus{Float64}(5)\nconfig = Options()\nassess_convergence!(status, config)\n\n# output\n\nfalse\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.meets_stopping_criteria-Tuple{SimpleSolvers.OptimizerStatus, Options}","page":"Home","title":"SimpleSolvers.meets_stopping_criteria","text":"meets_stopping_criteria(status, config)\n\nCheck if the optimizer has converged.\n\nImplementation\n\nmeets_stopping_criteria first calls assess_convergence! and then checks if one of the following is true:\n\nconverged (the output of assess_convergence!) is true and status.i geq config.min_iterations,\nif config.allow_f_increases is false: status.f_increased is true,\nstatus.i geq config.max_iterations,\nstatus.rxₐ  config.x_abstol_break\nstatus.rxᵣ  config.x_reltol_break\nstatus.rfₐ  config.f_abstol_break\nstatus.rfᵣ  config.f_reltol_break\nstatus.rg   config.g_restol_break\nstatus.x_isnan\nstatus.f_isnan\nstatus.g_isnan\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.method-Tuple{LinearSolver}","page":"Home","title":"SimpleSolvers.method","text":"method(ls)\n\nReturn the method (of type LinearSolverMethod) of the LinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.minimum_decrease_threshold-Union{Tuple{Type{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"SimpleSolvers.minimum_decrease_threshold","text":"minimum_decrease_threshold(T)\n\nThe minimum value by which a function f should decrease during an iteration.\n\nThe default value of 10^-4 is often used in the literature [bierlaire2015optimization], nocedal2006numerical(@cite).\n\nExamples\n\nminimum_decrease_threshold(Float64)\n\n# output\n\n0.0001\n\nminimum_decrease_threshold(Float32)\n\n# output\n\n0.0001f0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.next_iteration!-Tuple{SimpleSolvers.NonlinearSolverStatus}","page":"Home","title":"SimpleSolvers.next_iteration!","text":"next_iteration!(status)\n\nCall increase_iteration_number!, set x̄ and f̄ to x and f respectively and δ as well as γ to 0.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.nonlinearsystem-Tuple{NewtonSolver}","page":"Home","title":"SimpleSolvers.nonlinearsystem","text":"nonlinearsystem(solver)\n\nReturn the NonlinearSystem contained in the NewtonSolver. Compare this to linearsolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.print_status-Tuple{SimpleSolvers.NonlinearSolverStatus, Options}","page":"Home","title":"SimpleSolvers.print_status","text":"print_status(status, config)\n\nPrint the solver status if:\n\nThe following three are satisfied: (i) config.verbosity geq1 (ii) assess_convergence!(status, config) is false (iii) iteration_number(status) > config.max_iterations\nconfig.verbosity > 1.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.residual!-Tuple{SimpleSolvers.NonlinearSolverStatus}","page":"Home","title":"SimpleSolvers.residual!","text":"residual!(status)\n\nCompute the residuals for status::NonlinearSolverStatus. Note that this does not update x, f, δ or γ. These are updated with update!(::NonlinearSolverStatus, ::AbstractVector, ::NonlinearSystem). The computed residuals are the following:\n\nrxₐ: absolute residual in x,\nrxₛ : successive residual (the norm of delta),\nrfₐ: absolute residual in f,\nrfₛ : successive residual (the norm of gamma).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.residual!-Union{Tuple{GT}, Tuple{FT}, Tuple{XT}, Tuple{OS}, Tuple{OS, XT, XT, FT, FT, GT, GT}} where {OS<:SimpleSolvers.OptimizerStatus, XT, FT, GT}","page":"Home","title":"SimpleSolvers.residual!","text":"residual!(status, x, x̄, f, f̄, g, ḡ)\n\nCompute the residual based on previous iterates (x̄, f̄, ḡ) and current iterates (x, f, g).\n\nAlso see assess_convergence! and meets_stopping_criteria.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.residual!-Union{Tuple{OR}, Tuple{OST}, Tuple{YT}, Tuple{VT}, Tuple{XT}, Tuple{OR, VT, YT, VT}} where {XT, VT<:(AbstractArray{XT}), YT, OST<:SimpleSolvers.OptimizerStatus{XT, YT}, OR<:SimpleSolvers.OptimizerResult{XT, YT, VT, OST}}","page":"Home","title":"SimpleSolvers.residual!","text":"residual!(result, x, f, g)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.rhs-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.rhs","text":"rhs(cache)\n\nReturn the right hand side of an instance of NewtonOptimizerCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.rhs-Tuple{SimpleSolvers.NewtonSolverCache}","page":"Home","title":"SimpleSolvers.rhs","text":"rhs(cache)\n\nReturn the right-hand side of the equation, stored in cache::NewtonSolverCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.shift_χ_to_avoid_stalling-Union{Tuple{T}, NTuple{5, T}} where T","page":"Home","title":"SimpleSolvers.shift_χ_to_avoid_stalling","text":"shift_χ_to_avoid_stalling(χ, a, b, c, ε)\n\nCheck whether b is closer to a or c and shift χ accordingly.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solution-Tuple{SimpleSolvers.NonlinearSolverStatus}","page":"Home","title":"SimpleSolvers.solution","text":"solution(status)\n\nReturn the current value of x (i.e. the current solution).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, LinearSolver, AbstractMatrix, AbstractVector}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, ls::LinearSolver, A, b)\n\nSolve the linear system described by:\n\n    Ax = b\n\nand store it in x. Here A and b are provided as an input arguments.\n\nimplementation\n\nNote that, compared to solve(::LinearSolver, ::AbstractVector) this method involves an additional factorization of A.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, LinearSolver, AbstractVector}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, ls::LinearSolver, b)\n\nSolve the linear system described by:\n\n    Ax = b\n\nand store it in x. Here b is provided as an input argument and the factorized A is stored in the LinearSolver ls (respectively its LinearSolverCache).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, LinearSolver, LinearSystem}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, ls::LinearSolver, lsys::LinearSystem)\n\nSolve the LinearSystem lsys with the LinearSolver ls and store the result in x. Also see solve!(::LinearSolver, ::LinearSystem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{LinearSolver, Vararg{Any}}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(ls::LinearSolver, args...)\n\nSolve the LinearSystem with the LinearSolver ls.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{NewtonSolver, AbstractArray}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(s, x)\n\nExtended help\n\ninfo: Info\nThe function update! calls next_iteration!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{Optimizer, AbstractVector}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, opt)\n\nSolve the optimization problem described by opt::Optimizer and store the result in x.\n\nf(x) = sum(x .^ 2 + x .^ 3 / 3)\nx = [1f0, 2f0]\nopt = Optimizer(x, f; algorithm = Newton())\n\nsolve!(opt, x)\n\n# output\n2-element Vector{Float32}:\n 4.6478817f-8\n 3.0517578f-5\n\nWe can also check how many iterations it took:\n\niteration_number(opt)\n\n# output\n\n12\n\nToo see the value of x after one iteration confer the docstring of solver_step!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve-Tuple{LinearSystem, SimpleSolvers.LinearSolverMethod}","page":"Home","title":"SimpleSolvers.solve","text":"solve(ls, method)\n\nSolve the LinearSystem ls with the LinearSolverMethod method.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solver_step!-Union{Tuple{T}, Tuple{NewtonSolver, AbstractVector{T}}} where T","page":"Home","title":"SimpleSolvers.solver_step!","text":"solver_step!(s, x)\n\nCompute one Newton step for f based on the Jacobian jacobian!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solver_step!-Union{Tuple{VT}, Tuple{Optimizer, VT}} where VT<:(AbstractVector)","page":"Home","title":"SimpleSolvers.solver_step!","text":"solver_step!(x, state)\n\nCompute a full iterate for an instance of NewtonOptimizerState state.\n\nThis also performs a line search.\n\nExamples\n\nf(x) = sum(x .^ 2 + x .^ 3 / 3)\nx = [1f0, 2f0]\nopt = Optimizer(x, f; algorithm = Newton())\n\nsolver_step!(opt, x)\n\n# output\n\n2-element Vector{Float32}:\n 0.25\n 0.6666666\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.triple_point_finder-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T","page":"Home","title":"SimpleSolvers.triple_point_finder","text":"triple_point_finder(f, x)\n\nFind three points a > b > c s.t. f(a) > f(b) and f(c) > f(b). This is used for performing a quadratic line search (see QuadraticState).\n\nImplementation\n\nFor δ we take DEFAULT_BRACKETING_s as default. For nmax we take [DEFAULTBRACKETINGnmax`](@ref) as default.\n\nExtended help\n\nThe algorithm is taken from [2, Chapter 11.2.1].\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.value!!-Tuple{SimpleSolvers.AbstractUnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.value!!","text":"value!!(obj::AbstractObjective, x)\n\nSet obj.x_f to x and obj.f to value(obj, x) and return value(obj).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.value!-Tuple{SimpleSolvers.AbstractObjective, Union{Number, AbstractArray{<:Number}}}","page":"Home","title":"SimpleSolvers.value!","text":"value!(obj::AbstractObjective, x)\n\nCheck if x is not equal to obj.x_f and then apply value!!. Else simply return value(obj).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.value!-Union{Tuple{T}, Tuple{NonlinearSystem{T, TF, TJ, Tx, Tf, Tj} where {TF<:Union{Function, Type}, TJ<:Jacobian{T}, Tx<:AbstractVector{T}, Tf<:AbstractVector{T}, Tj<:AbstractMatrix{T}}, AbstractVector{T}}} where T<:Number","page":"Home","title":"SimpleSolvers.value!","text":"value!(nls::NonlinearSystem, x)\n\nCheck if x is not equal to f_argument(nls) and then apply value!!. Else simply return value(nls).\n\n\n\n\n\n","category":"method"},{"location":"initialize/#Initialization","page":"Initialization","title":"Initialization","text":"","category":"section"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"Before we can use update! we have to initialize with SimpleSolvers.initialize![1].","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"[1]: The different methods for SimpleSolvers.initialize! are however often called with the constructor of a struct (e.g. for SimpleSolvers.NewtonOptimizerCache).","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"Similar to update!, SimpleSolvers.initialize! returns the first input argument as output. Examples include:","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"SimpleSolvers.initialize!(::Hessian, ::AbstractVector): this routine exists for most Hessians, i.e. for HessianFunction, HessianAutodiff, HessianBFGS and HessianDFP,\nSimpleSolvers.initialize!(::SimpleSolvers.NewtonSolverCache, ::AbstractVector),\nSimpleSolvers.initialize!(::SimpleSolvers.NonlinearSolverStatus, ::AbstractVector, ::Base.Callable),\nSimpleSolvers.initialize!(::SimpleSolvers.NewtonOptimizerCache, ::AbstractVector).","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"We demonstrate these examples in code. First for an instance of SimpleSolvers.Hessian:","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"using SimpleSolvers # hide\nusing SimpleSolvers: initialize! # hide\nusing LinearAlgebra: norm\nimport Random # hide\nRandom.seed!(123) # hide\n\nx = rand(3)\nobj = MultivariateObjective(x -> norm(x - vcat(0., 0., 1.)) ^ 2, x)\nbt = Backtracking() # hide\nalg = Newton() # hide\n# opt = Optimizer(x, obj; algorithm = alg, linesearch = bt, config = config) # hide\n\nhes = Hessian(obj, x; mode = :autodiff)\ninitialize!(hes, x)\nhes.H","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"For an instance of SimpleSolvers.NewtonOptimizerCache[2]:","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"[2]: Here we remark that SimpleSolvers.NewtonOptimizerCache has five keys: x, x̄, δ, g and rhs. All of them are initialized with NaNs except for x. We also remark that the constructor, which is called by providing a single vector x as input argument, internally also calls SimpleSolvers.initialize!.","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"cache = SimpleSolvers.NewtonOptimizerCache(x)\ninitialize!(cache, x)\ncache.g","category":"page"},{"location":"initialize/#Clear-Routines","page":"Initialization","title":"Clear Routines","text":"","category":"section"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"For SimpleSolvers.OptimizerResult and SimpleSolvers.OptimizerStatus the SimpleSolvers.clear! routines are used instead of the SimpleSolvers.initialize! routines.","category":"page"},{"location":"initialize/#Reasoning-behind-Initialization-with-NaNs","page":"Initialization","title":"Reasoning behind Initialization with NaNs","text":"","category":"section"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"We initialize with NaNs instead of with zeros (or other values) as this clearly divides the initialization from the numerical operations (which are done with update!).","category":"page"},{"location":"initialize/#Alloc-Functions","page":"Initialization","title":"Alloc Functions","text":"","category":"section"}]
}
