var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"C. T. Kelley. Iterative methods for linear and nonlinear equations (SIAM, 1995).\n\n\n\nM. Bierlaire. Optimization: principles and algorithms (EPFL press, 2015).\n\n\n\nJ. Nocedal and S. J. Wright. Numerical optimization (Springer, New York, NY, 2006). Second Edition.\n\n\n\nM. J. Kochenderfer and T. A. Wheeler. Algorithms for optimization (Mit Press, 2019).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\n","category":"section"},{"location":"linesearch/bierlaire_quadratic/#Bierlaire-Quadratic-Line-Search","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic Line Search","text":"In [2] quadratic line search is defined as an interpolation between three points. For this consider\n\nusing SimpleSolvers\nusing SimpleSolvers: factorize!, linearsolver, jacobian, jacobian!, cache, linesearch_problem, direction, NullParameters, NonlinearSolverState, jacobianmatrix # hide\nusing LinearAlgebra: rmul!, ldiv! # hide\nusing Random # hide\nRandom.seed!(1234) # hide\n\nf(x::T) where {T<:Number} = exp(x) * (T(.5) * x ^ 3 - 5x ^ 2 + 2x) + 2one(T)\nf(x::AbstractArray{T}) where {T<:Number} = exp.(x) .* (T(.5) * (x .^ 3) - 5 * (x .^ 2) + 2x) .+ 2one(T)\nf!(y::AbstractVector{T}, x::AbstractVector{T}) where {T} = y .= f.(x)\nj!(j::AbstractMatrix{T}, x::AbstractVector{T}) where {T} = SimpleSolvers.ForwardDiff.jacobian!(j, f!, similar(x), x)\nF!(y, x, params) = f!(y, x)\nJ!(j, x, params) = j!(j, x)\nx = -10 * rand(1)\nsolver = NewtonSolver(x, f.(x); F = F!, DF! = J!)\nparams = NullParameters()\nstate = NonlinearSolverState(x)\nupdate!(state, x, f(x))\njacobian!(solver, x, params)\n\n# compute rhs\nf!(cache(solver).rhs, x)\nrmul!(cache(solver).rhs, -1)\n\n# multiply rhs with jacobian\nfactorize!(linearsolver(solver), jacobianmatrix(solver))\nldiv!(direction(cache(solver)), linearsolver(solver), cache(solver).rhs)\n\nnlp = NonlinearProblem(F!, x, f(x))\nls_obj = linesearch_problem(nlp, jacobian(solver), cache(solver), x, params)\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide\n\nFor the Bierlaire quadratic line search we need three points: a, b and c:\n\na, b, c = -2., 0.5, 2.5\nnothing # hide\n\n(Image: )\n\nIn the figure above we already plotted three points a, b and c on whose basis a second-order polynomial will be built that should approximate f^mathrmls.[1] The polynomial is built with the ansatz:\n\n[1]: These points further need to satisfy f^mathrmls(a)  f^mathrmls(b)  f^mathrmls(c).\n\np(alpha) = beta_1(alpha - a)(x - b) + beta_2(alpha - a) + beta_3(alpha - b)\n\nand by identifying \n\nbeginaligned\np(a)  = f^mathrmls(a) \np(b)  = f^mathrmls(b) \np(c)  = f^mathrmls(c) \nendaligned\n\nwe get\n\nbeginaligned\nbeta_1  = frac(b - c)f^mathrmls(a) + (c - a)f^mathrmls(b) + (a - b)f^mathrmls(c)(a - b)(c - a)(c - b)  \nbeta_2  = fracf^mathrmls(b)b - a \nbeta_3  = fracf^mathrmls(a)a - b\nendaligned\n\nWe can plot this polynomial:\n\n(Image: )\n\nWe can now easily determine the minimum of the polynomial p. It is:\n\nchi = frac12 frac f^mathrmls(a) (b^2 - c^2) + f^mathrmls(b) (c^2 - a^2) + f^mathrmls(c) (a^2 - b^2) f^mathrmls(a) (b - c) + f^mathrmls(b) (c - a) + f^mathrmls(c) (a - b)\n\n(Image: )\n\nWe now use this chi to either replace a, b or c and distinguish between the following four scenarios:\n\nchi  b and f^mathrmls(chi)  f^mathrmls(b) implies we replace c gets chi,\nchi  b and f^mathrmls(chi) leq f^mathrmls(b) implies we replace a b gets b chi,\nchi leq b and f^mathrmls(chi)  f^mathrmls(b) implies we replace a gets chi,\nchi leq b and f^mathrmls(chi) leq f^mathrmls(b) implies we replace b c gets chi b.\n\nIn our example we have the second case: chi is to the right of b and f^mathrmls(chi) is smaller than f(b). We therefore replace a with b and b with chi. The new approximation is the following one:\n\n(Image: )\n\nWe again observe the second case. By replacing a b gets b chi we get:\n\n(Image: )\n\nWe now observe the first case: chi is to the left of b and f^mathrmls(chi) is above f(b). Hence we replace b c gets chi b A successive iteration yields:\n\n(Image: )\n\ninfo: Info\nAfter having computed chi we further either shift it to the left or right depending on whether (c - b) or (b - a) is bigger respectively. The shift is made by either adding or subtracting the constant varepsilon.\n\nAlso see SimpleSolvers.DEFAULT_BIERLAIRE_ε.","category":"section"},{"location":"linesearch/curvature_condition/#The-Curvature-Condition","page":"The Curvature Condition","title":"The Curvature Condition","text":"The curvature condition is used together with the sufficient decrease condition and ensures that step sizes are not chosen too small (which might happen if we only use the sufficient decrease condition). The sufficient decrease condition and the curvature condition together are called the Wolfe conditions.","category":"section"},{"location":"linesearch/curvature_condition/#Standard-Curvature-Condition","page":"The Curvature Condition","title":"Standard Curvature Condition","text":"For the standard curvature condition (see SimpleSolvers.CurvatureCondition) we have:\n\n    fracpartialpartialalphaBigg_alpha=alpha_kf(R_x_k(alphap_k)) = g(mathrmgrad_R_x_k(alpha_kp_k)f p_k) geq c_2g(mathrmgrad_x_kf p_k) = c_2fracpartialpartialalphaBigg_alpha=0f(R_x_k(alphap_k))\n\nfor c_2in(c_1 1) In words this means that the derivative with respect to alpha_k should be bigger at the new iterate x_k+1 than at the old iterate x_k. ","category":"section"},{"location":"linesearch/curvature_condition/#Strong-Curvature-Condition","page":"The Curvature Condition","title":"Strong Curvature Condition","text":"For the strong curvature condition[1] we replace the curvature condition by:\n\n[1]: We consequently also speak of the strong Wolfe conditions when taking the strong curvature condition and the sufficient decrease condition together.\n\n    g(mathrmgrad_R_x_k(alpha_kp_k)f p_k)  c_2g(mathrmgrad_x_kf p_k)\n\nNote the sign change here. This is because the term g(mathrmgrad_x_kf p_k) is negative if p_k is a search direction. Both the standard curvature condition and the strong curvature condition are implemented under SimpleSolvers.CurvatureCondition.\n\ninfo: Info\nIn order to use the corresponding condition you have to either pass mode = :Standard or mode = :Strong to the constructor of CurvatureCondition.","category":"section"},{"location":"linesearch/curvature_condition/#Example","page":"The Curvature Condition","title":"Example","text":"We use the same example that we had when we explained the sufficient decrease condition:\n\nusing SimpleSolvers # hide\nusing SimpleSolvers: CurvatureCondition, NewtonOptimizerCache, update!, linesearch_problem, ldiv! # hide\n\nx = [3., 1.3]\nf = x -> 10 * sum(x .^ 3 / 6 - x .^ 2 / 2)\nobj = OptimizerProblem(f, x)\nhes = HessianAutodiff(obj, x)\nH = SimpleSolvers.alloc_h(x)\nhes(H, x)\n\nc₂ = .9\ng = similar(x)\ngrad = GradientAutodiff{Float64}(obj.F, length(x))\nrhs = -g\n# the search direction is determined by multiplying the right hand side with the inverse of the Hessian from the left.\np = similar(rhs)\np .= H \\ rhs\ncc = CurvatureCondition(c₂, x, g, p, obj, grad)\n\n# check different values\nα₁, α₂, α₃, α₄, α₅ = .09, .4, 0.7, 1., 1.3\n(cc(α₁), cc(α₂), cc(α₃), cc(α₄), cc(α₅))","category":"section"},{"location":"jacobians/#Jacobians","page":"Jacobians","title":"Jacobians","text":"The supertype Jacobian comprises different ways of taking Jacobians:\n\nJacobianFunction,\nJacobianAutodiff,\nJacobianFiniteDifferences.\n\nWe first start by showing JacobianAutodiff:\n\nusing SimpleSolvers, Random; using SimpleSolvers: JacobianAutodiff, Jacobian, JacobianFunction, JacobianFiniteDifferences; Random.seed!(123) # hide\n# the input and output dimensions of this function are the same\nF(y::AbstractArray, x::AbstractArray, params) = y .= tanh.(x)\ndim = 3\nx = rand(dim)\njac = JacobianAutodiff{eltype(x)}(F, dim)\n\nAnd the functor:\n\nj = zeros(3, 3)\njac(j, x, nothing)","category":"section"},{"location":"linear/linear_solvers/#Linear-Solvers","page":"Linear Solvers","title":"Linear Solvers","text":"Objects of type LinearSolver are used to solve LinearProblems, i.e. we want to find x for given A and y such that\n\n    Ax = y\n\nis satisfied. \n\nA linear system can be called with[1]:\n\n[1]: Here we also have to update the LinearProblem by calling update!. For more information see the page on the initialize! function.\n\nusing SimpleSolvers\n\nA = [(0. + 1e-6) 1. 2.; 3. 4. 5.; 6. 7. 8.]\ny = [1., 2., 3.]\nls = LinearProblem(A, y)\nupdate!(ls, A, y)\nnothing # hide\n\nNote that we here use the matrix:\n\nA = beginpmatrix 0 + varepsilon  1  2  3  4  5  6  7  8 endpmatrix\n\nThis matrix would be singular if we had varepsilon = 0 because 2cdotbeginpmatrix 3  4  5 endpmatrix - beginpmatrix 6  7  8 endpmatrix = beginpmatrix 0  1  2 endpmatrix So by choosing varepsilon = 10^-6 the matrix is ill-conditioned.\n\nWe first solve LinearProblem with an lu solver (using LU and solve) in double precision and without pivoting:\n\nlu = LU(; pivot = false)\ny¹ = solve(lu, ls)\n\nWe check the result:\n\nA * y¹\n\nWe now do the same in single precision:\n\nAˢ = Float32.(A)\nyˢ = Float32.(y)\nlsˢ = LinearProblem(Aˢ, yˢ)\nupdate!(lsˢ, Aˢ, yˢ)\ny² = solve(lu, lsˢ)\n\nand again check the result:\n\nAˢ * y²\n\nAs we can see the computation of the factorization returns a wrong solution. If we use pivoting however, the problem can also be solved with single precision:\n\nlu = LU(; pivot = true)\ny³ = solve(lu, lsˢ)\n\nAˢ * y³","category":"section"},{"location":"linear/linear_solvers/#Solving-the-System-with-Built-In-Functionality-from-the-LinearAlgebra-Package","page":"Linear Solvers","title":"Solving the System with Built-In Functionality from the LinearAlgebra Package","text":"We further try to solve the system with the inv operator from the LinearAlgebra package. First in double precision:\n\ninv(A) * y\n\nAnd also in single precision\n\ninv(Aˢ) * yˢ\n\nIn single precision the result is completely wrong as can also be seen by computing:\n\ninv(Aˢ) * Aˢ\n\nIf we however write:\n\nAˢ \\ yˢ\n\nwe again obtain a correct-looking result, as LinearAlgebra.\\ uses an algorithm very similar to factorize! in SimpleSolvers.","category":"section"},{"location":"linesearch/static/#Static-Line-Search","page":"Static","title":"Static Line Search","text":"Static line search is the simplest form of line search in which the guess for alpha is always just a fixed value. In the following we demonstrate how to use this line search.","category":"section"},{"location":"linesearch/static/#static_example","page":"Static","title":"Example","text":"We show how to use linesearches in SimpleSolvers to solve a simple toy problem:\n\nusing SimpleSolvers # hide\n\nx = [1., 0., 0.]\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nobj = OptimizerProblem(f, x)\n\nα = .1\nls_method = Static(α)\nnothing # hide\n\nSimpleSolvers contains a function SimpleSolvers.linesearch_problem that allocates a SimpleSolvers.LinesearchProblem that only depends on alpha:\n\nusing SimpleSolvers: linesearch_problem, NewtonOptimizerCache, update! # hide\ncache = NewtonOptimizerCache(x)\nstate = NewtonOptimizerState(x)\ngrad = GradientAutodiff{Float64}(obj.F, length(x))\nupdate!(cache, state, grad, x)\nx₂ = [.9, 0., 0.]\nupdate!(cache, state, grad, x₂)\nls_obj = linesearch_problem(obj, grad, cache, state)\nnothing # hide\n\nWe now use this to compute a static line search:\n\nls = Linesearch(ls_method)\nsolve(ls_obj, ls)\n\ninfo: Info\nWe note that for the static line search we always just return alpha.","category":"section"},{"location":"optimizers/optimizers/#Optimizers","page":"Optimizers","title":"Optimizers","text":"An Optimizer stores an OptimizerState, a OptimizerProblem, the SimpleSolvers.OptimizerResult and a SimpleSolvers.NonlinearMethod. Its purposes are:\n\nusing SimpleSolvers\nusing LinearAlgebra: norm\nimport Random # hide\nRandom.seed!(123) # hide\n\nx = rand(3)\nobj = OptimizerProblem(x -> sum((x - [0., 0., 1.]) .^ 2), x)\nbt = Backtracking()\nalg = Newton()\nopt = Optimizer(x, obj; algorithm = alg, linesearch = bt)","category":"section"},{"location":"optimizers/optimizers/#Optimizer-Constructor","page":"Optimizers","title":"Optimizer Constructor","text":"Internally the constructor for Optimizer calls SimpleSolvers.OptimizerResult and SimpleSolvers.NewtonOptimizerState and Hessian. We can also allocate these objects manually and then call a different constructor for Optimizer:\n\nusing SimpleSolvers: NewtonOptimizerState, NewtonOptimizerCache, initialize!\n\n_cache = NewtonOptimizerCache(x)\nhes = Hessian(alg, obj, x)\n_linesearch = Linesearch(Static(.1))\nopt₂ = Optimizer(alg, obj, hes, _cache, _linesearch)\n\nIf we want to solve the problem, we can call solve! on the Optimizer instance:\n\nx₀ = copy(x)\nstate = NewtonOptimizerState(x)\nsolve!(x₀, state, opt)\n\nInternally SimpleSolvers.solve! repeatedly calls SimpleSolvers.solver_step! until SimpleSolvers.meets_stopping_criteria is satisfied.\n\nusing SimpleSolvers: solver_step!\n\nx = rand(3)\nsolver_step!(x, state, opt)\n\nThe function SimpleSolvers.solver_step! in turn does the following:\n\n# update problem, hessian, state and result\nupdate!(opt, state, x)\n\n# solve H δx = - ∇f\n# rhs is -g\nldiv!(direction(opt), hessian(opt), rhs(opt))\n\n# apply line search\nα = linesearch(opt)(linesearch_problem(problem(opt), gradient(opt), cache(opt), state))\n\n# compute new minimizer\nx .= compute_new_iterate(x, α, direction(opt))\ncache(opt).x .= x","category":"section"},{"location":"optimizers/optimizers/#Solving-the-Line-Search-Problem-with-Backtracking","page":"Optimizers","title":"Solving the Line Search Problem with Backtracking","text":"Calling solve together with Linesearch (in this case SimpleSolvers.Backtracking) on an SimpleSolvers.LinesearchProblem in turn does:\n\nα *= ls.p\n\nas long as the SimpleSolvers.SufficientDecreaseCondition isn't satisfied. This condition checks the following:\n\nfₖ₊₁ ≤ sdc.fₖ + sdc.c₁ * αₖ * sdc.pₖ' * sdc.gradₖ\n\nsdc is first allocated as:\n\nusing SimpleSolvers: SufficientDecreaseCondition, linesearch, linesearch_problem, problem, cache # hide\nls = linesearch(opt)\nα = ls.algorithm.α₀\nx₀ = zero(α)\ngrad = GradientAutodiff{Float64}(problem(opt).F, length(x))\nlso = linesearch_problem(problem(opt), grad, cache(opt), state)\ny₀ = value(lso, x₀)\nd₀ = derivative(lso, x₀)\n\nsdc = SufficientDecreaseCondition(ls.algorithm.ϵ, x₀, y₀, d₀, d₀, obj)","category":"section"},{"location":"update/#Updates","page":"Updates","title":"Updates","text":"One of the most central objects in SimpleSolvers are update! routines. They can be used together with many different types and structs:\n\nSimpleSolvers.update!(::Hessian, ::AbstractVector): this routine exists for most Hessians, i.e. for HessianFunction, HessianAutodiff, HessianBFGS and HessianDFP,\nSimpleSolvers.update!(::SimpleSolvers.NonlinearSolverCache, ::AbstractVector),\nSimpleSolvers.update!(::SimpleSolvers.NewtonOptimizerCache, ::AbstractVector, ::AbstractVector, ::Hessian),\nSimpleSolvers.update!(::SimpleSolvers.NewtonOptimizerState, ::AbstractVector).\nSimpleSolvers.update!(::SimpleSolvers.OptimizerResult, ::AbstractVector, ::AbstractVector, ::AbstractVector).\n\nSo update! always takes an object that has to be updated and a single vector in the simplest case. For some methods more arguments need to be provided. ","category":"section"},{"location":"update/#Examples","page":"Updates","title":"Examples","text":"","category":"section"},{"location":"update/#Hessian","page":"Updates","title":"Hessian","text":"If we look at the case of the Hessian, we store a matrix H that has to be updated in every iteration. We first initialize the matrix[1]:\n\n[1]: The constructor uses the function SimpleSolvers.initialize!.\n\nusing SimpleSolvers # hide\nusing LinearAlgebra: norm # hide\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nx = [1., 0., 0.]\nhes = HessianAutodiff(f, x)\nH = SimpleSolvers.alloc_h(x)\n\nAnd then update:\n\nhes(H, x)\n\nH","category":"section"},{"location":"update/#NewtonOptimizerCache","page":"Updates","title":"NewtonOptimizerCache","text":"In order to update an instance of SimpleSolvers.NewtonOptimizerCache we have to supply a value of the Gradient and the Hessian in addition to x:\n\nusing SimpleSolvers: initialize!, NewtonOptimizerCache # hide\ngrad = GradientAutodiff(f, x)\ncache = NewtonOptimizerCache(x)\nstate = NewtonOptimizerState(x)\nupdate!(cache, state, grad, hes, x)\n\ninfo: Info\nWe note that when calling update! on the NewtonOptimizerCache, the Hessian hes is not automatically updated! This has to be done manually.\n\ninfo: Info\nCalling update! on the NewtonOptimizerCache updates everything except x as this in general requires another line search!\n\ninfo: Info\nWhen updating the cache we also need to supply the state. This is needed for the direction.","category":"section"},{"location":"update/#OptimizerResult","page":"Updates","title":"OptimizerResult","text":"warning: Warning\nNewtonOptimizerCache, OptimizerResult and NewtonOptimizerState (through OptimizerProblem) all store things that are somewhat similar, for example x. This may make it somewhat difficult to keep track of all the things that happen during optimization.\n\nAn Optimizer stores a OptimizerProblem, an SimpleSolvers.OptimizerResult and an OptimizerState (and therefore the OptimizerProblem again). We also give an example:\n\nobj = OptimizerProblem(f, x)\nopt = Optimizer(x, obj; algorithm = Newton())\n\nupdate!(opt, state, x)\n\nEquivalent to calling update! on SimpleSolvers.OptimizerResult, the diagnostics cannot be computed with only one iterations; we have to compute a second one:\n\nx₂ = [.9, 0., 0.]\nupdate!(opt, state, x₂)\n\nWe note that simply calling update! on an instance of SimpleSolvers.Optimizer is not enough to perform a complete iteration since the computation of a new x requires a line search procedure in general.\n\nWe also note that update! always returns the first input argument.","category":"section"},{"location":"linesearch/bisections/#Bisections","page":"Bisections","title":"Bisections","text":"Bisections work by moving an interval until we observe one in which the sign of the derivative of the function changes. ","category":"section"},{"location":"linesearch/bisections/#Example","page":"Bisections","title":"Example","text":"We consider the same example as we had when demonstrating backtracking line search:\n\nls_obj = linesearch_problem(obj, grad, cache, state)\nnothing # hide","category":"section"},{"location":"linesearch/bisections/#Bracketing","page":"Bisections","title":"Bracketing","text":"For bracketing [4] we move an interval successively and simultaneously increase it in the hope that we observe a local minimum (see bracket_minimum).\n\nα₀ = 0.0\n(a, c) = bracket_minimum(Function(ls_obj), α₀)\n\n(Image: )\n\nWe then use this interval to start the bisection algorithm.","category":"section"},{"location":"linesearch/bisections/#Potential-Problem-with-Backtracking","page":"Bisections","title":"Potential Problem with Backtracking","text":"We here illustrate a potential issue with backtracking. For this consider the following function:\n\nusing SimpleSolvers: bracket_root\nf2(α::T) where {T <: Number} = α^2 - one(T)\nα₀ = -3.0\n(a, c) = bracket_root(f2, α₀)\n\nAnd when we plot this we find:\n\n(Image: )\n\nAnd we see that the interval now contains two roots, r_1 and r_2.","category":"section"},{"location":"linesearch/backtracking/#Backtracking-Line-Search","page":"Backtracking","title":"Backtracking Line Search","text":"A backtracking line search method determines the amount to move in a given search direction by iteratively decreasing a step size alpha until an acceptable level is reached. In SimpleSolvers we can use the sufficient decrease condition and the curvature condition to quantify this acceptable level. The sufficient decrease condition is also referred to as the Armijo condition and the sufficient decrease condition and the curvature condition are referred to as the Wolfe conditions[1] [3]. \n\n[1]: If we use the strong curvature condition instead of the standard curvature condition we conversely also say that we use the strong Wolfe conditions.\n\ninfo: Info\nWe note that for the static line search we always just return alpha.","category":"section"},{"location":"linesearch/backtracking/#Backtracking-Line-Search-for-a-Line-Search-Problem","page":"Backtracking","title":"Backtracking Line Search for a Line Search Problem","text":"We note that when performing backtracking on a line search problem care needs to be taken. This is because we need to find equivalent quantities for mathrmgrad_x_kf and p. We first look at the derivative of the line search problem:\n\nfracddalphaf^mathrmls(alpha) = fracddalphaf(mathcalR_x_k(alphap)) = langle d_mathcalR_x_k(alphap)f alphap rangle\n\nbecause the tangent map of a retraction is the identity at zero [5], i.e. T_0_xmathcalR = mathrmid_T_xmathcalM. In the equation above d_mathcalR_x_k(alphap)finT^*mathcalM indicates the exterior derivative of f evaluated at mathcalR_x_k(alphap) and langle cdot cdot rangle T^*mathcalMtimesTmathcalMtomathbbR is the natural pairing between tangent and cotangent space[2] [6].\n\n[2]: If we are not dealing with general Riemannian manifolds but only vector spaces then d_mathcalR_x_k(alphap)f simply becomes nabla_mathcalR_x_k(alphap)f and we further have langle A Brangle = A^T B.\n\nWe again look at the example introduced when talking about the sufficient decrease condition and cast it in the form of a line search problem:\n\nls_obj = linesearch_problem(obj, grad, cache, state)\nnothing # hide\n\nThis optimizer problem only depends on the parameter alpha. We plot it:\n\n(Image: )","category":"section"},{"location":"linesearch/backtracking/#sdc_example","page":"Backtracking","title":"Example","text":"We show how to use line searches in SimpleSolvers to solve a simple toy problem[3]:\n\n[3]: Also compare this to the case of the static line search.\n\nusing SimpleSolvers # hide\n\nls_method = Backtracking()\nnothing # hide\n\nSimpleSolvers contains a function SimpleSolvers.linesearch_problem that allocates a SimpleSolvers.LinesearchProblem that only depends on alpha:\n\nWe now use this to compute a backtracking line search:\n\nls = Linesearch(ls_method)\nα = 50.\nαₜ = solve(ls_obj, ls, α)\n\nAnd we check whether the SimpleSolvers.SufficientDecreaseCondition is satisfied:\n\nsdc = SufficientDecreaseCondition(c₁, x, f(x), g, p, obj)\nsdc(αₜ)\n\nSimilarly for the SimpleSolvers.CurvatureCondition:\n\nusing SimpleSolvers: CurvatureCondition # hide\nc₂ = .9\ncc = CurvatureCondition(c₂, x, g, p, obj, grad)\ncc(αₜ)","category":"section"},{"location":"linesearch/quadratic/#Quadratic-Line-Search","page":"Quadratic","title":"Quadratic Line Search","text":"Quadratic line search is based on making a quadratic approximation to an optimizer problem and then pick the minimum of this quadratic approximation as the next iteration of alpha.\n\nThe quadratic polynomial is built the following way[1]:\n\n[1]: This is different from the Bierlaire quadratic polynomial described in [2].\n\np(alpha) = f^mathrmls(0) + (f^mathrmls)(0)alpha + p_2alpha^2\n\nand we also call p_0=f^mathrmls(0) and p_1=(f^mathrmls)(0). The coefficient p_2 is then determined the following way:\n\ntake a value alpha (typically initialized as SimpleSolvers.DEFAULT_ARMIJO_α₀) and compute y = f^mathrmls(alpha),\nset p_2 gets frac(y - p_0 - p_1alpha)alpha^2\n\nAfter the polynomial is found we then take its minimum (analogously to the Bierlaire quadratic line search) and check if it satisfies the sufficient decrease condition. If it does not satisfy this condition we repeat the process, but with the current alpha as the starting point for the line search (instead of the initial SimpleSolvers.DEFAULT_ARMIJO_α₀).","category":"section"},{"location":"linesearch/quadratic/#Example","page":"Quadratic","title":"Example","text":"Here we treat the following problem:\n\nf(x::Union{T, Vector{T}}) where {T<:Number} = exp.(x) .* (x .^ 3 - 5x + 2x) .+ 2one(T)\nf!(y::AbstractVector{T}, x::AbstractVector{T}) where {T} = y .= f.(x)\nF!(y::AbstractVector{T}, x::AbstractVector{T}, params) where {T} = f!(y, x)\nnothing # hide\n\n(Image: )\n\nWe now want to use quadratic line search to find the root of this function starting at x = 0. We compute the Jacobian of f and initialize a line search problem:\n\nusing SimpleSolvers\nusing SimpleSolvers: factorize!, update!, linearsolver, jacobian, jacobian!, cache, linesearch_problem, direction, determine_initial_α, NullParameters, NonlinearSolverState, jacobianmatrix # hide\nusing LinearAlgebra: rmul!, ldiv! # hide\nusing Random # hide\nRandom.seed!(123) # hide\n\nfunction J!(j::AbstractMatrix{T}, x::AbstractVector{T}, params) where {T}\n    SimpleSolvers.ForwardDiff.jacobian!(j, f, x)\nend\n\n# allocate solver\nsolver = NewtonSolver(x, f(x); F = F!, DF! = J!)\n# initialize solver\nparams = NullParameters()\nstate = NonlinearSolverState(x)\njacobian!(solver, x, params)\n\n# compute rhs\nF!(cache(solver).rhs, x, params)\nrmul!(cache(solver).rhs, -1)\n\n# multiply rhs with jacobian\nfactorize!(linearsolver(solver), jacobianmatrix(solver))\nldiv!(direction(cache(solver)), linearsolver(solver), cache(solver).rhs)\nnlp = NonlinearProblem(F!, J!, x, f(x))\nstate = NonlinearSolverState(x)\nupdate!(state, x, f(x))\nls_obj = linesearch_problem(nlp, jacobian(solver), cache(solver), x, params)\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide\n\n(Image: )\n\ninfo: Info\nThe second plot shows the optimization problem for the ideal step length, where we start from x_0 and proceed in the Newton direction. In the following we want to determine its minimum by fitting a quadratic polynomial, i.e. fitting p.\n\nThe first two coefficient of the polynomial p (i.e. p_1 and p_2) are easy to compute:\n\np₀ = fˡˢ(0.)\n\np₁ = ∂fˡˢ∂α(0.)","category":"section"},{"location":"linesearch/quadratic/#Initializing-\\alpha","page":"Quadratic","title":"Initializing alpha","text":"In order to compute p_2 we first have to initialize alpha. We start by guessing an initial alpha as SimpleSolvers.DEFAULT_ARMIJO_α₀. If this initial alpha does not satisfy the SimpleSolvers.BracketMinimumCriterion, i.e. it holds that f^mathrmls(alpha_0)  f^mathrmls(0), we call SimpleSolvers.bracket_minimum_with_fixed_point (similarly to calling SimpleSolvers.bracket_minimum for standard bracketing). \n\nLooking at SimpleSolvers.DEFAULT_ARMIJO_α₀, we see that the SimpleSolvers.BracketMinimumCriterion is not satisfied:\n\n(Image: )\n\nWe therefore see that calling SimpleSolvers.determine_initial_α returns a different alpha (the result of calling SimpleSolvers.bracket_minimum_with_fixed_point):\n\nα₀ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\n\n(Image: )\n\nWe can now finally compute p_2 and determine the minimum of the polynomial:\n\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nα₁ = -p₁ / (2p₂)\n\n(Image: )\n\nWe now check wether alpha_1 satisfies the sufficient decrease condition:\n\nusing SimpleSolvers: DEFAULT_WOLFE_c₁, SufficientDecreaseCondition # hide\nsdc = SufficientDecreaseCondition(DEFAULT_WOLFE_c₁, 0., fˡˢ(0.), derivative(ls_obj, 0.), 1., ls_obj)\n@assert sdc(α₁) # hide\nsdc(α₁)\n\nWe now move the original x in the Newton direction with step length alpha_1 by using SimpleSolvers.compute_new_iterate!:\n\nusing SimpleSolvers: compute_new_iterate! # hide\ncompute_new_iterate!(x, α₁, direction(cache(solver)))\n\n(Image: )\n\nAnd we see that we already very close to the root.","category":"section"},{"location":"linesearch/quadratic/#Example-for-Optimization","page":"Quadratic","title":"Example for Optimization","text":"We look again at the same example as before, but this time we want to find a minimum and not a root. We hence use SimpleSolvers.linesearch_problem not for a NewtonSolver, but for an Optimizer:\n\nusing SimpleSolvers: NewtonOptimizerCache, initialize!, gradient\n\nx₀, x₁ = [0.], x\nobj = OptimizerProblem(sum∘f, x₀)\ngrad = GradientAutodiff{Float64}(obj.F, length(x))\n_cache = NewtonOptimizerCache(x₀)\nstate = NewtonOptimizerState(x₀)\nhess = HessianAutodiff(obj, x₀)\nH = SimpleSolvers.alloc_h(x)\nhess(H, x₀)\nupdate!(_cache, state, grad, hess, x₀)\nhess(H, x₁)\nupdate!(_cache, state, grad, hess, x₁)\nls_obj = linesearch_problem(obj, grad, _cache, state)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide\n\n(Image: )\n\ninfo: Info\nNote the different shape of the line search problem in the case of the optimizer, especially that the line search problem can take negative values in this case!\n\nWe now again want to find the minimum with quadratic line search and repeat the procedure above:\n\np₀ = fˡˢ(0.)\n\np₁ = ∂fˡˢ∂α(0.)\n\nα₀ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nα₁ = -p₁ / (2p₂)\n\n(Image: )\n\nWe now again move the original x in the Newton direction with step length alpha_1:\n\ncompute_new_iterate!(x, α₁, direction(_cache))\n\n(Image: )\n\nWe make another iteration:\n\nhess(H, x)\nupdate!(_cache, state, grad, hess, x)\nls_obj = linesearch_problem(obj, grad, _cache, state)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\np₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nα₀⁽²⁾ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀⁽²⁾) / α₀⁽²⁾^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nα₂ = -p₁ / (2p₂)\n\n(Image: )\n\nWe now update x:\n\nusing SimpleSolvers: compute_new_iterate\nx .= compute_new_iterate(x, α₂, direction(_cache))\n\n(Image: )\n\nWe finally compute a third iterate:\n\nhess(H, x)\nupdate!(_cache, state, grad, hess, x)\nls_obj = linesearch_problem(obj, grad, _cache, state)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\np₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nα₀⁽³⁾ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀⁽³⁾) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nα₃ = -p₁ / (2p₂)\n\nx .= compute_new_iterate(x, α₃, direction(_cache))\n\n(Image: )","category":"section"},{"location":"linesearch/quadratic/#Example-II","page":"Quadratic","title":"Example II","text":"Here we consider the same example as when discussing the Bierlaire quadratic line search.\n\nstate = NonlinearSolverState(x)\nupdate!(state, x, f(x))\nls_obj = linesearch_problem(nlp, JacobianFunction{Float64}(F!, J!), cache(solver), x, params)\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide\n\nWe now try to find a minimum of f^mathrmls with quadratic line search. For this we first need to find a bracket; we again do this with SimpleSolvers.bracket_minimum_with_fixed_point[2]:\n\n[2]: Here we use SimpleSolvers.bracket_minimum_with_fixed_point directly instead of using SimpleSolvers.determine_initial_α.\n\n(a, b) = SimpleSolvers.bracket_minimum_with_fixed_point(fˡˢ, ∂fˡˢ∂α, 0.)\n\nWe plot the bracket:\n\nusing CairoMakie\nmred = RGBf(214 / 256, 39 / 256, 40 / 256)\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256)\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256)\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256)\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256)\n\nfig = Figure()\nax = Axis(fig[1, 1])\nalpha = -2:.01:6.\nlines!(ax, alpha, fˡˢ.(alpha); label = L\"f^\\mathrm{ls}(\\alpha)\")\nscatter!(ax, a, fˡˢ(a); color = mred, label = L\"a\")\nscatter!(ax, b, fˡˢ(b); color = mpurple, label = L\"b\")\n# ylims!(ax, (-1., 6.)) # hide\naxislegend(ax)\nsave(\"f_ls_1.png\", fig)\nnothing # hide\n\n(Image: )\n\nWe now build the polynomial:\n\np₀ = fˡˢ(a)\np₁ = ∂fˡˢ∂α(a)\ny = fˡˢ(b)\np₂ = (y - p₀ - p₁*b) / b^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nnothing # hide\n\nand compute its minimum:\n\nαₜ = -p₁ / (2p₂)\n\nlines!(ax, alpha, p.(alpha); label = L\"p(\\alpha)\")\nscatter!(ax, αₜ, p(αₜ); label = L\"\\alpha_t\")\n# ylims!(ax, (-1., 6.)) # hide\naxislegend(ax)\nsave(\"f_ls_2.png\", fig)\nnothing # hide\n\n(Image: )\n\nWe now set a gets alpha_t and perform another iteration:\n\n(a, b) = SimpleSolvers.bracket_minimum_with_fixed_point(fˡˢ, ∂fˡˢ∂α, αₜ)\n\nWe again build the polynomial:\n\np₀ = fˡˢ(a)\np₁ = ∂fˡˢ∂α(a)\ny = fˡˢ(b)\np₂ = (y - p₀ - p₁*(b-a)) / (b-a)^2\np(α) = p₀ + p₁ * (α-a) + p₂ * (α-a)^2\nnothing # hide\n\nand compute its minimum:\n\nαₜ = -p₁ / (2p₂) + a\n\n(Image: )","category":"section"},{"location":"linesearch/sufficient_decrease_condition/#The-Sufficient-Decrease-Condition","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"The Armijo condition or sufficient decrease condition states:\n\n    f(R_x_k(alpha_kp_k)) leq f(x_k) + c_1g_x_k(alpha_kp_k mathrmgrad^g_x_kf)  \n\nfor some constant c_1in(0 1) (see SimpleSolvers.DEFAULT_WOLFE_c₁).\n\nThe sufficient decrease condition can also be written as \n\n    fracf(R_x_k(alpha_kp_k)) - f(x_k)alpha_k leq g_x_k(c_1p_k mathrmgrad^g_x_kf)\n\nAs we assume that f(R_x_k(alpha_kp_k)) leq f(x_k) and g_x_k(c_1p_k mathrmgrad^g_x_kf)  0, we can rewrite this as:\n\n    fracf(R_x_k(alpha_kp_k)) - f(x_k)alpha_k geq g_x_k(c_1p_k mathrmgrad^g_x_kf)\n\nmaking clear why this is called the sufficient decrease condition. The parameter c_1 is typically chosen very small, around 10^-4. This is implemented as SimpleSolvers.SufficientDecreaseCondition.","category":"section"},{"location":"linesearch/sufficient_decrease_condition/#sdc_example_full","page":"The Sufficient Decrease Condition","title":"Example","text":"We can visualize the sufficient decrease condition with an example:\n\nusing SimpleSolvers # hide\nusing SimpleSolvers: SufficientDecreaseCondition, NewtonOptimizerCache, update!, linesearch_problem # hide\n\nx = [3., 1.3]\nf = x -> 10 * sum(x .^ 3 / 6 - x .^ 2 / 2)\nobj = OptimizerProblem(f, x)\nhes = HessianAutodiff(obj, x)\nH = SimpleSolvers.alloc_h(x)\nhes(H, x)\n\nc₁ = 1e-4\ngrad = GradientAutodiff{Float64}(obj.F, length(x))\ng = grad(x)\nrhs = -g\n# the search direction is determined by multiplying the right hand side with the inverse of the Hessian from the left.\np = similar(rhs)\np .= H \\ rhs\nsdc = SufficientDecreaseCondition(c₁, x, f(x), g, p, obj)\n\n# check different values\nα₁, α₂, α₃, α₄, α₅ = .09, .4, 0.7, 1., 1.3\n(sdc(α₁), sdc(α₂), sdc(α₃), sdc(α₄), sdc(α₅))\n\nWe can also illustrate this:\n\n(Image: )","category":"section"},{"location":"in_place_out_of_place/#What-is-In-Place-and-what-is-Out-Of-Place","page":"In-place vs out-of-place","title":"What is In-Place and what is Out-Of-Place","text":"In SimpleSolvers we almost always use in-place functions internally for performance, but let the user deal with out-of-place functions for ease of use.","category":"section"},{"location":"in_place_out_of_place/#Example","page":"In-place vs out-of-place","title":"Example","text":"using SimpleSolvers\n\nf(x) = sum(x.^2 .* exp.(-abs.(x)) + 2 * cos.(x) .* exp.(-x.^2))\nnothing # hide\n\n(Image: )\n\nIf we now allocate a OptimizerProblem based on this, we can use value[1] with it:\n\n[1]: See the section on optimizer problems for an explanation of how to use value.\n\nx = [0.]\nobj = OptimizerProblem(f, x)\ny = [0.]\nval = value(obj, x)\n@assert val == f(x) # hide\nval == f(x)\n\nTo compute the derivative we can use the functor of GradientAutodiff:\n\nx = [[x] for x in -7.:.1:7.]\ny = Vector{Float64}[]\nfor x_sing in x\n    grad = GradientAutodiff{Float64}(obj.F, length(x_sing))\n    push!(y, grad(x_sing))\nend\nnothing # hide\n\nwarning: Warning\nNote that here we used GradientAutodiff to compute the gradient. We can also use GradientFunction and GradientFiniteDifferences.\n\n(Image: )\n\nThe idea is however that the user almost never used the in-place versions of these routines directly, but instead functions like solve! and value, gradient etc. as a possible diagnostic.","category":"section"},{"location":"gradients/#Gradients","page":"Gradients","title":"Gradients","text":"The supertype Gradient comprises different ways of taking gradients:\n\nGradientFunction,\nGradientAutodiff,\nGradientFiniteDifferences.\n\nWe first start by showing GradientAutodiff:\n\nusing SimpleSolvers, Random; using SimpleSolvers: GradientAutodiff, Gradient, GradientFunction, GradientFiniteDifferences; Random.seed!(123) # hide\nf(x::AbstractArray) = sum(x .^ 2)\nx = rand(3)\ngrad = GradientAutodiff(f, x)\n\nEvery struct derived from Gradient (including GradientAutodiff) has an associated functor:\n\ngrad(x)","category":"section"},{"location":"nonlinear_solver_status/#Solver-Status","page":"Solver Status","title":"Solver Status","text":"In SimpleSolvers we can use the SimpleSolvers.NonlinearSolverStatus to provide a diagnostic tool for a NonlinearSolver. We first make an instance of NonlinearProblem:\n\nusing SimpleSolvers # hide\nusing SimpleSolvers: NonlinearSolverState, update!, solver_step! # hide\n\nx = [3., 1.3]\nf = x -> tanh.(x)\nF!(y, x, params) = y .= f(x)\nnlp = NonlinearProblem(F!, x, f(x))\n\nWe now create an instance of NewtonSolver and NonlinearSolverState:\n\nsolver = NewtonSolver(x, f(x); F = F!)\nstate = NonlinearSolverState(x)\n\nNote that all variables are initialized with NaNs.\n\nFor the first step we call solver_step! (which updates the state internally via update![1]):\n\n[1]: Also see the page on the update! function.\n\nusing SimpleSolvers: NullParameters, cache # hide\nparams = NullParameters()\nsolver_step!(x, solver, state, params)\n\nWe now compute the NonlinearSolverStatus:\n\nusing SimpleSolvers: NonlinearSolverStatus # hide\n\nNonlinearSolverStatus(state, config(solver))","category":"section"},{"location":"linesearch/linesearch/#Line-Search","page":"Line Searches","title":"Line Search","text":"This page is largely a summary of [3, Chapter 3]. We summarize this reference by omitting proofs, but also aim to extend it to manifolds.\n\nA line search method has the goal of minimizing an optimizer problem (either a LinesearchProblem or a OptimizerProblem) approximately, based on a search direction[1].\n\n[1]: in [3] (and other references) a search direction is called a descent direction.\n\ninfo: Definition\nFor an optimizer problem fmathcalMtomathbbR on a manifold mathcalM a search direction at point x_kinmathcalM is a vector p_kinT_x_kmathcalM for which we have    g_x_k(p_k mathrmgrad^g_x_kf)  0where g_x_kT_x_kmathcalMtimesT_x_kmathcalMtomathbbR is a Riemannian metric.\n\nA line search is therefore a sub-optimization problem in a nonlinear optimizer (or solver) in which we want to find an alpha that minimizes:\n\n    min_alphaf^mathrmls(alpha) = min_alphaf(mathcalR_x_k(alpha_kp_k))\n\nwhere p_k is the search direction.\n\nFor line search methods we have to (i) find a search direction p_k and (ii) find an appropriate step size alpha_k = mathrmargmin_alphaf(alpha). We then update x_k based on these quantities:\n\n    x_k+1 gets mathcalR_x_k(alpha_kp_k)\n\nwhere mathcalR_x_kT_x_kmathcalMtomathcalM is a retraction at x_k\n\nIn practice we will not be able to find the ideal alpha at every step, but only an approximation thereof. Examples of line search algorithms that aim at finding this alpha are the static line search and the backtracking line search.","category":"section"},{"location":"linesearch/linesearch/#Line-Search-Problem","page":"Line Searches","title":"Line Search Problem","text":"SimpleSolvers contains a function SimpleSolvers.linesearch_problem that allocates a LinesearchProblem that realizes the function f^mathrmls described above.","category":"section"},{"location":"linesearch/linesearch/#Search-Directions-for-Optimizers","page":"Line Searches","title":"Search Directions for Optimizers","text":"In SimpleSolvers we typically build the search direction by multiplying the gradient with a Hessian. When starting at x_k we take:\n\n    p_k = H_x_k^-1(nabla_x_kf)\n\nwhere H_x_k_ij = partial^2fpartialx_ipartialx_j_x_k is the Hessian. Note that we often use approximations of this Hessian in practice (such as the HessianBFGS).\n\nFor manifolds [5] defining a Hessian, equivalently to defining a gradient, requires a Riemannian metric and the associated Levi-Civita connection nabla:\n\nmathrmHess(f) = nablanablaf = nabladf in Gamma(T^*mathcalMotimesT^*mathcalM)\n\nFor specific vector fields xi eta in Gamma(TmathcalM) we can write this as:\n\nlangle mathrmHess(f)xi eta  rangle = xi(etaf) - (nabla_xieta)f","category":"section"},{"location":"hessians/#Hessians","page":"Hessians","title":"Hessians","text":"Hessians are a crucial ingredient in NewtonSolvers and SimpleSolvers.NewtonOptimizerStates.\n\nusing SimpleSolvers\nusing LinearAlgebra: norm\n\nx = rand(3)\nobj = OptimizerProblem(x -> norm(x - vcat(0., 0., 1.))  ^ 2, x)\nhes = HessianAutodiff(obj, x)\n\nThe instance of HessianAutodiff can be called:\n\nhes(x)\n\nOr alternative in-place:\n\nH = SimpleSolvers.alloc_h(x)\nhes(H, x)\nH","category":"section"},{"location":"optimizer_problems/#Optimizer-Problems","page":"Optimizer Problems","title":"Optimizer Problems","text":"A central object in SimpleSolvers are optimizer problems (see SimpleSolvers.AbstractOptimizerProblem). They are either SimpleSolvers.LinesearchProblems or OptimizerProblems. The goal of a solver (both LinearSolvers and NonlinearSolvers) is to make the optimizer problem have value zero. The goal of an Optimizer is to minimize a OptimizerProblem.","category":"section"},{"location":"optimizer_problems/#Examples","page":"Optimizer Problems","title":"Examples","text":"","category":"section"},{"location":"optimizer_problems/#Multivariate-Optimizer-Problems","page":"Optimizer Problems","title":"Multivariate Optimizer Problems","text":"OptimizerProblems are used in a way similar to LinesearchProblems.\n\nusing SimpleSolvers # hide\nusing Random # hide\nRandom.seed!(123) # hide\nf(x::AbstractArray) = sum(x .^ 2)\nx = rand(3)\n\nobj = OptimizerProblem(f, x)","category":"section"},{"location":"initialize/#Initialization","page":"Initialization","title":"Initialization","text":"Before we can use update! we have to initialize with SimpleSolvers.initialize![1].\n\n[1]: The different methods for SimpleSolvers.initialize! are however often called with the constructor of a struct (e.g. for SimpleSolvers.NewtonOptimizerCache).\n\nSimilar to update!, SimpleSolvers.initialize! returns the first input argument as output. Examples include:\n\nSimpleSolvers.initialize!(::SimpleSolvers.NonlinearSolverCache, ::AbstractVector),\nSimpleSolvers.initialize!(::SimpleSolvers.NewtonOptimizerCache, ::AbstractVector).\n\nWe demonstrate this for an instance of SimpleSolvers.NewtonOptimizerCache[2]:\n\n[2]: Here we remark that SimpleSolvers.NewtonOptimizerCache has five keys: x, x̄, δ, g and rhs. All of them are initialized with NaNs except for x. We also remark that the constructor, which is called by providing a single vector x as input argument, internally also calls SimpleSolvers.initialize!.\n\nusing SimpleSolvers # hide\nusing SimpleSolvers: initialize! # hide\nusing Random: seed! # hide\nseed!(123) # hide\nx = rand(3)\ncache = SimpleSolvers.NewtonOptimizerCache(x)\ninitialize!(cache, x)\ncache.g","category":"section"},{"location":"initialize/#Reasoning-behind-Initialization-with-NaNs","page":"Initialization","title":"Reasoning behind Initialization with NaNs","text":"We initialize with NaNs instead of with zeros (or other values) as this clearly divides the initialization from the numerical operations (which are done with update!).","category":"section"},{"location":"initialize/#Alloc-Functions","page":"Initialization","title":"Alloc Functions","text":"","category":"section"},{"location":"#SimpleSolvers","page":"Home","title":"SimpleSolvers","text":"","category":"section"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_p","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_p","text":"const DEFAULT_ARMIJO_p\n\nConstant used in Backtracking. Its value is 0.5\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_α₀","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_α₀","text":"const DEFAULT_ARMIJO_α₀\n\nThe default starting value for alpha used in SufficientDecreaseCondition (also see Backtracking and Quadratic). Its value is 1.0\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_σ₀","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_σ₀","text":"const DEFAULT_ARMIJO_σ₀\n\nConstant used in Quadratic. Also see DEFAULT_ARMIJO_σ₁.\n\nIt is meant to safeguard against stagnation when performing line searches (see [1]).\n\nIts value is 0.1\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_σ₁","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_σ₁","text":"const DEFAULT_ARMIJO_σ₁\n\nConstant used in Quadratic. Also see DEFAULT_ARMIJO_σ₀. Its value is 0.5\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BIERLAIRE_ε","page":"Home","title":"SimpleSolvers.DEFAULT_BIERLAIRE_ε","text":"DEFAULT_BIERLAIRE_ε\n\nA constant that determines the precision in BierlaireQuadratic. The constant recommended in [2] is 1E-3.\n\nNote that this constant may also depend on whether we deal with optimizers or solvers.\n\nwarning: Warning\nWe have deactivated the use of this constant for the moment and are only using eps(T) in BierlaireQuadratic. This is because solvers and optimizers should rely on different choices of this constant.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BIERLAIRE_ξ","page":"Home","title":"SimpleSolvers.DEFAULT_BIERLAIRE_ξ","text":"DEFAULT_BIERLAIRE_ξ\n\nA constant on basis of which the b in BierlaireQuadratic is perturbed in order \"to avoid stalling\" (see [2, Chapter 11.2.1]; in this reference the author recommends 10^-7 as a value). Its value is 2.384185791015625e-7.\n\nwarning: Warning\nWe have deactivated the use of this constant for the moment and are only using eps(T) in BierlaireQuadratic. This is because solvers and optimizers should rely on different choices of this constant.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_k","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_k","text":"const DEFAULT_BRACKETING_k\n\nGives the default ratio by which the bracket is increased if bracketing was not successful. See bracket_minimum.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_nmax","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_nmax","text":"Default constant\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_s","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_s","text":"const DEFAULT_BRACKETING_s\n\nGives the default width of the interval (the bracket). See bracket_minimum.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_GRADIENT_ϵ","page":"Home","title":"SimpleSolvers.DEFAULT_GRADIENT_ϵ","text":"DEFAULT_GRADIENT_ϵ\n\nA constant on whose basis finite differences are computed.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER","page":"Home","title":"SimpleSolvers.DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER","text":"The default number of iterations before the Jacobian is refactored in the QuasiNewtonSolver\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_JACOBIAN_ϵ","page":"Home","title":"SimpleSolvers.DEFAULT_JACOBIAN_ϵ","text":"DEFAULT_JACOBIAN_ϵ\n\nA constant used for computing the finite difference Jacobian.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_WOLFE_c₁","page":"Home","title":"SimpleSolvers.DEFAULT_WOLFE_c₁","text":"const DEFAULT_WOLFE_c₁\n\nA constant epsilon on which a finite difference approximation of the derivative of the problem is computed. This is then used in the following stopping criterion:\n\nfracf(alpha) - f(alpha_0)epsilon  alphacdotf(alpha_0)\n\nExtended help\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_WOLFE_c₂","page":"Home","title":"SimpleSolvers.DEFAULT_WOLFE_c₂","text":"const DEFAULT_WOLFE_c₂\n\nThe constant used in the second Wolfe condition (the CurvatureCondition). According to [3, 4] we should have\n\nc_2 in (c_1 1)\n\nFurthermore [3] recommend c_2 = 09 and [4] write that \"it is common to set [c_2=01] when approximate line search is used with the conjugate gradient method and to 0.9 when used with Newton's method.\" We also use c_2 =DEFAULT_WOLFE_c₂ here.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_s_REDUCTION","page":"Home","title":"SimpleSolvers.DEFAULT_s_REDUCTION","text":"A factor by which s is reduced in each bracketing iteration (see bracket_minimum_with_fixed_point).\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.MAX_NUMBER_OF_ITERATIONS_FOR_QUADRATIC_LINESEARCH","page":"Home","title":"SimpleSolvers.MAX_NUMBER_OF_ITERATIONS_FOR_QUADRATIC_LINESEARCH","text":"This constant is used for Quadratic and BierlaireQuadratic.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.N_STATIC_THRESHOLD","page":"Home","title":"SimpleSolvers.N_STATIC_THRESHOLD","text":"Threshold for the maximum size a static matrix should have.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.AbstractLinearProblem","page":"Home","title":"SimpleSolvers.AbstractLinearProblem","text":"Encompasses the NoLinearProblem and the LinearProblem.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.AbstractNonlinearSolverCache","page":"Home","title":"SimpleSolvers.AbstractNonlinearSolverCache","text":"AbstractNonlinearSolverCache\n\nAn abstract type that comprises e.g. the NonlinearSolverCache.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.AbstractOptimizerProblem","page":"Home","title":"SimpleSolvers.AbstractOptimizerProblem","text":"AbstractOptimizerProblem\n\nAn optimizer problem is a quantity to has to be made zero by a solver or minimized by an optimizer.\n\nSee LinesearchProblem and OptimizerProblem.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BFGS","page":"Home","title":"SimpleSolvers.BFGS","text":"Algorithm taken from [3].\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BFGSCache","page":"Home","title":"SimpleSolvers.BFGSCache","text":"BFGSCache\n\nThe OptimizerCache for the BFGS algorithm. Also see update!(::BFGSCache, ::OptimizerState, ::AbstractVector, ::AbstractVector).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BFGSState","page":"Home","title":"SimpleSolvers.BFGSState","text":"BFGSState <: OptimizerState\n\nThe OptimizerState corresponding to the BFGS method.\n\nKeys\n\nx̄\nḡ\nf̄\nQ\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Backtracking","page":"Home","title":"SimpleSolvers.Backtracking","text":"Backtracking <: LinesearchMethod\n\nKeys\n\nThe keys are:\n\nα₀:\nϵ=$(DEFAULT_WOLFE_c₁): a default step size on whose basis we compute a finite difference approximation of the derivative of the problem. Also see DEFAULT_WOLFE_c₁.\np=$(DEFAULT_ARMIJO_p): a parameter with which alpha is decreased in every step until the stopping criterion is satisfied.\n\nFunctor\n\nThe functor is used the following way:\n\nls(obj, α = ls.α₀)\n\nImplementation\n\nThe algorithm starts by setting:\n\nx_0 gets 0\ny_0 gets f(x_0)\nd_0 gets f(x_0)\nalpha gets alpha_0\n\nwhere f is the univariate optimizer problem (of type LinesearchProblem) and alpha_0 is stored in ls. It then repeatedly does alpha gets alphacdotp until either (i) the maximum number of iterations is reached (the max_iterations keyword in Options) or (ii) the following holds:\n\n    f(alpha)  y_0 + epsilon cdot alpha cdot d_0\n\nwhere epsilon is stored in ls.\n\ninfo: Info\nThe algorithm allocates an instance of SufficientDecreaseCondition by calling SufficientDecreaseCondition(ls.ϵ, x₀, y₀, d₀, one(α), obj), here we take the value one for the search direction p, this is because we already have the search direction encoded into the line search problem.\n\nExtended help\n\nThe backtracking algorithm starts by setting y_0 gets f(0) and d_0 gets nabla_0f.\n\nThe algorithm is executed by calling the functor of Backtracking.\n\nThe following is then repeated until the stopping criterion is satisfied or config.max_iterations (1000 by default) is reached:\n\nif value(obj, α) ≥ y₀ + ls.ϵ * α * d₀\n    α *= ls.p\nelse\n    break\nend\n\nThe stopping criterion as an equation can be written as:\n\nf(lpha)  y_0 + psilon lpha \nabla_0f = y_0 + psilon (lpha - 0)\nabla_0f\n\nNote that if the stopping criterion is not reached, lpha is multiplied with p and the process continues.\n\nSometimes the parameters p and psilon have different names such as au and c.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BacktrackingCondition","page":"Home","title":"SimpleSolvers.BacktrackingCondition","text":"BacktrackingCondition\n\nAbstract type comprising the conditions that are used for checking step sizes for the backtracking line search (see Backtracking).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BierlaireQuadratic","page":"Home","title":"SimpleSolvers.BierlaireQuadratic","text":"BierlaireQuadratic <: Linesearch\n\nAlgorithm taken from [2].\n\nExtended help\n\nNote that the performance of BierlaireQuadratic may heavily depend on the choice of DEFAULT_BIERLAIRE_ε (i.e. the precision) and DEFAULT_BIERLAIRE_ξ.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Bisection","page":"Home","title":"SimpleSolvers.Bisection","text":"Bisection <: Linesearch\n\nSee bisection for the implementation of the algorithm.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BracketMinimumCriterion","page":"Home","title":"SimpleSolvers.BracketMinimumCriterion","text":"BracketMinimumCriterion <: BracketingCriterion\n\nThe criterion used for bracket_minimum.\n\nFunctor\n\nbc(yb, yc)\n\nThis checks whether yc is bigger than yb, i.e. whether c is past the minimum.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.CurvatureCondition","page":"Home","title":"SimpleSolvers.CurvatureCondition","text":"CurvatureCondition <: LinesearchCondition\n\nThe second of the Wolfe conditions [3]. The first one is the SufficientDecreaseCondition.\n\nThis encompasses the standard curvature condition and the strong curvature condition.\n\nConstructor\n\nCurvatureCondition(c, xₖ, gradₖ, pₖ, obj, grad; mode)\n\nHere grad has to be a Gradient and obj an AbstractOptimizerProblem. The other inputs are either arrays or numbers.\n\nImplementation\n\nFor computational reasons CurvatureCondition also has a field gradₖ₊₁ in which the temporary new gradient is saved.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.DFP","page":"Home","title":"SimpleSolvers.DFP","text":"Algorithm taken from [3].\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.DFPCache","page":"Home","title":"SimpleSolvers.DFPCache","text":"DFPCache <: OptimizerCache\n\nThe OptimizerCache corresponding to the DFP method.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.DFPState","page":"Home","title":"SimpleSolvers.DFPState","text":"DFPState\n\nThis is equivalent to BFGSState.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.FixedPointIterator-Union{Tuple{AT}, Tuple{T}, Tuple{AT, Union{Function, Type}, AT}} where {T, AT<:AbstractVector{T}}","page":"Home","title":"SimpleSolvers.FixedPointIterator","text":"FixedPointIterator(x, F)\n\nKeywords\n\noptions_kwargs: see Options\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.Gradient","page":"Home","title":"SimpleSolvers.Gradient","text":"Gradient\n\nAbstract type. structs that are derived from this need an associated functor that computes the gradient of a function (in-place).\n\nImplementation\n\nWhen a custom Gradient is implemented, a functor is needed:\n\n(grad::Gradient)(g::AbstractVector, x::AbstractVector)\n\nThere is also an out-of place version for convenience:\n\n(grad::Gradient)(x::AbstractVector)\n\nThis is using alloc_g to allocate the array g for the gradient.\n\nExamples\n\nExamples include:\n\nGradientFunction\nGradientAutodiff\nGradientFiniteDifferences\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientAutodiff","page":"Home","title":"SimpleSolvers.GradientAutodiff","text":"GradientAutodiff <: Gradient\n\nA struct that realizes Gradient by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\n∇config: result of applying ForwardDiff.GradientConfig.\n\nConstructors\n\nGradientAutodiff(F, x::AbstractVector)\nGradientAutodiff{T}(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\ngrad(g, x) = ForwardDiff.gradient!(g, grad.F, x, grad.∇config)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientFiniteDifferences","page":"Home","title":"SimpleSolvers.GradientFiniteDifferences","text":"GradientFiniteDifferences <: Gradient\n\nA struct that realizes Gradient by using finite differences.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nϵ: small constant on whose basis the finite differences are computed.\ne: auxiliary vector used for computing finite differences. It's of the form e_1 = beginbmatrix 1  0  cdots  0 endbmatrix.\ntx: auxiliary vector used for computing finite differences. It stores the offset in the x vector.\n\nConstructor(s)\n\nGradientFiniteDifferences{T}(F, nx::Integer; ϵ)\n\nBy default for ϵ is DEFAULT_GRADIENT_ϵ.\n\nFunctor\n\nThe functor does:\n\nfor j in eachindex(x,g)\n    ϵⱼ = grad.ϵ * x[j] + grad.ϵ\n    fill!(grad.e, 0)\n    grad.e[j] = 1\n    grad.tx .= x .- ϵⱼ .* grad.e\n    f1 = grad.F(grad.tx)\n    grad.tx .= x .+ ϵⱼ .* grad.e\n    f2 = grad.F(grad.tx)\n    g[j] = (f2 - f1) / (2ϵⱼ)\nend\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientFunction","page":"Home","title":"SimpleSolvers.GradientFunction","text":"GradientFunction <: Gradient\n\nA struct that realizes a Gradient by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\n∇F!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\ngrad(g, x) = grad.∇F!(g, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Hessian","page":"Home","title":"SimpleSolvers.Hessian","text":"Hessian\n\nAbstract type. structs derived from this need an associated functor that computes the Hessian of a function (in-place).\n\nAlso see Gradient.\n\nImplementation\n\nWhen a custom Hessian is implemented, a functor is needed:\n\nfunction (hessian::Hessian)(h::AbstractMatrix, x::AbstractVector) end\n\nExamples\n\nExamples include:\n\nHessianFunction\nHessianAutodiff\nHessianBFGS\nHessianDFP\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianAutodiff","page":"Home","title":"SimpleSolvers.HessianAutodiff","text":"HessianAutodiff <: Hessian\n\nA struct that realizes Hessian by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nH: a matrix in which the (updated) Hessian is stored. \nHconfig: result of applying ForwardDiff.HessianConfig.\n\nConstructors\n\nHessianAutodiff(F, x::AbstractVector)\nHessianAutodiff(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\nhes(g, x) = ForwardDiff.hessian!(hes.H, hes.F, x, grad.Hconfig)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianBFGS","page":"Home","title":"SimpleSolvers.HessianBFGS","text":"HessianBFGS <: Hessian\n\nA struct derived from Hessian to be used for an Optimizer.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianDFP","page":"Home","title":"SimpleSolvers.HessianDFP","text":"HessianDFP <: Hessian\n\nThe Hessian corresponding to the DFP method.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianFunction","page":"Home","title":"SimpleSolvers.HessianFunction","text":"HessianFunction <: Hessian\n\nA struct that realizes a Hessian by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\nH!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\nhes(H, x) = hes.H!(H, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.IterativeHessian","page":"Home","title":"SimpleSolvers.IterativeHessian","text":"IterativeHessian <: Hessian\n\nAn abstract type derived from Hessian. Its main purpose is defining a supertype that encompasses HessianBFGS and HessianDFP for dispatch.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Jacobian","page":"Home","title":"SimpleSolvers.Jacobian","text":"Jacobian\n\nAbstract type. structs that are derived from this need an associated functor that computes the Jacobian of a function (in-place).\n\nImplementation\n\nWhen a custom Jacobian is implemented, a functor is needed:\n\nfunction (j::Jacobian)(g::AbstractMatrix, x::AbstractVector) end\n\nExamples\n\nExamples include:\n\nJacobianFunction\nJacobianAutodiff\nJacobianFiniteDifferences\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianAutodiff","page":"Home","title":"SimpleSolvers.JacobianAutodiff","text":"JacobianAutodiff <: Jacobian\n\nA struct that realizes Jacobian by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nJconfig: result of applying ForwardDiff.JacobianConfig.\nty: vector that is used for evaluating ForwardDiff.jacobian!\n\nConstructors\n\nJacobianAutodiff(F, y::AbstractVector)\nJacobianAutodiff{T}(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\njac(J, x) = ForwardDiff.jacobian!(J, jac.ty, x, grad.Jconfig)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianFiniteDifferences","page":"Home","title":"SimpleSolvers.JacobianFiniteDifferences","text":"JacobianFiniteDifferences <: Jacobian\n\nA struct that realizes Jacobian by using finite differences.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nϵ: small constant on whose basis the finite differences are computed.\nf1:\nf2:\ne1: auxiliary vector used for computing finite differences. It's of the form e_1 = beginbmatrix 1  0  cdots  0 endbmatrix.\ne2:\ntx: auxiliary vector used for computing finite differences. It stores the offset in the x vector.\n\nConstructor(s)\n\nJacobianFiniteDifferences{T}(F, nx::Integer, ny::Integer; ϵ)\n\nBy default for ϵ is DEFAULT_JACOBIAN_ϵ.\n\nFunctor\n\nThe functor does:\n\nfor j in eachindex(x)\n    ϵⱼ = jac.ϵ * x[j] + jac.ϵ\n    fill!(jac.e, 0)\n    jac.e[j] = 1\n    jac.tx .= x .- ϵⱼ .* jac.e\n    f(jac.f1, jac.tx)\n    jac.tx .= x .+ ϵⱼ .* jac.e\n    f(jac.f2, jac.tx)\n    for i in eachindex(x)\n        J[i,j] = (jac.f2[i] - jac.f1[i]) / (2ϵⱼ)\n    end\nend\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianFunction","page":"Home","title":"SimpleSolvers.JacobianFunction","text":"JacobianFunction <: Jacobian\n\nA struct that realizes a Jacobian by explicitly supplying a function taken from the NonlinearProblem.\n\nFunctor\n\nThere is no functor associated to JacobianFunction.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LU","page":"Home","title":"SimpleSolvers.LU","text":"struct LU <: DirectMethod\n\nA custom implementation of an LU solver, meant to solve a LinearProblem.\n\nRoutines that use the LU solver include factorize!, ldiv! and solve!. In practice the LU solver is used by calling the LinearSolver constructor and ldiv! or solve!, or with an instance of LU as an argument directly, as shown in the Example section of this docstring.\n\nconstructor\n\nThe constructor is called with either no argument:\n\nLU()\n\n# output\n\nLU{Missing}(missing, true)\n\nor with pivot and static as optional booleans:\n\nLU(; pivot=true, static=true)\n\n# output\n\nLU{Bool}(true, true)\n\nNote that if we do not supply an explicit keyword static, the corresponding field is missing (as in the first case). Also see _static.\n\nExample\n\nWe use the LU together with solve to solve a linear system:\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\nv = rand(3)\nls = LinearProblem(A, v)\nupdate!(ls, A, v)\n\nlu = LU()\n\nsolve(lu, ls) ≈ inv(A) * v\n\n# output\n\ntrue\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LUSolverCache","page":"Home","title":"SimpleSolvers.LUSolverCache","text":"LUSolverCache <: LinearSolverCache\n\nKeys\n\nA: the factorized matrix A,\npivots:\nperms:\ninfo\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LUSolverLAPACK","page":"Home","title":"SimpleSolvers.LUSolverLAPACK","text":"LUSolverLAPACK <: LinearSolver\n\nThe LU Solver taken from LinearAlgebra.BLAS.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearProblem","page":"Home","title":"SimpleSolvers.LinearProblem","text":"LinearProblem\n\nA LinearProblem describes Ax = y, where we want to solve for x.\n\nKeys\n\nA\ny\n\nConstructors\n\nA LinearProblem can be allocated by calling:\n\nLinearProblem(A, y)\nLinearProblem(A)\nLinearProblem(y)\nLinearProblem{T}(n, m)\nLinearProblem{T}(n)\n\nNote that in any case the allocated system is initialized with NaNs:\n\nA = [1. 2. 3.; 4. 5. 6.; 7. 8. 9.]\ny = [1., 2., 3.]\nls = LinearProblem(A, y)\n\n# output\n\nLinearProblem{Float64, Vector{Float64}, Matrix{Float64}}([NaN NaN NaN; NaN NaN NaN; NaN NaN NaN], [NaN, NaN, NaN])\n\nIn order to initialize the system with values, we have to call update!:\n\nupdate!(ls, A, y)\n\n# output\n\nLinearProblem{Float64, Vector{Float64}, Matrix{Float64}}([1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0], [1.0, 2.0, 3.0])\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSolver","page":"Home","title":"SimpleSolvers.LinearSolver","text":"LinearSolver <: AbstractSolver\n\nA struct that stores LinearSolverMethods and LinearSolverCaches. LinearSolvers are used to solve LinearProblems.\n\nConstructors\n\nLinearSolver(method, cache)\nLinearSolver(method, A)\nLinearSolver(method, ls::LinearProblem)\nLinearSolver(method, x)\n\ninfo: Info\nWe note that the constructors do not call the function factorize, so only allocate a new matrix. The factorization needs to be done manually.\n\nYou can manually factorize by either calling factorize! or solve!.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSolverCache","page":"Home","title":"SimpleSolvers.LinearSolverCache","text":"LinearSolverCache\n\nAn abstract type that summarizes all the caches used for LinearSolvers. See e.g. LUSolverCache.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSolverMethod","page":"Home","title":"SimpleSolvers.LinearSolverMethod","text":"LinearSolverMethod <: SolverMethod\n\nSummarizes all the methods used for solving linear systems of equations such as the LU method.\n\nExtended help\n\nThe abstract type SolverMethod was imported from GeometricBase.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Linesearch","page":"Home","title":"SimpleSolvers.Linesearch","text":"Linesearch\n\nA struct that stores the LinesearchMethod and Options.\n\nKeys\n\nalgorithm::LinesearchMethod\nconfig::Options\n\nConstructors\n\nThe following constructors can be used:\n\nLinesearch(alg, config)\nLinesearch(; algorithm, config, kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinesearchMethod","page":"Home","title":"SimpleSolvers.LinesearchMethod","text":"LinesearchMethod\n\nExamples include Static, Backtracking, Bisection and Quadratic. See these examples for specific information on linesearch algorithms.\n\nExtended help\n\nA LinesearchMethod always has to be used together in Linesearch (or with solve).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinesearchProblem","page":"Home","title":"SimpleSolvers.LinesearchProblem","text":"LinesearchProblem <: AbstractOptimizerProblem\n\nDoesn't store f, d, x_f and x_d.\n\nIn practice LinesearchProblems are allocated by calling linesearch_problem.\n\nConstructors\n\nwarning: Calling line search problems\nBelow we show a few constructors that can be used to allocate LinesearchProblems. Note however that in practice one probably should not do that and instead call linesearch_problem.\n\nf(x) = x^2 - 1\ng(x) = 2x\nδx(x) = - g(x) / 2\nx₀ = 3.\n_f(α) = f(compute_new_iterate(x₀, α, δx(x₀)))\n_d(α) = g(compute_new_iterate(x₀, α, δx(x₀)))\nls_obj = LinesearchProblem{typeof(x₀)}(_f, _d)\n\n# output\n\nLinesearchProblem{Float64, typeof(_f), typeof(_d)}(_f, _d)\n\nAlternatively one can also do:\n\nls_obj = LinesearchProblem(_f, _d, x₀)\n\n# output\n\nLinesearchProblem{Float64, typeof(_f), typeof(_d)}(_f, _d)\n\nHere we wrote ls_obj to mean line search problem.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonMethod","page":"Home","title":"SimpleSolvers.NewtonMethod","text":"NewtonMethod(refactorize)\n\nMake an instance of a quasi Newton solver based on an integer refactorize that determines how often the rhs is refactored.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonOptimizerCache","page":"Home","title":"SimpleSolvers.NewtonOptimizerCache","text":"NewtonOptimizerCache\n\nKeys\n\nx: current iterate (this stores the guess called by the functions generated with linesearch_problem),\nδ: direction of optimization step (difference between x and x̄); this is obtained by multiplying rhs with the inverse of the Hessian,\ng: gradient value (this stores the gradient associated with x called by the derivative part of linesearch_problem),\nrhs: the right hand side used to compute the update.\n\nTo understand how these are used in practice see e.g. linesearch_problem.\n\nAlso compare this to NonlinearSolverCache.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonOptimizerState","page":"Home","title":"SimpleSolvers.NewtonOptimizerState","text":"NewtonOptimizerState <: OptimizerState\n\nThe optimizer state is needed to update the Optimizer. This is different to OptimizerStatus and OptimizerResult which serve as diagnostic tools.\n\nNote that this is also used for the BFGS and the DFP optimizer.\n\nKeys\n\nx̄\nḡ\nf̄\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonSolver","page":"Home","title":"SimpleSolvers.NewtonSolver","text":"NewtonSolver\n\nA const derived from NonlinearSolver\n\nConstructors\n\nThe NewtonSolver can be called with an NonlinearProblem or with a Callable. Note however that the latter will probably be deprecated in the future.\n\nWhat is shown here is the status of the NewtonSolver, i.e. an instance of NonlinearSolverStatus.\n\nKeywords\n\nnonlinearproblem::NonlinearProblem: the system that has to be solved. This can be accessed by calling nonlinearproblem,\njacobian::Jacobian\nlinear::LinearSolver: the linear solver is used to compute the direction of the solver step (see solver_step!). This can be accessed by calling linearsolver,\nlinesearch::Linesearch\nrefactorize::Int: determines after how many steps the Jacobian is updated and refactored (see factorize!). If we have refactorize > 1, then we speak of a QuasiNewtonSolver,\ncache::NonlinearSolverCache\nconfig::Options\nstatus::NonlinearSolverStatus:\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonSolver-Union{Tuple{AT}, Tuple{T}, Tuple{AT, Union{Function, Type}, AT}} where {T, AT<:AbstractVector{T}}","page":"Home","title":"SimpleSolvers.NewtonSolver","text":"NewtonSolver(x, F, y)\n\nKeywords\n\nlinear_solver_method\nDF!\nlinesearch\nmode\noptions_kwargs: see Options\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.NoLinearProblem","page":"Home","title":"SimpleSolvers.NoLinearProblem","text":"A dummy linear system used for the fixed point iterator (PicardMethod).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearMethod","page":"Home","title":"SimpleSolvers.NonlinearMethod","text":"A supertype collecting all nonlinear methods, including NewtonMethods.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearProblem","page":"Home","title":"SimpleSolvers.NonlinearProblem","text":"NonlinearProblem\n\nA NonlinearProblem describes F(x) = y, where we want to solve for x and F is in nonlinear in general (also compare this to LinearProblem and OptimizerProblem).\n\ninfo: Info\nNonlinearProblems are used for solvers whereas OptimizerProblems are their equivalent for optimizers.\n\nKeys\n\nF: accessed by calling Function(nlp),\nJ::Union{Callable, Missing}: accessed by calling jacobian(nlp),\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearProblem-Tuple{Union{Function, Type}, AbstractArray, AbstractArray}","page":"Home","title":"SimpleSolvers.NonlinearProblem","text":"NonlinearProblem(F, x, f)\n\nSet jacobian gets missing and call the NonlinearProblem constructor.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.NonlinearSolver","page":"Home","title":"SimpleSolvers.NonlinearSolver","text":"NonlinearSolver\n\nA struct that comprises Newton solvers (see NewtonMethod) and the fixed point iterator (see PicardMethod).\n\nConstructors\n\nNonlinearSolver(x, nlp, ls, linearsolver, linesearch, cache; method)\n\nThe NonlinearSolver can be called with an NonlinearProblem or with a Callable. Note however that the latter will probably be deprecated in the future. See NewtonSolver for examples (as well as NonlinearSolverStatus).\n\nIt's arguments are:\n\nnlp::NonlinearProblem: the system that has to be solved. This can be accessed by calling nonlinearproblem,\nls::LinearProblem,\nlinearsolver::LinearSolver: the linear solver is used to compute the direction of the solver step (see solver_step!). This can be accessed by calling linearsolver,\nlinesearch::Linesearch\ncache::NonlinearSolverCache\nconfig::Options\nstatus::NonlinearSolverStatus:\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSolverCache","page":"Home","title":"SimpleSolvers.NonlinearSolverCache","text":"NonlinearSolverCache\n\nStores x, Δx, rhs, y, and J.\n\nCompare this to NewtonOptimizerCache.\n\nKeys\n\nx: the next iterate (or guess thereof). The guess is computed when calling the functions created by linesearch_problem,\nΔx: search direction. This is updated when calling solver_step! via the LinearSolver stored in the NewtonSolver,\nrhs: the right-hand-side (this can be accessed by calling rhs),\ny: the problem evaluated at x. This is used in linesearch_problem,\nj::AbstractMatrix: the Jacobian evaluated at x. This is used in linesearch_problem. Note that this is not of type Jacobian!\n\nConstructor\n\nNonlinearSolverCache(x, y)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSolverState","page":"Home","title":"SimpleSolvers.NonlinearSolverState","text":"NonlinearSolverState(x, y)\nNonlinearSolverState(x)\nNonlinearSolverState{T}(n, m)\nNonlinearSolverState{T}(n)\n\nThe NonlinearSolverState to be used together with a NonlinearSolver.\n\nwarn: Warn\nNote the difference to the NonlinearSolverCache and the NonlinearSolverStatus.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSolverStatus","page":"Home","title":"SimpleSolvers.NonlinearSolverStatus","text":"NonlinearSolverStatus\n\nStores absolute, relative and successive residuals for x and f. It is used as a diagnostic tool in NewtonSolver.\n\nKeys\n\niteration: number of iterations\nrxₛ: successive residual in x,\nrfₐ: absolute residual in f,\nrfₛ: successive residual in f,\nx_converged::Bool\nf_converged::Bool\nf_increased::Bool\n\nExamples\n\nx = [1., 2., 3., 4.]\nstate = NonlinearSolverState(x)\ncache = NonlinearSolverCache(x, x)\nconfig = Options()\nNonlinearSolverStatus(state, config)\n\n# output\n\ni=   0,\nrxₛ= NaN,\nrfₐ= NaN,\nrfₛ= NaN\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Optimizer","page":"Home","title":"SimpleSolvers.Optimizer","text":"Optimizer\n\nThe optimizer that stores all the information needed for an optimization problem. This problem can be solved by calling solve!(::AbstractVector, ::Optimizer).\n\nKeys\n\nalgorithm::OptimizerState,\nproblem::OptimizerProblem,\nconfig::Options,\nstate::OptimizerState.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerCache","page":"Home","title":"SimpleSolvers.OptimizerCache","text":"OptimizerCache\n\nSee NewtonOptimizerCache and BFGSCache.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerMethod","page":"Home","title":"SimpleSolvers.OptimizerMethod","text":"OptimizerMethod <: SolverMethod\n\nThe OptimizerMethod is used in Optimizer and determines the algorithm that is used.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerProblem","page":"Home","title":"SimpleSolvers.OptimizerProblem","text":"OptimizerProblem <: AbstractOptimizerProblem\n\nStores gradients. Also compare this to NonlinearProblem.\n\nThe type of the stored gradient has to be a subtype of Gradient.\n\nFunctor\n\nIf OptimizerProblem is called on a single function, the gradient is generated with GradientAutodiff.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerResult","page":"Home","title":"SimpleSolvers.OptimizerResult","text":"OptimizerResult\n\nStores x, f and g (as keys).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerState","page":"Home","title":"SimpleSolvers.OptimizerState","text":"An OptimizerState is a data structure that is used to dispatch on different algorithms.\n\nIt needs to implement three methods,\n\ninitialize!(alg::OptimizerState, ::AbstractVector)\nupdate!(alg::OptimizerState, ::AbstractVector)\nsolver_step!(::AbstractVector, alg::OptimizerState)\n\nthat initialize and update the state of the algorithm and perform an actual optimization step.\n\nFurther the following convenience methods should be implemented,\n\nproblem(alg::OptimizerState)\ngradient(alg::OptimizerState)\nhessian(alg::OptimizerState)\nlinesearch(alg::OptimizerState)\n\nwhich return the problem to optimize, its gradient and (approximate) Hessian as well as the linesearch algorithm used in conjunction with the optimization algorithm if any.\n\nSee NewtonOptimizerState for a struct that was derived from OptimizerState.\n\ninfo: Info\nNote that a OptimizerState is not necessarily a NewtonOptimizerState as we can also have other optimizers, Adam for example.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerStatus","page":"Home","title":"SimpleSolvers.OptimizerStatus","text":"OptimizerStatus\n\nContains residuals (relative and absolute) and various convergence properties.\n\nSee OptimizerResult.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerStatus-Union{Tuple{OCT}, Tuple{OST}, Tuple{T}, Tuple{OST, OCT, T}} where {T, OST<:OptimizerState{T}, OCT<:SimpleSolvers.OptimizerCache{T}}","page":"Home","title":"SimpleSolvers.OptimizerStatus","text":"residual!(status, state, cache, f)\n\nCompute the residual based on previous iterates (x̄, f̄, ḡ) (stored in e.g. NewtonOptimizerState) and current iterates (x, f, g) (partly stored in e.g. NewtonOptimizerCache).\n\nAlso meets_stopping_criteria.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.Options","page":"Home","title":"SimpleSolvers.Options","text":"Options\n\nKeys\n\nConfigurable options with defaults (values 0 and NaN indicate unlimited):\n\nx_abstol = 2eps(T): absolute tolerance for x (the function argument). Used in e.g. assess_convergence and bisection,\nx_reltol = 2eps(T): relative tolerance for x (the function argument). Used in e.g. assess_convergence,\nx_suctol = 2eps(T): succesive tolerance for x. Used in e.g. assess_convergence,\nf_abstol = zero(T): absolute tolerance for how close the function value should be to zero. See absolute_tolerance. Used in e.g. bisection and assess_convergence,\nf_reltol = 2eps(T): relative tolerance for the function value. Used in e.g. assess_convergence,\nf_suctol = 2eps(T): succesive tolerance for the function value. Used in e.g. assess_convergence,\nf_mindec = T(10)^-4: minimum value by which the function has to decrease (also see minimum_decrease_threshold),\ng_restol = 2eps(T): tolerance for the residual (?) of the gradient,\nx_abstol_break = -Inf: see meets_stopping_criteria,\nx_reltol_break = Inf: see meets_stopping_criteria,\nf_abstol_break = Inf: see meets_stopping_criteria,\nf_reltol_break = Inf: see meets_stopping_criteria.,\ng_restol_break = Inf,\nallow_f_increases = true,\nmin_iterations = 0,\nmax_iterations = 1000: the maximum number of iterations used in an alorithm, e.g. bisection and the functor for Backtracking,\nwarn_iterations = 1000,\nshow_trace = false,\nstore_trace = false,\nextended_trace = false,\nshow_every = 1,\nverbosity = 1\nlinesearch_nan_max_iterations = 10\nlinesearch_nan_factor = 0.5\n\nSome of the constants are defined by the functions default_tolerance and absolute_tolerance.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.PicardMethod","page":"Home","title":"SimpleSolvers.PicardMethod","text":"PicardMethod()\n\nMake an instance of a Picard solver (fixed point iterator).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Quadratic","page":"Home","title":"SimpleSolvers.Quadratic","text":"Quadratic <: LinesearchMethod\n\nQuadratic Polynomial line search. Performs multiple iterations in which all parameters p_0, p_1 and p_2 are changed. This is different from the old Quadratic (taken from [1]), where only p_2 is changed. We further do not check the SufficientDecreaseCondition but rather whether the derivative is small enough.\n\nwarning: Warning\nThe old Quadratic was deprecated!\n\nThis algorithm repeatedly builds new quadratic polynomials until a minimum is found (to sufficient accuracy). The iteration may also stop after we reaches the maximum number of iterations (see MAX_NUMBER_OF_ITERATIONS_FOR_QUADRATIC_LINESEARCH).\n\nKeywords\n\nconfig::Options\nε: A constant that checks the precision/tolerance.\ns: A constant that determines the initial interval for bracketing. By default this is DEFAULT_BRACKETING_s.\ns_reduction: A constant that determines the factor by which s is decreased in each new bracketing iteration.\n\nExtended help\n\nThe quadratic method. Compare this to BierlaireQuadratic. The algorithm is adjusted from [1].\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.QuasiNewtonOptimizerMethod","page":"Home","title":"SimpleSolvers.QuasiNewtonOptimizerMethod","text":"QuasiNewtonOptimizerMethod <: OptimizerMethod\n\nIncludes BFGS and DFP.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Static","page":"Home","title":"SimpleSolvers.Static","text":"Static <: LinesearchMethod\n\nThe static method.\n\nConstructors\n\nStatic(α)\n\nKeys\n\nKeys include: -α: equivalent to a step size. The default is 1.\n\nExtended help\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.SufficientDecreaseCondition","page":"Home","title":"SimpleSolvers.SufficientDecreaseCondition","text":"SufficientDecreaseCondition <: LinesearchCondition\n\nThe condition that determines if alpha_k is big enough.\n\nConstructor\n\nSufficientDecreaseCondition(c₁, xₖ, fₖ, gradₖ, pₖ, obj)\n\nFunctors\n\nsdc(xₖ₊₁, αₖ)\nsdc(αₖ)\n\nThe second functor is shorthand for sdc(compute_new_iterate(sdc.xₖ, αₖ, sdc.pₖ), T(αₖ)), also see compute_new_iterate!.\n\nExtended help\n\nWe call the constant that pertains to the sufficient decrease condition c. This is typically called c_1 in the literature [3]. See DEFAULT_WOLFE_c₁ for the relevant constant\n\n\n\n\n\n","category":"type"},{"location":"#GeometricBase.update!-Tuple{FixedPointIterator{T} where T, AbstractArray, Any}","page":"Home","title":"GeometricBase.update!","text":"update!(iterator, x, params)\n\nUpdate the solver::FixedPointIterator based on x. This updates the cache (instance of type NonlinearSolverCache) and the status (instance of type NonlinearSolverStatus). In course of updating the latter, we also update the nonlinear stored in iterator (and status(iterator)).\n\ninfo: Info\nAt the moment this is neither used in solver_step! nor solve!.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{NewtonOptimizerState, Gradient, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(state::NewtonOptimizerState, gradient, x)\n\nUpdate an instance of NewtonOptimizerState based on x, g and hes, where g can either be an AbstractVector or a Gradient and hes is a Hessian. This updates the NewtonOptimizerCache contained in the NewtonOptimizerState by calling update!(::NewtonOptimizerCache, ::AbstractVector, ::Union{AbstractVector, Gradient}, ::Hessian).\n\nExamples\n\nWe show that after initializing, update has to be called together with a Gradient and a Hessian:\n\nIf we only call update! once there are still NaNs in the ...\n\nf(x) = sum(x.^2)\nx = [1., 2.]\nstate = NewtonOptimizerState(x)\ngrad = GradientAutodiff{Float64}(f, length(x))\nupdate!(state, grad, x)\n\n# output\n\nNewtonOptimizerState{Float64, Vector{Float64}, Vector{Float64}}([1.0, 2.0], [2.0, 4.0], 0.0, 0)\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{NewtonSolver{T} where T, NonlinearSolverState, AbstractArray, Any}","page":"Home","title":"GeometricBase.update!","text":"update!(solver, x, params)\n\nUpdate the solver::NewtonSolver based on x. This updates the cache (instance of type NonlinearSolverCache) and the status (instance of type NonlinearSolverStatus). In course of updating the latter, we also update the nonlinear stored in solver (and status(solver)).\n\ninfo: Info\nAt the moment this is neither used in solver_step! nor solve!.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{Optimizer, OptimizerState, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(opt, x)\n\nCompute problem and gradient at new solution.\n\nThis first calls update!(::OptimizerResult, ::AbstractVector, ::AbstractVector, ::AbstractVector) and then update!(::NewtonOptimizerState, ::AbstractVector). We note that the OptimizerStatus (unlike the NewtonOptimizerState) is updated when calling update!(::OptimizerResult, ::AbstractVector, ::AbstractVector, ::AbstractVector).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{SimpleSolvers.NewtonOptimizerCache, OptimizerState, AbstractVector, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(cache, x, g)\n\nUpdate the NewtonOptimizerCache based on x and g.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Tuple{SimpleSolvers.NewtonOptimizerCache, OptimizerState, Gradient, Hessian, AbstractVector}","page":"Home","title":"GeometricBase.update!","text":"update!(cache::NewtonOptimizerCache, x, g, hes)\n\nUpdate an instance of NewtonOptimizerCache based on x.\n\nThis is used in update!(::OptimizerState, ::AbstractVector).\n\nThis sets:\n\nbarx^mathttcache gets x\nx^mathttcache gets x\ng^mathttcache gets g\nmathrmrhs^mathttcache gets -g\ndelta^mathttcache gets H^-1mathrmrhs^mathttcache\n\nwhere we wrote H for the Hessian (i.e. the input argument hes).\n\nAlso see update!(::NonlinearSolverCache, ::AbstractVector).\n\nwarning: Warning\nNote that this is not updating the Hessian hes. For this call update! on the Hessian.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Union{Tuple{HT}, Tuple{HT, AbstractVector}} where HT<:Hessian","page":"Home","title":"GeometricBase.update!","text":"update!(hessian, x)\n\nUpdate the Hessian based on the vector x. For an explicit example see e.g. update!(::HessianAutodiff).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Union{Tuple{T}, Tuple{LinearProblem{T, VT, AT} where {VT<:AbstractVector{T}, AT<:AbstractMatrix{T}}, AbstractMatrix{T}, AbstractVector{T}}} where T","page":"Home","title":"GeometricBase.update!","text":"update!(ls, A, y)\n\nSet the rhs vector to y and the matrix stored in ls to A.\n\ninfo: Info\nCalling update! doesn't solve the LinearProblem, you still have to call solve! in combination with a LinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Union{Tuple{T}, Tuple{SimpleSolvers.BFGSCache{T}, SimpleSolvers.BFGSState{T, AT, GT, MT} where {AT<:(AbstractArray{T}), GT<:(AbstractArray{T}), MT<:AbstractMatrix{T}}, AbstractVector{T}, AbstractVector{T}}} where T","page":"Home","title":"GeometricBase.update!","text":"update!(cache, x, g)\n\nUpdate the BFGSCache based on x and g.\n\nExtended help\n\nThe update rule used here can be found in [4] and [3]:\n\nWhat this is doing is:\n\nbeginaligned\ndelta  gets x^(k) - x^(k-1) \ngamma  gets nablaf^(k) - nablaf^(k-1) \nT_1  gets deltagamma^TQ \nT_2  gets Qgammadelta^T \nT_3  gets (1 + fracgamma^TQgammadelta^gamma)deltadelta^T\nQ  gets Q - (T_1 + T_2 - T_3)delta^Tgamma\nendaligned\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.update!-Union{Tuple{T}, Tuple{SimpleSolvers.DFPCache{T}, SimpleSolvers.BFGSState{T, AT, GT, MT} where {AT<:(AbstractArray{T}), GT<:(AbstractArray{T}), MT<:AbstractMatrix{T}}, AbstractVector{T}, AbstractVector{T}}} where T","page":"Home","title":"GeometricBase.update!","text":"update!(cache, x, g)\n\nUpdate the DFPCache based on x and g.\n\nExtended help\n\nThe update rule used here can be found in [4] and [3].\n\n\n\n\n\n","category":"method"},{"location":"#GeometricBase.value-Tuple{SimpleSolvers.AbstractOptimizerProblem, Union{Number, AbstractArray{<:Number}}}","page":"Home","title":"GeometricBase.value","text":"value(obj::AbstractOptimizerProblem, x)\n\nEvaluates the value at x (i.e. computes obj.F(x)).\n\n\n\n\n\n","category":"method"},{"location":"#LinearAlgebra.ldiv!-Union{Tuple{LUT}, Tuple{T}, Tuple{AbstractVector{T}, LinearSolver{T, LUT}, AbstractVector{T}}} where {T, LUT<:LU}","page":"Home","title":"LinearAlgebra.ldiv!","text":"ldiv!(x, lu, b)\n\nCompute inv(cache(lsolver).A) * b by utilizing the factorization of the lu solver (see LU and LinearSolver) and store the result in x.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.QuasiNewtonSolver-Tuple","page":"Home","title":"SimpleSolvers.QuasiNewtonSolver","text":"QuasiNewtonSolver\n\nA convenience constructor for NewtonSolver. Also see DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER.\n\nCalling QuasiNewtonSolver hence produces an instance of NewtonSolver with the difference that refactorize ≠ 1. The Jacobian is thus not evaluated and refactored in every step.\n\nImplementation\n\nIt does:\n\nQuasiNewtonSolver(args...; kwargs...) = NewtonSolver(args...; refactorize=DEFAULT_ITERATIONS_QUASI_NEWTON_SOLVER, kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers._static-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers._static","text":"_static(A)\n\nDetermine whether to allocate a StaticArray or simply copy the input array. This is used when calling LinearSolverCache on LU. Every matrix that is smaller or equal to N_STATIC_THRESHOLD is turned into a StaticArray as a consequence.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.absolute_tolerance-Union{Tuple{Type{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"SimpleSolvers.absolute_tolerance","text":"absolute_tolerance(T)\n\nDetermine the absolute tolerance for a specific data type. This is used in the constructor of Options.\n\nIn comparison to default_tolerance, this should return a very small number, close to zero (i.e. not just machine precision).\n\nExamples\n\nabsolute_tolerance(Float64)\n\n# output\n\n0.0\n\nabsolute_tolerance(Float32)\n\n# output\n\n0.0f0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.alloc_d","page":"Home","title":"SimpleSolvers.alloc_d","text":"alloc_d(x)\n\nAllocate NaNs of the size of the derivative of f (with respect to x).\n\nThis is used in combination with a LinesearchProblem.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_f","page":"Home","title":"SimpleSolvers.alloc_f","text":"alloc_f(x)\n\nAllocate NaNs of the size the size of f (evaluated at x).\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_g","page":"Home","title":"SimpleSolvers.alloc_g","text":"alloc_g(x)\n\nAllocate NaNs of the size of the gradient of f (with respect to x).\n\nThis is used in combination with a OptimizerProblem.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_h","page":"Home","title":"SimpleSolvers.alloc_h","text":"alloc_h(x)\n\nAllocate NaNs of the size of the Hessian of f (with respect to x).\n\nThis is used in combination with a OptimizerProblem.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_x","page":"Home","title":"SimpleSolvers.alloc_x","text":"alloc_x(x)\n\nAllocate NaNs of the size of x.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.assess_convergence-Tuple{Number, Number, Number, Options, NonlinearSolverState}","page":"Home","title":"SimpleSolvers.assess_convergence","text":"assess_convergence(rxₛ, rfₐ, rfₛ, config, cache, state)\n\nCheck if one of the following is true for status::NonlinearSolverStatus:\n\nrxₛ ≤ config.x_suctol,\nrfₐ ≤ config.f_abstol,\nrfₛ ≤ config.f_suctol.\n\nAlso see meets_stopping_criteria. The tolerances are by default determined with default_tolerance.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bisection-Tuple{Any, Number}","page":"Home","title":"SimpleSolvers.bisection","text":"bisection(f, x)\n\nUse bracket_minimum to find a starting interval and then do bisections.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bisection-Union{Tuple{T}, Tuple{Union{Function, Type}, T, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bisection","text":"bisection(f, xmin, xmax; config)\n\nPerform bisection of f in the interval [xmin, xmax] with Options config.\n\nThe algorithm is repeated until a root is found (up to tolerance config.f_abstol which is determined by default_tolerance by default).\n\nimplementation\n\nWhen calling bisection it first checks if x_mathrmmin  x_mathrmmax and else flips the two entries.\n\nExtended help\n\nThe bisection algorithm divides an interval into equal halves until a root is found (up to a desired accuracy).\n\nWe first initialize:\n\nbeginaligned\nx_0 gets  x_mathrmmin\nx_1 gets  x_mathrmmax\nendaligned\n\nand then repeat:\n\nbeginaligned\n x gets fracx_0 + x_12 \n textif f(x_0)f(x)  0 \n qquad x_0 gets x \n textelse \n qquad x_1 gets x \n textend\nendaligned\n\nSo the algorithm checks in each step where the sign change occurred and moves the x_0 or x_1 accordingly. The loop is terminated (and errors) if config.max_iterations is reached (by default1000 and the Options struct).\n\nwarning: Warning\nThe obvious danger with using bisections is that the supplied interval can have multiple roots (or no roots). One should be careful to avoid this when fixing the interval.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_minimum-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_minimum","text":"bracket_minimum(f, x)\n\nMove a bracket successively in the search direction (starting at x) and increase its size until a local minimum of f is found.  This is used for performing Bisections when only one x is given (and not an entire interval).  This bracketing algorithm is taken from [4]. Also compare it to bracket_minimum_with_fixed_point.\n\nKeyword arguments\n\ns::DEFAULT_BRACKETING_s\nk::DEFAULT_BRACKETING_k\nnmax::DEFAULT_BRACKETING_nmax\n\nExtended help\n\nFor bracketing we need two constants s and k (see DEFAULT_BRACKETING_s and DEFAULT_BRACKETING_k). \n\nBefore we start the algorithm we initialize it, i.e. we check that we indeed have a descent direction:\n\nbeginaligned\n a gets x \n b gets a + s \n mathrmif quad f(b)  f(a)\n qquadtextFlip a and b and set sgets-s\n mathrmend\nendaligned\n\nThe algorithm then successively computes:\n\nc gets b + s\n\nand then checks whether f(c)  f(b). If this is true it returns (a c) or (c a), depending on whether ac or ca respectively. If this is not satisfied a b and s are updated:\n\nbeginaligned\na gets  b \nb gets  c \ns gets  sk \nendaligned\n\nand the algorithm is continued. If we have not found a sign chance after n_mathrmmax iterations (see DEFAULT_BRACKETING_nmax) the algorithm is terminated and returns an error. The interval that is returned by bracket_minimum is then typically used as a starting point for bisection.\n\ninfo: Info\nThe function bracket_root is equivalent to bracket_minimum with the only difference that the criterion we check for is:f(c)f(b)  0i.e. that a sign change in the function occurs.\n\nSee bracket_root.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_minimum_with_fixed_point-Union{Tuple{T}, Tuple{Union{Function, Type}, Union{Function, Type}}, Tuple{Union{Function, Type}, Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_minimum_with_fixed_point","text":"bracket_minimum_with_fixed_point(f, x)\n\nFind a bracket while keeping the left side (i.e. x) fixed.  The algorithm is similar to bracket_minimum (also based on DEFAULT_BRACKETING_s and DEFAULT_BRACKETING_k) with the difference that for the latter the left side is also moving.\n\nThe function bracket_minimum_with_fixed_point is used as a starting point for Quadratic (taken from [1]), as the minimum of the polynomial approximation is:\n\np_2 = fracf(b) - f(a) - f(0)bb^2\n\nwhere b = mathttbracket_minimum_with_fixed_point(a). We check that f(b)  f(a) in order to ensure that the curvature of the polynomial (i.e. p_2 is positive) and we have a minimum.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_root-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_root","text":"bracket_root(f, x)\n\nMake a bracket for the function based on x (for root finding).\n\nThis is largely equivalent to bracket_minimum. See the end of that docstring for more information.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.cache-Tuple{LinearSolver}","page":"Home","title":"SimpleSolvers.cache","text":"cache(ls)\n\nReturn the cache (of type LinearSolverCache) of the LinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_gradient-Tuple{AbstractVector}","page":"Home","title":"SimpleSolvers.check_gradient","text":"check_gradient(g)\n\nCheck norm, maximum value and minimum value of a vector.\n\nExamples\n\nusing SimpleSolvers\n\ng = [1., 1., 1., 2., 0.9, 3.]\nSimpleSolvers.check_gradient(g; digits=3)\n\n# output\n\nnorm(Gradient):               4.1\nminimum(|Gradient|):          0.9\nmaximum(|Gradient|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_hessian-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers.check_hessian","text":"check_hessian(H)\n\nCheck the condition number, determinant, max and min value of the Hessian H.\n\nusing SimpleSolvers\n\nH = [1. √2.; √2. 3.]\nSimpleSolvers.check_hessian(H)\n\n# output\n\nCondition Number of Hessian: 13.9282\nDeterminant of Hessian:      1.0\nminimum(|Hessian|):          1.0\nmaximum(|Hessian|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_jacobian-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers.check_jacobian","text":"check_jacobian(J)\n\nCheck the condition number, determinant, max and min value of the Jacobian J.\n\nusing SimpleSolvers\n\nJ = [1. √2.; √2. 3.]\nSimpleSolvers.check_jacobian(J)\n\n# output\n\nCondition Number of Jacobian: 13.9282\nDeterminant of Jacobian:      1.0\nminimum(|Jacobian|):          1.0\nmaximum(|Jacobian|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Union{Tuple{LinearProblem{T, VT, AT} where {VT<:AbstractVector{T}, AT<:AbstractMatrix{T}}}, Tuple{T}} where T","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(ls)\n\nWrite NaNs into Matrix(ls) and Vector(ls).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Union{Tuple{SimpleSolvers.OptimizerResult{XT, YT, VT, OST} where {VT<:(AbstractArray{XT}), OST<:SimpleSolvers.OptimizerStatus{XT, YT}}}, Tuple{YT}, Tuple{XT}} where {XT, YT}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(result)\n\nClear all the information contained in result::OptimizerResult. This also calls clear!(::OptimizerStatus).\n\ninfo: Info\n\n\nCalling initialize! on an OptimizerResult calls clear! internally.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_new_iterate!-Union{Tuple{TVT}, Tuple{VT}, Tuple{T}, Tuple{VT, VT, T, TVT}} where {T, VT, TVT}","page":"Home","title":"SimpleSolvers.compute_new_iterate!","text":"compute_new_iterate!(xₖ₊₁, xₖ, αₖ, pₖ)\n\nCompute xₖ₊₁ based on a direction pₖ and a step length αₖ.\n\nExtended help\n\nIn the case of vector spaces this function simply does:\n\nxₖ = xₖ + αₖ * pₖ\n\nFor manifolds we instead perform a retraction [5].\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.convergence_measures-Tuple{SimpleSolvers.OptimizerStatus, Options}","page":"Home","title":"SimpleSolvers.convergence_measures","text":"convergence_measures(status, config)\n\nChecks if the optimizer converged.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.default_precision","page":"Home","title":"SimpleSolvers.default_precision","text":"default_precision(T)\n\nCompute the default precision used for BierlaireQuadratic. Compare this to the default_tolerance used in Options.\n\nExamples\n\ndefault_precision(Float64)\n\n# output\n\n1.7763568394002505e-15\n\ndefault_precision(Float32)\n\n# output\n\n9.536743f-7\n\ndefault_precision(Float16)\n\n# output\n\nERROR: No default precision defined for Float16.\n[...]\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.default_tolerance-Union{Tuple{Type{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"SimpleSolvers.default_tolerance","text":"default_tolerance(T)\n\nDetermine the default tolerance for a specific data type. This is used in the constructor of Options.\n\nExamples\n\ndefault_tolerance(Float64)\n\n# output\n\n4.440892098500626e-16\n\ndefault_tolerance(Float32)\n\n# output\n\n2.3841858f-7\n\ndefault_tolerance(Float16)\n\n# output\n\nFloat16(0.001953)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.determine_initial_α-Union{Tuple{T}, Tuple{LinesearchProblem, T}, Tuple{LinesearchProblem, T, T}, Tuple{LinesearchProblem, T, T, T}} where T","page":"Home","title":"SimpleSolvers.determine_initial_α","text":"determine_initial_α(y₀, obj, α₀)\n\nCheck whether α₀ satisfies the BracketMinimumCriterion for obj. If the criterion is not satisfied we call bracket_minimum_with_fixed_point. This is used as a starting point for using the functor of Quadratic and makes sure that α describes a point past the minimum.\n\nwarning: Warning\nThis was used for the old Quadratic line search and seems to be not used anymore for Quadratic and other line searches.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.direction-Tuple{SimpleSolvers.BFGSCache}","page":"Home","title":"SimpleSolvers.direction","text":"direction(cache)\n\nReturn the direction of the gradient step (i.e. δ) of an instance of BFGSCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.direction-Tuple{SimpleSolvers.DFPCache}","page":"Home","title":"SimpleSolvers.direction","text":"direction(cache)\n\nReturn the direction of the gradient step (i.e. δ) of an instance of DFPCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.direction-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.direction","text":"direction(::NewtonOptimizerCache)\n\nReturn the direction of the gradient step (i.e. Δx) of an instance of NewtonOptimizerCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.factorize!-Tuple{LinearSolver}","page":"Home","title":"SimpleSolvers.factorize!","text":"factorize!(lsolver)\n\nFactorize the matrix stored in the LinearSolverCache in lsolver.\n\nSee factorize!(::LinearSolver{T, LUT}) where {T, LUT <: LU} for a concrete example.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.factorize!-Union{Tuple{LinearSolver{T, LUT}}, Tuple{LUT}, Tuple{T}} where {T, LUT<:LU}","page":"Home","title":"SimpleSolvers.factorize!","text":"factorize!(lsolver::LinearSolver, A)\n\nFactorize the matrix A and store the result in cache(lsolver).A. Note that calling cache on lsolver returns the instance of LUSolverCache stored in lsolver.\n\nExamples\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\ny = [1., 0., 0.]\nx = similar(y)\n\nlsolver = LinearSolver(LU(; static=false), x)\nfactorize!(lsolver, A)\ncache(lsolver).A\n\n# output\n\n3×3 Matrix{Float64}:\n 13.0        17.0       19.0\n  0.0769231   0.692308   1.53846\n  0.384615    0.666667   2.66667\n\nHere cache(lsolver).A stores the factorized matrix. If we call factorize! with two input arguments as above, the method first copies the matrix A into the LUSolverCache. We can equivalently also do:\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\ny = [1., 0., 0.]\n\nlsolver = LinearSolver(LU(), A)\nfactorize!(lsolver)\ncache(lsolver).A\n\n# output\n\n3×3 StaticArraysCore.MMatrix{3, 3, Float64, 9} with indices SOneTo(3)×SOneTo(3):\n 13.0        17.0       19.0\n  0.0769231   0.692308   1.53846\n  0.384615    0.666667   2.66667\n\nAlso note the difference between the output types of the two refactorized matrices. This is because we set the keyword static to false when calling LU. Also see _static.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.find_maximum_value-Union{Tuple{T}, Tuple{AbstractVector{T}, Integer}} where T<:Number","page":"Home","title":"SimpleSolvers.find_maximum_value","text":"find_maximum_value(v, k)\n\nFind the maximum value of vector v starting from the index k. This is used for pivoting in factorize!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{SimpleSolvers.BFGSCache}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(cache)\n\nReturn the stored gradient (array) of an instance of BFGSCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{SimpleSolvers.DFPCache}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(cache)\n\nReturn the stored gradient (array) of an instance of DFPCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(::NewtonOptimizerCache)\n\nReturn the stored gradient (array) of an instance of NewtonOptimizerCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.increase_iteration_number!-Tuple{NonlinearSolverState}","page":"Home","title":"SimpleSolvers.increase_iteration_number!","text":"increase_iteration_number!(s)\n\nTo be used together with NonlinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{LinearProblem, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(ls, x)\n\nInitialize the LinearProblem ls. See clear!(::LinearProblem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Union{Tuple{T}, Tuple{SimpleSolvers.NonlinearSolverCache{T, AT, JT} where {AT<:AbstractVector{T}, JT<:AbstractMatrix{T}}, AbstractVector{T}}} where T","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(cache, x)\n\nInitialize the NonlinearSolverCache based on x.\n\nImplementation\n\nThis calls alloc_x to do all the initialization.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.isaOptimizerState-Tuple{Any}","page":"Home","title":"SimpleSolvers.isaOptimizerState","text":"isaOptimizerState(alg)\n\nVerify if an object implements the OptimizerState interface.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.jacobian-Tuple{NonlinearProblem}","page":"Home","title":"SimpleSolvers.jacobian","text":"jacobian(nlp::NonlinearProblem)\n\nReturn the Jacobian function stored in nlp.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.jacobianmatrix-Tuple{NonlinearSolver}","page":"Home","title":"SimpleSolvers.jacobianmatrix","text":"jacobianmatrix(solver::NewtonSolver)\n\nReturn the evaluated Jacobian (a matrix) stored in the NonlinearProblem of solver.\n\nAlso see jacobian(::NonlinearProblem) and Jacobian(::NonlinearProblem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linearsolver-Tuple{NonlinearSolver}","page":"Home","title":"SimpleSolvers.linearsolver","text":"linearsolver(solver)\n\nReturn the linear part (i.e. a LinearSolver) of an NewtonSolver.\n\nExamples\n\nx = rand(3)\ny = rand(3)\nF(x) = tanh.(x)\nF!(y, x, params) = y .= F(x)\ns = NewtonSolver(x, y; F = F!)\nlinearsolver(s)\n\n# output\n\nLinearSolver{Float64, LU{Missing}, SimpleSolvers.LUSolverCache{Float64, StaticArraysCore.MMatrix{3, 3, Float64, 9}}}(LU{Missing}(missing, true), SimpleSolvers.LUSolverCache{Float64, StaticArraysCore.MMatrix{3, 3, Float64, 9}}([0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0], [0, 0, 0], [0, 0, 0], 0))\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_problem-Tuple{NonlinearSolver, AbstractVector, Any}","page":"Home","title":"SimpleSolvers.linesearch_problem","text":"linesearch_problem(nl::NonlinearSolver, state, params)\n\nBuild a line search problem based on a NonlinearSolver (almost always a NewtonSolver in practice).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_problem-Union{Tuple{T}, Tuple{NonlinearProblem{T}, Jacobian{T}, SimpleSolvers.NonlinearSolverCache{T, AT, JT} where {AT<:AbstractVector{T}, JT<:AbstractMatrix{T}}, AbstractVector{T}, Any}} where T","page":"Home","title":"SimpleSolvers.linesearch_problem","text":"linesearch_problem(nlp, cache, params)\n\nMake a line search problem for a Newton solver (the cache here is an instance of NonlinearSolverCache).\n\nImplementation\n\ninfo: Producing a single-valued output\nDifferent from the linesearch_problem for NewtonOptimizerCaches, we apply L2norm to the output of problem!. This is because the solver operates on an optimizer problem with multiple outputs from which we have to find roots, whereas an optimizer operates on an optimizer problem with a single output of which we should find a minimum.\n\nAlso see linesearch_problem(::OptimizerProblem{T}, ::Gradient, ::OptimizerCache{T}, ::OptimizerState) where {T}.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_problem-Union{Tuple{T}, Tuple{OptimizerProblem{T}, Gradient, SimpleSolvers.OptimizerCache{T}, OptimizerState}} where T","page":"Home","title":"SimpleSolvers.linesearch_problem","text":"linesearch_problem(problem, cache)\n\nCreate LinesearchProblem for linesearch algorithm. The variable on which this problem depends is alpha.\n\nExample\n\nx = [1, 0., 0.]\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nobj = OptimizerProblem(f, x)\ngrad = GradientAutodiff{Float64}(obj.F, length(x))\ncache = NewtonOptimizerCache(x)\nstate = NewtonOptimizerState(x)\nstate.x̄ .= x\nhess = HessianAutodiff(obj, x)\nupdate!(cache, state, grad, hess, x)\nx₂ = [.9, 0., 0.]\nupdate!(cache, state, grad, hess, x₂)\nls_obj = linesearch_problem(obj, grad, cache, state)\nα = .1\n(ls_obj.F(α), ls_obj.D(α))\nx = [1, 0., 0.]\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nobj = OptimizerProblem(f, x)\ngrad = GradientAutodiff{Float64}(obj.F, length(x))\ncache = NewtonOptimizerCache(x)\nstate = NewtonOptimizerState(x)\nstate.x̄ .= x\nhess = HessianAutodiff(obj, x)\nupdate!(cache, state, grad, hess, x)\nx₂ = [.9, 0., 0.]\nupdate!(cache, state, grad, hess, x₂)\nls_obj = linesearch_problem(obj, grad, cache, state)\nα = .1\n(ls_obj.F(α), ls_obj.D(α))\n\n# output\n\n(0.5683038684544637, -0.9375328383328476)\n\nIn the example above we have to apply update! twice on the instance of NewtonOptimizerCache because it needs to store the current and the previous iterate.\n\nImplementation\n\nCalling the function and derivative stored in the LinesearchProblem created with linesearch_problem does not allocate a new array, but uses the one stored in the instance of NewtonOptimizerCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.meets_stopping_criteria-Tuple{NonlinearSolverState, Options}","page":"Home","title":"SimpleSolvers.meets_stopping_criteria","text":"meets_stopping_criteria(status, config)\n\nDetermines whether the iteration stops based on the current NonlinearSolverStatus.\n\nwarning: Warning\nThe function meets_stopping_criteria may return true even if the solver has not converged. To check convergence, call assess_convergence (with the same input arguments).\n\nThe function meets_stopping_criteria returns true if one of the following is satisfied:\n\nthe status::NonlinearSolverStatus is converged (checked with assess_convergence) and iteration_number(status) ≥ config.min_iterations,\nstatus.f_increased and config.allow_f_increases = false (i.e. f increased even though we do not allow it),\niteration_number(status) ≥ config.max_iterations,\nif any component in solution(status) is NaN,\nif any component in status.f is NaN,\nstatus.rxₐ > config.x_abstol_break (by default Inf. In theory this returns true if the residual gets too big,\nstatus.rfₐ > config.f_abstol_break (by default Inf. In theory this returns true if the residual gets too big,\n\nSo convergence is only one possible criterion for which meets_stopping_criteria. We may also satisfy a stopping criterion without having convergence!\n\nExamples\n\nIn the following example we show that meets_stopping_criteria evaluates to true when used on a freshly allocated NonlinearSolverStatus:\n\nconfig = Options(verbosity=0)\nx = [NaN, 2., 3.]\ncache = NonlinearSolverCache(x, copy(x))\nstate = NonlinearSolverState(x)\nstatus = NonlinearSolverStatus(state, config)\nmeets_stopping_criteria(state, config)\n\n# output\n\nfalse\n\nThis obviously has not converged. To check convergence we can use assess_convergence. ```\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.meets_stopping_criteria-Tuple{SimpleSolvers.OptimizerStatus, Options, Integer}","page":"Home","title":"SimpleSolvers.meets_stopping_criteria","text":"meets_stopping_criteria(status, config, iterations)\n\nCheck if the optimizer has converged.\n\nImplementation\n\nmeets_stopping_criteria checks if one of the following is true:\n\nconverged (the output of assess_convergence) is true and iterations geq config.min_iterations,\nif config.allow_f_increases is false: status.f_increased is true,\niterations geq config.max_iterations,\nstatus.rxₐ  config.x_abstol_break\nstatus.rxᵣ  config.x_reltol_break\nstatus.rfₐ  config.f_abstol_break\nstatus.rfᵣ  config.f_reltol_break\nstatus.rg   config.g_restol_break\nstatus.x_isnan\nstatus.f_isnan\nstatus.g_isnan\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.method-Tuple{LinearSolver}","page":"Home","title":"SimpleSolvers.method","text":"method(ls)\n\nReturn the method (of type LinearSolverMethod) of the LinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.minimum_decrease_threshold-Union{Tuple{Type{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"SimpleSolvers.minimum_decrease_threshold","text":"minimum_decrease_threshold(T)\n\nThe minimum value by which a function f should decrease during an iteration.\n\nThe default value of 10^-4 is often used in the literature [bierlaire2015optimization], nocedal2006numerical(@cite).\n\nExamples\n\nminimum_decrease_threshold(Float64)\n\n# output\n\n0.0001\n\nminimum_decrease_threshold(Float32)\n\n# output\n\n0.0001f0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.nonlinearproblem-Tuple{NonlinearSolver}","page":"Home","title":"SimpleSolvers.nonlinearproblem","text":"nonlinearproblem(solver)\n\nReturn the NonlinearProblem contained in the NonlinearSolver. Compare this to linearsolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.print_status-Tuple{NonlinearSolverStatus, Options}","page":"Home","title":"SimpleSolvers.print_status","text":"print_status(status, config)\n\nPrint the solver status if:\n\nThe following three are satisfied: (i) config.verbosity geq1 (ii) assess_convergence!(status, config) is false (iii) iteration_number(status) > config.max_iterations\nconfig.verbosity > 1.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.residuals-Tuple{NonlinearSolverState}","page":"Home","title":"SimpleSolvers.residuals","text":"residuals(cache, state)\n\nCompute the residuals for cache::NonlinearSolverCache. The computed residuals are the following:\n\nrxₛ : successive residual (the norm of delta),\nrfₐ: absolute residual in f,\nrfₛ : successive residual (the norm of Deltay).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.rhs-Tuple{SimpleSolvers.BFGSCache}","page":"Home","title":"SimpleSolvers.rhs","text":"rhs(cache)\n\nReturn the right hand side of an instance of BFGSCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.rhs-Tuple{SimpleSolvers.DFPCache}","page":"Home","title":"SimpleSolvers.rhs","text":"rhs(cache)\n\nReturn the right hand side of an instance of DFPCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.rhs-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.rhs","text":"rhs(cache)\n\nReturn the right hand side of an instance of NewtonOptimizerCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.rhs-Tuple{SimpleSolvers.NonlinearSolverCache}","page":"Home","title":"SimpleSolvers.rhs","text":"rhs(cache)\n\nReturn the right-hand side of the equation, stored in cache::NonlinearSolverCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.shift_χ_to_avoid_stalling-Union{Tuple{T}, NTuple{5, T}} where T","page":"Home","title":"SimpleSolvers.shift_χ_to_avoid_stalling","text":"shift_χ_to_avoid_stalling(χ, a, b, c, ε)\n\nCheck whether b is closer to a or c and shift χ accordingly. This is taken from [2].\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, s)\n\nExtended help\n\ninfo: Info\nThe function update! calls increase_iteration_number!.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, LinearSolver, AbstractMatrix, AbstractVector}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, ls::LinearSolver, A, b)\n\nSolve the linear system described by:\n\n    Ax = b\n\nand store it in x. Here A and b are provided as an input arguments.\n\nimplementation\n\nNote that, compared to solve(::LinearSolver, ::AbstractVector) this method involves an additional factorization of A.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, LinearSolver, AbstractVector}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, ls::LinearSolver, b)\n\nSolve the linear system described by:\n\n    Ax = b\n\nand store it in x. Here b is provided as an input argument and the factorized A is stored in the LinearSolver ls (respectively its LinearSolverCache).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, LinearSolver, LinearProblem}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, ls::LinearSolver, lsys::LinearProblem)\n\nSolve the LinearProblem lsys with the LinearSolver ls and store the result in x. Also see solve!(::LinearSolver, ::LinearProblem).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, OptimizerState, Optimizer}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, state, opt)\n\nSolve the optimization problem described by opt::Optimizer and store the result in x.\n\nf(x) = sum(x .^ 2 + x .^ 3 / 3)\nx = [1f0, 2f0]\nopt = Optimizer(x, f; algorithm = Newton())\nstate = NewtonOptimizerState(x)\n\nsolve!(x, state, opt)\n\n# output\n\nSimpleSolvers.OptimizerResult{Float32, Float32, Vector{Float32}, SimpleSolvers.OptimizerStatus{Float32, Float32}}(\n * Convergence measures\n\n    |x - x'|               = 7.82e-03\n    |x - x'|/|x'|          = 2.56e+02\n    |f(x) - f(x')|         = 9.31e-10\n    |f(x) - f(x')|/|f(x')| = 1.00e+00\n    |g(x) - g(x')|         = 1.57e-02\n    |g(x)|                 = 6.10e-05\n\n, Float32[4.6478817f-8, 3.0517578f-5], 9.313341f-10)\n\nWe can also check how many iterations it took:\n\niteration_number(state)\n\n# output\n\n4\n\nToo see the value of x after one iteration confer the docstring of solver_step!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{LinearSolver, Vararg{Any}}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(ls::LinearSolver, args...)\n\nSolve the LinearProblem with the LinearSolver ls.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve-Tuple{LinearProblem, SimpleSolvers.LinearSolverMethod}","page":"Home","title":"SimpleSolvers.solve","text":"solve(ls, method)\n\nSolve the LinearProblem ls with the LinearSolverMethod method.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve-Union{Tuple{ALG}, Tuple{T}, Tuple{LinesearchProblem{T}, Linesearch{T, ALG, OPT} where OPT<:Options{T}}} where {T, ALG<:LinesearchMethod{T}}","page":"Home","title":"SimpleSolvers.solve","text":"solve(ls_prob, ls)\nsolve(ls_prob, ls_method)\n\nMinimize the LinesearchProblem with the LinesearchMethod ls_method.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solver_step!-Union{Tuple{T}, Tuple{AbstractVector{T}, NonlinearSolver{T, MT, AT, NLST, LST, JT, LSoT, LiSeT, CT} where {MT<:SimpleSolvers.NonlinearSolverMethod, AT, NLST<:(NonlinearProblem{T}), LST<:SimpleSolvers.AbstractLinearProblem, JT<:Jacobian{T}, LSoT<:SimpleSolvers.AbstractLinearSolver, LiSeT<:(Linesearch{T, ALG, OPT} where {ALG<:LinesearchMethod{T}, OPT<:Options{T}}), CT<:(SimpleSolvers.NonlinearSolverCache{T, AT, JT} where {AT<:AbstractVector{T}, JT<:AbstractMatrix{T}})}, NonlinearSolverState{T, XT, YT} where {XT<:AbstractVector{T}, YT<:AbstractVector{T}}, Any}} where T","page":"Home","title":"SimpleSolvers.solver_step!","text":"solver_step!(x, s, state, params)\n\nSolve the problem stored in an instance s of NonlinearSolver.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solver_step!-Union{Tuple{VT}, Tuple{VT, OptimizerState, Optimizer}} where VT<:(AbstractVector)","page":"Home","title":"SimpleSolvers.solver_step!","text":"solver_step!(x, state)\n\nCompute a full iterate for an instance of NewtonOptimizerState state.\n\nThis also performs a line search.\n\nExamples\n\nf(x) = sum(x .^ 2 + x .^ 3 / 3)\nx = [1f0, 2f0]\nopt = Optimizer(x, f; algorithm = Newton())\nstate = NewtonOptimizerState(x)\n\nsolver_step!(x, state, opt)\n\n# output\n\n2-element Vector{Float32}:\n 0.25\n 0.6666666\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.triple_point_finder-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T","page":"Home","title":"SimpleSolvers.triple_point_finder","text":"triple_point_finder(f, x)\n\nFind three points a > b > c s.t. f(a) > f(b) and f(c) > f(b). This is used for performing a quadratic line search (see Quadratic).\n\nImplementation\n\nFor δ we take DEFAULT_BRACKETING_s as default. For nmax we take [DEFAULTBRACKETINGnmax`](@ref) as default.\n\nExtended help\n\nThe algorithm is taken from [2, Chapter 11.2.1].\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.value!-Union{Tuple{T}, Tuple{AbstractArray{T}, NonlinearProblem{T}, AbstractArray{T}, Any}} where T","page":"Home","title":"SimpleSolvers.value!","text":"value!(y, x, params)\n\nEvaluate the NonlinearProblem at x.\n\n\n\n\n\n","category":"method"}]
}
