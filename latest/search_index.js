var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"C. T. Kelley. Iterative methods for linear and nonlinear equations (SIAM, 1995).\n\n\n\nM. Bierlaire. Optimization: principles and algorithms (EPFL press, 2015).\n\n\n\nJ. Nocedal and S. J. Wright. Numerical optimization (Springer, New York, NY, 2006). Second Edition.\n\n\n\nM. J. Kochenderfer and T. A. Wheeler. Algorithms for optimization (Mit Press, 2019).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\n","category":"page"},{"location":"linesearch/bierlaire_quadratic/#Bierlaire-Quadratic-Line-Search","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic Line Search","text":"","category":"section"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"In [2] quadratic line search is defined as an interpolation between three points. For this consider","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"using SimpleSolvers\nusing SimpleSolvers: update!, compute_jacobian!, factorize!, linearsolver, jacobian, cache, linesearch_objective, direction # hide\nusing LinearAlgebra: rmul!, ldiv! # hide\nusing Random # hide\nRandom.seed!(123) # hide\n\nf(x::T) where {T<:Number} = exp(x) * (x ^ 3 - 5x + 2x) + 2one(T)\nf(x::AbstractArray{T}) where {T<:Number} = exp.(x) .* (x .^ 3 - 5x + 2x) .+ 2one(T)\nf!(y::AbstractVector{T}, x::AbstractVector{T}) where {T} = y .= f.(x)\nj!(j::AbstractMatrix{T}, x::AbstractVector{T}) where {T} = SimpleSolvers.ForwardDiff.jacobian!(j, f!, similar(x), x)\nx = rand(1)\nsolver = NewtonSolver(x, f.(x); F = f)\nupdate!(solver, x)\ncompute_jacobian!(solver, x, j!; mode = :function)\n\n# compute rhs\nf!(cache(solver).rhs, x)\nrmul!(cache(solver).rhs, -1)\n\n# multiply rhs with jacobian\nfactorize!(linearsolver(solver), jacobian(cache(solver)))\nldiv!(direction(cache(solver)), linearsolver(solver), cache(solver).rhs)\n\nobj = MultivariateObjective(f, x)\nls_obj = linesearch_objective(obj, JacobianFunction(j!, x), cache(solver))\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"In the figure above we already plotted three points a, b and c on whose basis a second-order polynomial will be built that should approximate f^mathrmls.[1] The polynomial is built with the ansatz:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"[1]: These points further need to satisfy f^mathrmls(a)  f^mathrmls(b)  f^mathrmls(c).","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"p(alpha) = beta_1(alpha - a)(x - b) + beta_2(alpha - a) + beta_3(alpha - b)","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"and by identifying ","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"beginaligned\np(a)  = f^mathrmls(a) \np(b)  = f^mathrmls(b) \np(c)  = f^mathrmls(c) \nendaligned","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"we get","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"beginaligned\nbeta_1  = frac(b - c)f^mathrmls(a) + (c - a)f^mathrmls(b) + (a - b)f^mathrmls(c)(a - b)(c - a)(c - b)  \nbeta_2  = fracf^mathrmls(b)b - a \nbeta_3  = fracf^mathrmls(a)a - b\nendaligned","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We can plot this polynomial:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We can now easily determine the minimum of the polynomial p. It is:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"chi = frac12 frac f^mathrmls(a) * (b^2 - c^2) + f^mathrmls(b) * (c^2 - a^2) + f^mathrmls(c) * (a^2 - b^2) f^mathrmls(a) * (b - c) + f^mathrmls(b) * (c - a) + f^mathrmls(c) * (a - b)","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We now use this chi to either replace a, b or c and distinguish between the following four scenarios:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"chi  b and f^mathrmls(chi)  f^mathrmls(b) implies we replace c gets chi,\nchi  b and f^mathrmls(chi) leq f^mathrmls(b) implies we replace a b gets b chi,\nchi leq b and f^mathrmls(chi)  f^mathrmls(b) implies we replace a gets chi,\nchi leq b and f^mathrmls(chi) leq f^mathrmls(b) implies we replace b c gets chi b.","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"In our example we have the second case: chi is to the right of b and f^mathrmls(chi) is less than f(b). We therefore replace a with b and b with chi. The new approximation is the following one:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"We again observe the second case. By again replacing a b gets b chi we get:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"No we observe the fourth case: chi is to the left of b and f^mathrmls(chi) is below f(b). Hence we replace b c gets chi b A successive iteration yields:","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"info: Info\nAfter having computed chi we further either shift it to the left or right depending on whether (c - b) or (b - a) is bigger respectively. The shift is made by either adding or subtracting the constant varepsilon.","category":"page"},{"location":"linesearch/bierlaire_quadratic/","page":"Bierlaire Quadratic","title":"Bierlaire Quadratic","text":"Also see SimpleSolvers.DEFAULT_BIERLAIRE_ε.","category":"page"},{"location":"linesearch/curvature_condition/#The-Curvature-Condition","page":"The Curvature Condition","title":"The Curvature Condition","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"The curvature condition is used together with the sufficient decrease condition and ensures that step sizes are not chosen too small (which might happen if we only use the sufficient decrease condition). The sufficient decrease condition and the curvature condition together are called the Wolfe conditions.","category":"page"},{"location":"linesearch/curvature_condition/#Standard-Curvature-Condition","page":"The Curvature Condition","title":"Standard Curvature Condition","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"For the standard curvature condition (see SimpleSolvers.CurvatureCondition) we have:","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"    fracpartialpartialalphaBigg_alpha=alpha_kf(R_x_k(alphap_k)) = g(mathrmgrad_R_x_k(alpha_kp_k)f p_k) geq c_2g(mathrmgrad_x_kf p_k) = c_2fracpartialpartialalphaBigg_alpha=0f(R_x_k(alphap_k))","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"for c_2in(c_1 1) In words this means that the derivative with respect to alpha_k should be bigger at the new iterate x_k+1 than at the old iterate x_k. ","category":"page"},{"location":"linesearch/curvature_condition/#Strong-Curvature-Condition","page":"The Curvature Condition","title":"Strong Curvature Condition","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"For the strong curvature condition[1] we replace the curvature condition by:","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"[1]: We consequently also speak of the strong Wolfe conditions when taking the strong curvature condition and the sufficient decrease condition together.","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"    g(mathrmgrad_R_x_k(alpha_kp_k)f p_k)  c_2g(mathrmgrad_x_kf p_k)","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"Note the sign change here. This is because the term g(mathrmgrad_x_kf p_k) is negative if p_k is a search direction. Both the standard curvature condition and the strong curvature condition are implemented under SimpleSolvers.CurvatureCondition.","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"info: Info\nIn order to use the corresponding condition you have to either pass mode = :Standard or mode = :Strong to the constructor of CurvatureCondition.","category":"page"},{"location":"linesearch/curvature_condition/#Example","page":"The Curvature Condition","title":"Example","text":"","category":"section"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"We use the same example that we had when we explained the sufficient decrease condition:","category":"page"},{"location":"linesearch/curvature_condition/","page":"The Curvature Condition","title":"The Curvature Condition","text":"using SimpleSolvers # hide\nusing SimpleSolvers: CurvatureCondition, NewtonOptimizerCache, update!, gradient!, linesearch_objective, ldiv! # hide\n\nx = [3., 1.3]\nf = x -> 10 * sum(x .^ 3 / 6 - x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\nhes = Hessian(obj, x; mode = :autodiff)\nupdate!(hes, x)\n\nc₂ = .9\ng = gradient(obj, x)\nrhs = -g\n# the search direction is determined by multiplying the right hand side with the inverse of the Hessian from the left.\np = similar(rhs)\nldiv!(p, hes, rhs)\ncc = CurvatureCondition(c₂, x, g, p, obj, obj.G)\n\n# check different values\nα₁, α₂, α₃, α₄, α₅ = .09, .4, 0.7, 1., 1.3\n(cc(α₁), cc(α₂), cc(α₃), cc(α₄), cc(α₅))","category":"page"},{"location":"jacobians/#Jacobians","page":"Jacobians","title":"Jacobians","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"The supertype Jacobian comprises different ways of taking Jacobians:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"JacobianFunction,\nJacobianAutodiff,\nJacobianFiniteDifferences.","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"We first start by showing JacobianAutodiff:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"using SimpleSolvers, Random; using SimpleSolvers: JacobianAutodiff, Jacobian, JacobianFunction, JacobianFiniteDifferences; Random.seed!(123) # hide\n# the input and output dimensions of this function are the same\nF(y::AbstractArray, x::AbstractArray) = y .= tanh.(x)\ndim = 3\nx = rand(dim)\njac = JacobianAutodiff{eltype(x)}(F, dim)","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"Instead of calling JacobianAutodiff(f, x) we can equivalently do:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"jac = Jacobian{eltype(x)}(F, dim; mode = :autodiff)","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"When calling an instance of Jacobian we can use the function [compute_jacobian!]:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"j = zeros(dim, dim)\ncompute_jacobian!(j, x, jac)","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"This is equivalent to calling:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"jac(j, x)","category":"page"},{"location":"linesearch/static/#Static-Line-Search","page":"Static","title":"Static Line Search","text":"","category":"section"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"Static line search is the simplest form of line search in which the guess for alpha is always just a fixed value. In the following we demonstrate how to use this line search.","category":"page"},{"location":"linesearch/static/#static_example","page":"Static","title":"Example","text":"","category":"section"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"We show how to use linesearches in SimpleSolvers to solve a simple toy problem:","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"using SimpleSolvers # hide\n\nx = [1., 0., 0.]\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\n\nα = .1\nsl = Static(α)\nnothing # hide","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"SimpleSolvers contains a function SimpleSolvers.linesearch_objective that allocates a SimpleSolvers.TemporaryUnivariateObjective that only depends on alpha:","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"using SimpleSolvers: linesearch_objective, NewtonOptimizerCache, LinesearchState, update! # hide\ncache = NewtonOptimizerCache(x)\n\nupdate!(cache, x)\nx₂ = [.9, 0., 0.]\nupdate!(cache, x₂)\nvalue!(obj, x₂)\ngradient!(obj, x₂)\nls_obj = linesearch_objective(obj, cache)\nnothing # hide","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"We now use this to compute a static line search[1]:","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"[1]: We also note the use of the SimpleSolvers.LinesearchState constructor here, which has to be used together with a SimpleSolvers.LinesearchMethod.","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"ls = LinesearchState(sl)\nls(ls_obj, α)","category":"page"},{"location":"linesearch/static/","page":"Static","title":"Static","text":"info: Info\nWe note that for the static line search we always just return alpha.","category":"page"},{"location":"optimizers/optimizers/#Optimizers","page":"Optimizers","title":"Optimizers","text":"","category":"section"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"An Optimizer stores an OptimizationAlgorithm, a MultivariateObjective, the SimpleSolvers.OptimizerResult and a SimpleSolvers.NonlinearMethod. Its purposes are:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers\nusing LinearAlgebra: norm\nimport Random # hide\nRandom.seed!(123) # hide\n\nx = rand(3)\nobj = MultivariateObjective(x -> sum((x - [0., 0., 1.]) .^ 2), x)\nbt = Backtracking()\nconfig = Options()\nalg = Newton()\nopt = Optimizer(x, obj; algorithm = alg, linesearch = bt, config = config)","category":"page"},{"location":"optimizers/optimizers/#Optimization-Algorithm","page":"Optimizers","title":"Optimization Algorithm","text":"","category":"section"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Internally the constructor for Optimizer calls SimpleSolvers.OptimizationAlgorithm and SimpleSolvers.OptimizerResult. SimpleSolvers.OptimizationAlgorithm in turn calls SimpleSolvers.LinesearchState and Hessian:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers: OptimizationAlgorithm\n\nstate = OptimizationAlgorithm(alg, obj, x; linesearch = bt)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"The call above is equivalent to ","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers: NewtonOptimizerCache, NewtonOptimizerState, LinesearchState, linesearch_objective, initialize!, update!\n\ncache = NewtonOptimizerCache(x)\nhess = Hessian(obj, x; mode = :autodiff)\ninitialize!(hess, x)\nls = LinesearchState(bt)\nvalue!(obj, x)\ngradient!(obj, x)\nlso = linesearch_objective(obj, cache)\n\nNewtonOptimizerState(obj, hess, ls, lso, cache)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Note that we use a separate objective here that only depends on alpha (i.e. the step length for a single iteration) via SimpleSolvers.linesearch_objective.","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Also note that:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"NewtonOptimizerState <: OptimizationAlgorithm","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"If we want to use the Optimizer we can call:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"x₀ = copy(x)\nupdate!(opt, rand(3))\nupdate!(opt, rand(3))\n\nsolve!(x, opt)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Internally SimpleSolvers.solve! repeatedly calls SimpleSolvers.solver_step! (and SimpleSolvers.update!) until SimpleSolvers.meets_stopping_criteria is satisfied.","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers: solver_step!\n\nupdate!(opt, rand(3))\nsolver_step!(x, state)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"The function SimpleSolvers.solver_step! in turn does the following:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"update!(state, x)\nldiv!(direction(state), hessian(state), rhs(state))\nls = linesearch(state)\nα = ls(state.ls_objective)\nx .= x .+ α .* direction(state)","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"Calling an instance of SimpleSolvers.LinesearchState (in this case SimpleSolvers.BacktrackingState) on an SimpleSolvers.AbstractUnivariateObjective in turn does:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"α *= ls.p","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"as long as the SimpleSolvers.SufficientDecreaseCondition isn't satisfied. This condition checks the following:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"fₖ₊₁ ≤ sdc.fₖ + sdc.c₁ * αₖ * sdc.pₖ' * sdc.gradₖ","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"sdc is first allocated as:","category":"page"},{"location":"optimizers/optimizers/","page":"Optimizers","title":"Optimizers","text":"using SimpleSolvers: SufficientDecreaseCondition # hide\nα = ls.α₀\nx₀ = zero(α)\ny₀ = value!(lso, x₀)\nd₀ = derivative!(lso, x₀)\n\nsdc = SufficientDecreaseCondition(ls.ϵ, x₀, y₀, d₀, d₀, obj)","category":"page"},{"location":"update/#Updates","page":"Updates","title":"Updates","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"One of the most central objects in SimpleSolvers are SimpleSolvers.update! routines. They can be used together with many different types and structs:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"SimpleSolvers.update!(::Hessian, ::AbstractVector): this routine exists for most Hessians, i.e. for HessianFunction, HessianAutodiff, HessianBFGS and HessianDFP,\nSimpleSolvers.update!(::SimpleSolvers.NewtonSolverCache, ::AbstractVector),\nSimpleSolvers.update!(::SimpleSolvers.NonlinearSolverStatus, ::AbstractVector, ::Base.Callable),\nSimpleSolvers.update!(::SimpleSolvers.NewtonOptimizerCache, ::AbstractVector, ::AbstractVector, ::Hessian),\nSimpleSolvers.update!(::SimpleSolvers.NewtonOptimizerState, ::AbstractVector).\nSimpleSolvers.update!(::SimpleSolvers.OptimizerResult, ::AbstractVector, ::AbstractVector, ::AbstractVector).","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"So SimpleSolvers.update! always takes an object that has to be updated and a single vector in the simplest case. For some methods more arguments need to be provided. ","category":"page"},{"location":"update/#Examples","page":"Updates","title":"Examples","text":"","category":"section"},{"location":"update/#Hessian","page":"Updates","title":"Hessian","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"If we look at the case of the Hessian, we store a matrix H that has to be updated in every iteration. We first initialize the matrix[1]:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"[1]: The constructor uses the function SimpleSolvers.initialize!.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers # hide\nusing SimpleSolvers: update! # hide\nusing LinearAlgebra: norm # hide\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nx = [1., 0., 0.]\nhes = Hessian(f, x; mode = :autodiff)\nhes.H","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"And then update:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"update!(hes, x)\n\nhes.H","category":"page"},{"location":"update/#NewtonOptimizerCache","page":"Updates","title":"NewtonOptimizerCache","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"In order to update an instance of SimpleSolvers.NewtonOptimizerCache we have to supply a value of the Gradient and the Hessian in addition to x:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers: initialize!, NewtonOptimizerCache # hide\ngrad = Gradient(f, x; mode = :autodiff)\ncache = NewtonOptimizerCache(x)\nupdate!(cache, x, grad, hes)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"info: Info\nWe note that when calling update! on the NewtonOptimizerCache, the Hessian hes is not automatically updated! This has to be done manually.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"info: Info\nCalling update! on the NewtonOptimizerCache updates everything except x as this in general requires another line search!","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"In order that we do not have to update the Hessian and the SimpleSolvers.NewtonOptimizerCache separately we can use SimpleSolvers.NewtonOptimizerState:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers: NewtonOptimizerState # hide\nobj = MultivariateObjective(f, x)\nstate = NewtonOptimizerState(x, obj)\nupdate!(state, x)","category":"page"},{"location":"update/#OptimizerResult","page":"Updates","title":"OptimizerResult","text":"","category":"section"},{"location":"update/","page":"Updates","title":"Updates","text":"We also show how to update an instance of SimpleSolvers.OptimizerResult:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"using SimpleSolvers: OptimizerResult # hide\n\nresult = OptimizerResult(x, obj)\n\nupdate!(result, x, obj, grad)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"Note that the residuals are still NaNs here. In order to get proper values for these we have to perform two updating steps:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"x₂ = [.9, 0., 0.]\nupdate!(result, x₂, obj, grad)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"warn: Warn\nNewtonOptimizerCache, OptimizerResult and NewtonOptimizerState (through MultivariateObjective) all store things that are somewhat similar, for example x. This may make it somewhat difficult to keep track of all the things that happen during optimization.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"An Optimizer stores a MultivariateObjective, an SimpleSolvers.OptimizerResult and an OptimizationAlgorithm (and therefore the MultivariateObjective again). We also give an example:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"opt = Optimizer(x, obj)\n\nupdate!(opt, x)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"Equivalent to calling SimpleSolvers.update! on SimpleSolvers.OptimizerResult, the diagnostics cannot be computed with only one iterations; we have to compute a second one:","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"x₂ = [.9, 0., 0.]\nupdate!(opt, x₂)","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"We note that simply calling SimpleSolvers.update! on an instance of SimpleSolvers.Optimizer is not enough to perform a complete iteration since the computation of a new x requires a line search procedure in general.","category":"page"},{"location":"update/","page":"Updates","title":"Updates","text":"We also note that SimpleSolvers.update! always returns the first input argument.","category":"page"},{"location":"linesearch/bisections/#Bisections","page":"Bisections","title":"Bisections","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"Bisections work by moving an interval until we observe one in which the sign of the derivative of the function changes. ","category":"page"},{"location":"linesearch/bisections/#Example","page":"Bisections","title":"Example","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"We consider the same example as we had when demonstrating backtracking line search:","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"ls_obj = linesearch_objective(obj, cache)\nnothing # hide","category":"page"},{"location":"linesearch/bisections/#Bracketing","page":"Bisections","title":"Bracketing","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"For bracketing [4] we move an interval successively and simultaneously increase it in the hope that we observe a local minimum (see bracket_minimum).","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"α₀ = 0.0\n(a, c) = bracket_minimum(Function(ls_obj), α₀)","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"(Image: )","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"We then use this interval to start the bisection algorithm.","category":"page"},{"location":"linesearch/bisections/#Potential-Problem-with-Backtracking","page":"Bisections","title":"Potential Problem with Backtracking","text":"","category":"section"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"We here illustrate a potential issue with backtracking. For this consider the following function:","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"using SimpleSolvers: bracket_root\nf2(α::T) where {T <: Number} = α^2 - one(T)\nα₀ = -3.0\n(a, c) = bracket_root(f2, α₀)","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"And when we plot this we find:","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"(Image: )","category":"page"},{"location":"linesearch/bisections/","page":"Bisections","title":"Bisections","text":"And we see that the interval now contains two roots, r_1 and r_2.","category":"page"},{"location":"linesearch/backtracking/#Backtracking-Line-Search","page":"Backtracking","title":"Backtracking Line Search","text":"","category":"section"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"A backtracking line search method determines the amount to move in a given search direction by iteratively decreasing a step size alpha until an acceptable level is reached. In SimpleSolvers we can use the sufficient decrease condition and the curvature condition to quantify this acceptable level. The sufficient decrease condition is also referred to as the Armijo condition and the sufficient decrease condition and the curvature condition are referred to as the Wolfe conditions[1] [3]. ","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[1]: If we use the strong curvature condition instead of the standard curvature condition we conversely also say that we use the strong Wolfe conditions.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"info: Info\nWe note that for the static line search we always just return alpha.","category":"page"},{"location":"linesearch/backtracking/#Backtracking-Line-Search-for-a-Line-Search-Objective","page":"Backtracking","title":"Backtracking Line Search for a Line Search Objective","text":"","category":"section"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We note that when performing backtracking on a line search objective care needs to be taken. This is because we need to find equivalent quantities for mathrmgrad_x_kf and p. We first look at the derivative of the line search objective:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"fracddalphaf^mathrmls(alpha) = fracddalphaf(mathcalR_x_k(alphap)) = langle d_mathcalR_x_k(alphap)f alphap rangle","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"because the tangent map of a retraction is the identity at zero [5], i.e. T_0_xmathcalR = mathrmid_T_xmathcalM. In the equation above d_mathcalR_x_k(alphap)finT^*mathcalM indicates the exterior derivative of f evaluated at mathcalR_x_k(alphap) and langle cdot cdot rangle T^*mathcalMtimesTmathcalMtomathbbR is the natural pairing between tangent and cotangent space[2] [6].","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[2]: If we are not dealing with general Riemannian manifolds but only vector spaces then d_mathcalR_x_k(alphap)f simply becomes nabla_mathcalR_x_k(alphap)f and we further have langle A Brangle = A^T B.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We again look at the example introduced when talking about the sufficient decrease condition and cast it in the form of a line search objective:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"ls_obj = linesearch_objective(obj, cache)\nnothing # hide","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"This objective only depends on the parameter alpha. We plot it:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"(Image: )","category":"page"},{"location":"linesearch/backtracking/#sdc_example","page":"Backtracking","title":"Example","text":"","category":"section"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We show how to use linesearches in SimpleSolvers to solve a simple toy problem[3]:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[3]: Also compare this to the case of the static line search.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"using SimpleSolvers # hide\n\nsl = Backtracking()\nnothing # hide","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"SimpleSolvers contains a function SimpleSolvers.linesearch_objective that allocates a SimpleSolvers.TemporaryUnivariateObjective that only depends on alpha:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"We now use this to compute a backtracking line search[4]:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"[4]: We also note the use of the SimpleSolvers.LinesearchState constructor here, which has to be used together with a SimpleSolvers.LinesearchMethod.","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"ls = LinesearchState(sl)\nα = 50.\nαₜ = ls(ls_obj, α)","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"And we check whether the SimpleSolvers.SufficientDecreaseCondition is satisfied:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"sdc = SufficientDecreaseCondition(c₁, x, f(x), g, p, obj)\nsdc(αₜ)","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"Similarly for the SimpleSolvers.CurvatureCondition:","category":"page"},{"location":"linesearch/backtracking/","page":"Backtracking","title":"Backtracking","text":"using SimpleSolvers: CurvatureCondition # hide\nc₂ = .9\ncc = CurvatureCondition(c₂, x, g, p, obj, obj.G)\ncc(αₜ)","category":"page"},{"location":"linesearch/quadratic/#Quadratic-Line-Search","page":"Quadratic","title":"Quadratic Line Search","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"Quadratic line search is based on making a quadratic approximation to an objective and then pick the minimum of this quadratic approximation as the next iteration of alpha.","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"The quadratic polynomial is built the following way[1]:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"[1]: This is different from the Bierlaire quadratic polynomial described in [2].","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p(alpha) = f^mathrmls(0) + (f^mathrmls)(0)alpha + p_2alpha^2","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"and we also call p_0=f^mathrmls(0) and p_1=(f^mathrmls)(0). The coefficient p_2 is then determined the following way:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"take a value alpha (typically initialized as SimpleSolvers.DEFAULT_ARMIJO_α₀) and compute y = f^mathrmls(alpha),\nset p_2 gets frac(y^2 - p_0 - p_1alpha)alpha^2","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"After the polynomial is found we then take its minimum (analogously to the Bierlaire quadratic line search) and check if it satisfies the sufficient decrease condition. If it does not satisfy this condition we repeat the process, but with the current alph as the starting point for the line search (instead of the initial SimpleSolvers.DEFAULT_ARMIJO_α₀).","category":"page"},{"location":"linesearch/quadratic/#Example","page":"Quadratic","title":"Example","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"Here we treat the following problem:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"f(x::Union{T, Vector{T}}) where {T<:Number} = exp.(x) .* (x .^ 3 - 5x + 2x) .+ 2one(T)\nf!(y::AbstractVector{T}, x::AbstractVector{T}) where {T} = y .= f.(x)\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now want to use quadratic line search to find the root of this function starting at x = 0. We compute the Jacobian of f and initialize a line search objective:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers\nusing SimpleSolvers: update!, compute_jacobian!, factorize!, linearsolver, jacobian, cache, linesearch_objective, direction, determine_initial_α # hide\nusing LinearAlgebra: rmul!, ldiv! # hide\nusing Random # hide\nRandom.seed!(123) # hide\n\nj!(j::AbstractMatrix{T}, x::AbstractVector{T}) where {T} = SimpleSolvers.ForwardDiff.jacobian!(j, f, x)\nx = [0.]\n# allocate solver\nsolver = NewtonSolver(x, f(x); F = f)\n# initialize solver\nupdate!(solver, x)\ncompute_jacobian!(solver, x, j!; mode = :function)\n\n# compute rhs\nf!(cache(solver).rhs, x)\nrmul!(cache(solver).rhs, -1)\n\n# multiply rhs with jacobian\nfactorize!(linearsolver(solver), jacobian(cache(solver)))\nldiv!(direction(cache(solver)), linearsolver(solver), cache(solver).rhs)\nobj = MultivariateObjective(f, x)\nls_obj = linesearch_objective(obj, JacobianFunction(j!, x), cache(solver))\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"The first two coefficient of the polynomial p (i.e. p_1 and p_2) are easy to compute:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/#Initializing-\\alpha","page":"Quadratic","title":"Initializing alpha","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"In order to compute p_2 we first have to initialize alpha. We start by guessing an initial alpha as SimpleSolvers.DEFAULT_ARMIJO_α₀. If this initial alpha does not satisfy the SimpleSolvers.BracketMinimumCriterion, i.e. it holds that f^mathrmls(alpha_0)  f^mathrmls(0), we call SimpleSolvers.bracket_minimum_with_fixed_point (similarly to calling SimpleSolvers.bracket_minimum for standard bracketing). ","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"Looking at SimpleSolvers.DEFAULT_ARMIJO_α₀, we see that the SimpleSolvers.BracketMinimumCriterion is not satisfied:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We therefore see that calling SimpleSolvers.determine_initial_α returns a different alpha (the result of calling SimpleSolvers.bracket_minimum_with_fixed_point):","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"α₀ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We can now finally compute p_2 and determine the minimum of the polynomial:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"y = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"When using SimpleSolvers.QuadraticState we in addition call SimpleSolvers.adjust_α:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: adjust_α # hide\nα₁ = adjust_α(αₜ, α₀)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now check wether alpha_1 satisfies the sufficient decrease condition:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: DEFAULT_WOLFE_c₁, SufficientDecreaseCondition # hide\nsdc = SufficientDecreaseCondition(DEFAULT_WOLFE_c₁, 0., fˡˢ(0.), derivative(ls_obj, 0.), 1., ls_obj)\n@assert sdc(α₁) # hide\nsdc(α₁)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now move the original x in the Newton direction with step length alpha_1 by using SimpleSolvers.compute_new_iterate:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: compute_new_iterate # hide\nx .= compute_new_iterate(x, α₁, direction(cache(solver)))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"And we see that we already very close to the root.","category":"page"},{"location":"linesearch/quadratic/#Example-for-Optimization","page":"Quadratic","title":"Example for Optimization","text":"","category":"section"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We look again at the same example as before, but this time we want to find a minimum and not a root. We hence use SimpleSolvers.linesearch_objective not for a NewtonSolver, but for an Optimizer:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: NewtonOptimizerCache, initialize!, gradient\n\nx₀, x₁ = [0.], x\nobj = MultivariateObjective(sum∘f, x₀)\ngradient!(obj, x₀)\nvalue!(obj, x₀)\n_cache = NewtonOptimizerCache(x₀)\nhess = Hessian(obj, x₀; mode = :autodiff)\nupdate!(hess, x₀)\nupdate!(_cache, x₀, gradient(obj), hess)\ngradient!(obj, x₁)\nvalue!(obj, x₁)\nupdate!(hess, x₁)\nupdate!(_cache, x₁, gradient(obj), hess)\nls_obj = linesearch_objective(obj, _cache)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\nnothing # hide","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"info: Info\nNote the different shape of the line search objective in the case of the optimizer, especially that the line search objective can take negative values in this case!","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now again want to find the minimum with quadratic line search and repeat the procedure above:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₀ = fˡˢ(0.)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"p₁ = ∂fˡˢ∂α(0.)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"α₀ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)\nα₁ = adjust_α(αₜ, α₀)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"What we see here is that we do not use alpha_t = -p_1  (2p_2) as SimpleSolvers.adjust_α instead picks the left point in the interval sigma_0alpha_0 sigma_1alpha_0 as the change computed with alpha_t would be too small.","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We now again move the original x in the Newton direction with step length alpha_1:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"x .= compute_new_iterate(x, α₁, direction(_cache))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We make another iteration:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"gradient!(obj, x)\nvalue!(obj, x)\nupdate!(hess, x)\nupdate!(_cache, x, gradient(obj), hess)\nls_obj = linesearch_objective(obj, _cache)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\np₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nα₀⁽²⁾ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀⁽²⁾) / α₀⁽²⁾^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"α₂ = adjust_α(αₜ, α₀⁽²⁾)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We see that for alpha_2 (as opposed to alpha_1) we have alpha_2 = alpha_t as alpha_t is in (this is what SimpleSolvers.adjust_α checks for):","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"using SimpleSolvers: DEFAULT_ARMIJO_σ₀, DEFAULT_ARMIJO_σ₁ # hide\n(DEFAULT_ARMIJO_σ₀ * α₀⁽²⁾, DEFAULT_ARMIJO_σ₁ * α₀⁽²⁾)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"x .= compute_new_iterate(x, α₂, direction(_cache))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"We finally compute a third iterate:","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"gradient!(obj, x)\nvalue!(obj, x)\nupdate!(hess, x)\nupdate!(_cache, x, gradient(obj), hess)\nls_obj = linesearch_objective(obj, _cache)\n\nfˡˢ = ls_obj.F\n∂fˡˢ∂α = ls_obj.D\np₀ = fˡˢ(0.)\np₁ = ∂fˡˢ∂α(0.)\nα₀⁽³⁾ = determine_initial_α(ls_obj, SimpleSolvers.DEFAULT_ARMIJO_α₀)\ny = fˡˢ(α₀)\np₂ = (y - p₀ - p₁*α₀⁽³⁾) / α₀^2\np(α) = p₀ + p₁ * α + p₂ * α^2\nαₜ = -p₁ / (2p₂)\nα₃ = adjust_α(αₜ, α₀⁽³⁾)","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"x .= compute_new_iterate(x, α₃, direction(_cache))","category":"page"},{"location":"linesearch/quadratic/","page":"Quadratic","title":"Quadratic","text":"(Image: )","category":"page"},{"location":"linesearch/sufficient_decrease_condition/#The-Sufficient-Decrease-Condition","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"","category":"section"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"The Armijo condition or sufficient decrease condition states:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"    f(R_x_k(alpha_kp_k)) leq f(x_k) + c_1g_x_k(alpha_kp_k mathrmgrad^g_x_kf)  ","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"for some constant c_1in(0 1) (see SimpleSolvers.DEFAULT_WOLFE_c₁).","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"The sufficient decrease condition can also be written as ","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"    fracf(R_x_k(alpha_kp_k)) - f(x_k)alpha_k leq g_x_k(c_1p_k mathrmgrad^g_x_kf)","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"As we assume that f(R_x_k(alpha_kp_k)) leq f(x_k) and g_x_k(c_1p_k mathrmgrad^g_x_kf)  0, we can rewrite this as:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"    fracf(R_x_k(alpha_kp_k)) - f(x_k)alpha_k geq g_x_k(c_1p_k mathrmgrad^g_x_kf)","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"making clear why this is called the sufficient decrease condition. The parameter c_1 is typically chosen very small, around 10^-4. This is implemented as SimpleSolvers.SufficientDecreaseCondition.","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"We can visualize the sufficient decrease condition with an example:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"using SimpleSolvers # hide\nusing SimpleSolvers: SufficientDecreaseCondition, NewtonOptimizerCache, update!, gradient!, linesearch_objective, ldiv! # hide\n\nx = [3., 1.3]\nf = x -> 10 * sum(x .^ 3 / 6 - x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\nhes = Hessian(obj, x; mode = :autodiff)\nupdate!(hes, x)\n\nc₁ = 1e-4\ng = gradient!(obj, x)\nrhs = -g\n# the search direction is determined by multiplying the right hand side with the inverse of the Hessian from the left.\np = similar(rhs)\nldiv!(p, hes, rhs)\nsdc = SufficientDecreaseCondition(c₁, x, f(x), g, p, obj)\n\n# check different values\nα₁, α₂, α₃, α₄, α₅ = .09, .4, 0.7, 1., 1.3\n(sdc(α₁), sdc(α₂), sdc(α₃), sdc(α₄), sdc(α₅))","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"We can also illustrate this:","category":"page"},{"location":"linesearch/sufficient_decrease_condition/","page":"The Sufficient Decrease Condition","title":"The Sufficient Decrease Condition","text":"(Image: )","category":"page"},{"location":"objectives/#Objectives","page":"Objectives","title":"Objectives","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"A central object in SimpleSolvers are objectives (see SimpleSolvers.AbstractObjective). They are either SimpleSolvers.AbstractUnivariateObjectives or MultivariateObjectives. The goal of a solver (both LinearSolvers and NonlinearSolvers) is to make the objective have value zero. The goal of an Optimizer is to minimize a MultivariateObjective.","category":"page"},{"location":"objectives/#Examples","page":"Objectives","title":"Examples","text":"","category":"section"},{"location":"objectives/#Univariate-Objectives","page":"Objectives","title":"Univariate Objectives","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"We can allocate a univariate objective with:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"using SimpleSolvers\nusing Random; Random.seed!(123) # hide\n\nf(x::Number) = x ^ 2\nx = rand()\nobj = UnivariateObjective(f, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Associated to UnivariateObjective are the following functions (amongst others):","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"value\nvalue!\nvalue!!\nderivative\nderivative!\nderivative!!","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The function value evaluates the objective at the provided input and increases the counter by 1:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value(obj, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"We can check how the function call changed obj:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"obj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The stored value for f has not been updated. In order to so we can call the in-place function value!:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value!(obj, x)\nobj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"We further note that SimpleSolvers contains another function value!! that forces evaluation. This is opposed to value! which does not always force evaluation:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value!(obj, x)\nobj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"So value! first checks if the objective obj has already been called on x. In order to force another evaluation we can write:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"y = value!!(obj, x)\nobj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The function value can also be called without additional input arguments:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"value(obj)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"But then the associated function is not called again (calling value this way does not increase the counter):","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"obj","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"An equivalent relationship exists between the functions derivative, derivative! and derivative!!.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"In addition to UnivariateObjective, SimpleSolvers also contains a TemporaryUnivariateObjective[1]:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"[1]: To be used together with SimpleSolvers.linesearch_objective.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"t_obj = TemporaryUnivariateObjective(obj.F, obj.D)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"info: Why are there two types of univariate objectives?\nThere are two types of univariate objectives in SimpleSolvers: UnivariateObjectives and TemporaryUnivariateObjectives. The latter is only used for allocating line search objectives and contains less functionality.","category":"page"},{"location":"objectives/#Multivariate-Objectives","page":"Objectives","title":"Multivariate Objectives","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"MultivariateObjectives are used in a way similar to UnivariateObjectives, the difference is that the derivative functions are replaced by gradient functions, i.e.:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"derivative implies gradient,\nderivative! implies gradient!,\nderivative!! implies gradient!!.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Random.seed!(123) # hide\nf(x::AbstractArray) = sum(x .^ 2)\nx = rand(3)\n\nobj = MultivariateObjective(f, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Every instance of MultivariateObjective stores an instance of Gradient to which we similarly can apply the functions gradient or gradient!:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"gradient(obj, x)","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"The difference to Gradient is that we also store the value for the evaluated gradient, which can be accessed by calling:","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"gradient(obj)","category":"page"},{"location":"in_place_out_of_place/#What-is-In-Place-and-what-is-Out-Of-Place","page":"In-place vs out-of-place","title":"What is In-Place and what is Out-Of-Place","text":"","category":"section"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"In SimpleSolvers we almost always use in-place functions internally for performance, but let the user deal with out-of-place functions for ease of use.","category":"page"},{"location":"in_place_out_of_place/#Example","page":"In-place vs out-of-place","title":"Example","text":"","category":"section"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"using SimpleSolvers\n\nf(x) = sum(x.^2 .* exp.(-abs.(x)) + 2 * cos.(x) .* exp.(-x.^2))\nnothing # hide","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"(Image: )","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"If we now allocate a MultivariateObjective based on this, we get a series of in-place functions based on this. For example value![1]:","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"[1]: See the section on objectives for an explanation of how to use value! and value.","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"x = [0.]\nobj = MultivariateObjective(f, x)\ny = [0.]\nvalue!(obj, x)\n@assert value(obj) == f(x) # hide\nvalue(obj) == f(x)","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"To compute the derivative we can use gradient![2]:","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"[2]: Note that we are using a MultivariateObjective and therefore gradient!. A UnivariateObjective has to be used together with derivative.","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"x = [[x] for x in -7.:.1:7.]\ny = Vector{Float64}[]\nfor x_sing in x\n    gradient!(obj, x_sing)\n    push!(y, copy(gradient(obj)))\nend\nnothing # hide","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"(Image: )","category":"page"},{"location":"in_place_out_of_place/","page":"In-place vs out-of-place","title":"In-place vs out-of-place","text":"The idea is however that the user almost never used the in-place versions of these routines directly, but instead functions like solve! and value, gradient etc. as a possible diagnostic.","category":"page"},{"location":"gradients/#Gradients","page":"Gradients","title":"Gradients","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"The supertype Gradient comprises different ways of taking gradients:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"GradientFunction,\nGradientAutodiff,\nGradientFiniteDifferences.","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"We first start by showing GradientAutodiff:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"using SimpleSolvers, Random; using SimpleSolvers: GradientAutodiff, Gradient, GradientFunction, GradientFiniteDifferences; Random.seed!(123) # hide\nf(x::AbstractArray) = sum(x .^ 2)\nx = rand(3)\ngrad = GradientAutodiff(f, x)","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"Instead of calling GradientAutodiff(f, x) we can equivalently do:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"grad = Gradient(f, x; mode = :autodiff)","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"When calling an instance of Gradient we can use the functions gradient and gradient![1]:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"[1]: Internally these functions call functors that are implemented for the individual structs derived from Gradient, but for consistency (especially with regards to MultivariateObjectives) we recommend using the functions gradient and gradient!.","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"gradient(x, grad)","category":"page"},{"location":"linesearch/linesearch/#Line-Search","page":"Line Searches","title":"Line Search","text":"","category":"section"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"This page is largely a summary of [3, Chapter 3]. We summarize this reference by omitting proofs, but also aim to extend it to manifolds.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"A line search method has the goal of minimizing an objective (either a UnivariateObjective or a MultivariateObjective) approximately, based on a search direction[1].","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"[1]: in [3] (and other references) a search direction is called a descent direction.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"info: Definition\nFor an objective fmathcalMtomathbbR on a manifold mathcalM a search direction at point x_kinmathcalM is a vector p_kinT_x_kmathcalM for which we have    g_x_k(p_k mathrmgrad^g_x_kf)  0where g_x_kT_x_kmathcalMtimesT_x_kmathcalMtomathbbR is a Riemannian metric.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"A line search is therefore a sub-optimization problem in a nonlinear optimizer (or solver) in which we want to find an alpha that minimizes:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"    min_alphaf^mathrmls(alpha) = min_alphaf(mathcalR_x_k(alpha_kp_k))","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"where p_k is the search direction.","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"For line search methods we have to (i) find a search direction p_k and (ii) find an appropriate step size alpha_k = mathrmargmin_alphaf(alpha). We then update x_k based on these quantities:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"    x_k+1 gets mathcalR_x_k(alpha_kp_k)","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"where mathcalR_x_kT_x_kmathcalMtomathcalM is a retraction at x_k","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"In practice we will not be able to find the ideal alpha at every step, but only an approximation thereof. Examples of line search algorithms that aim at finding this alpha are the static line search and the backtracking line search.","category":"page"},{"location":"linesearch/linesearch/#Line-Search-Objective","page":"Line Searches","title":"Line Search Objective","text":"","category":"section"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"SimpleSolvers contains a function SimpleSolvers.linesearch_objective that allocates a TemporaryUnivariateObjective that realizes the function f^mathrmls described above.","category":"page"},{"location":"linesearch/linesearch/#Search-Directions-for-Optimizers","page":"Line Searches","title":"Search Directions for Optimizers","text":"","category":"section"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"In SimpleSolvers we typically build the search direction by multiplying the gradient with a Hessian. When starting at x_k we take:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"    p_k = H_x_k^-1(nabla_x_kf)","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"where H_x_k_ij = partial^2fpartialx_ipartialx_j_x_k is the Hessian. Note that we often use approximations of this Hessian in practice (such as the HessianBFGS).","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"For manifolds [5] defining a Hessian, equivalently to defining a gradient, requires a Riemannian metric and the associated Levi-Civita connection nabla:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"mathrmHess(f) = nablanablaf = nabladf in Gamma(T^*mathcalMotimesT^*mathcalM)","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"For specific vector fields xi eta in Gamma(TmathcalM) we can write this as:","category":"page"},{"location":"linesearch/linesearch/","page":"Line Searches","title":"Line Searches","text":"langle mathrmHess(f)xi eta  rangle = xi(etaf) - (nabla_xieta)f","category":"page"},{"location":"hessians/#Hessians","page":"Hessians","title":"Hessians","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Hessians are a crucial ingredient in NewtonSolvers and SimpleSolvers.NewtonOptimizerStates.","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"using SimpleSolvers\nusing LinearAlgebra: norm\n\nx = rand(3)\nobj = MultivariateObjective(x -> norm(x - vcat(0., 0., 1.))  ^ 2, x)\nhes = HessianAutodiff(obj, x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"An instance of HessianAutodiff stores a Hessian matrix:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"hes.H","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"The instance of HessianAutodiff can be called:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"hes(x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Or equivalently with:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"update!(hes, x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"This updates hes.H:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"hes.H","category":"page"},{"location":"hessians/#BFGS-Hessian","page":"Hessians","title":"BFGS Hessian","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"using SimpleSolvers: initialize!\nhes = HessianBFGS(obj, x)\ninitialize!(hes, x)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"For computational reasons we save the inverse of the Hessian, it can be accessed by calling inv:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"inv(hes)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Similarly to HessianAutodiff we can call SimpleSolvers.update!:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"using SimpleSolvers: update!\n\nupdate!(hes, x)","category":"page"},{"location":"#SimpleSolvers","page":"Home","title":"SimpleSolvers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_p","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_p","text":"const DEFAULT_ARMIJO_p\n\nConstant used in BacktrackingState. Its value is 0.5\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_α₀","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_α₀","text":"const DEFAULT_ARMIJO_α₀\n\nThe default starting value for alpha used in SufficientDecreaseCondition (also see BacktrackingState and QuadraticState). Its value is 1.0\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_σ₀","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_σ₀","text":"const DEFAULT_ARMIJO_σ₀\n\nConstant used in QuadraticState. Also see DEFAULT_ARMIJO_σ₁.\n\nIt is meant to safeguard against stagnation when performing line searches (see [1]).\n\nIts value is 0.1\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_ARMIJO_σ₁","page":"Home","title":"SimpleSolvers.DEFAULT_ARMIJO_σ₁","text":"const DEFAULT_ARMIJO_σ₁\n\nConstant used in QuadraticState. Also see DEFAULT_ARMIJO_σ₀. Its value is 0.5\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BIERLAIRE_ε","page":"Home","title":"SimpleSolvers.DEFAULT_BIERLAIRE_ε","text":"DEFAULT_BIERLAIRE_ε\n\nA constant that determines the precision in BierlaireQuadraticState. This constant is taken from [2].\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BIERLAIRE_ξ","page":"Home","title":"SimpleSolvers.DEFAULT_BIERLAIRE_ξ","text":"DEFAULT_BIERLAIRE_ξ\n\nA constant on basis of which the b in BierlaireQuadraticState is perturbed in order \"to avoid stalling\" (see [2, Chapter 11.2.1]). Its value is 1.0e-6.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_k","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_k","text":"const DEFAULT_BRACKETING_k\n\nGives the default ratio by which the bracket is increased if bracketing was not successful. See bracket_minimum.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_nmax","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_nmax","text":"Default constant\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_BRACKETING_s","page":"Home","title":"SimpleSolvers.DEFAULT_BRACKETING_s","text":"const DEFAULT_BRACKETING_s\n\nGives the default width of the interval (the bracket). See bracket_minimum.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_GRADIENT_ϵ","page":"Home","title":"SimpleSolvers.DEFAULT_GRADIENT_ϵ","text":"DEFAULT_GRADIENT_ϵ\n\nA constant on whose basis finite differences are computed.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_JACOBIAN_ϵ","page":"Home","title":"SimpleSolvers.DEFAULT_JACOBIAN_ϵ","text":"DEFAULT_JACOBIAN_ϵ\n\nA constant used for computing the finite difference Jacobian.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.DEFAULT_WOLFE_c₁","page":"Home","title":"SimpleSolvers.DEFAULT_WOLFE_c₁","text":"const DEFAULT_WOLFE_c₁\n\nA constant epsilon on which a finite difference approximation of the derivative of the objective is computed. This is then used in the following stopping criterion:\n\nfracf(alpha) - f(alpha_0)epsilon  alphacdotf(alpha_0)\n\nExtended help\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.F_ABSTOL","page":"Home","title":"SimpleSolvers.F_ABSTOL","text":"Absolute tolerance for how close the function value should be to zero. Used in e.g. bisection.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.F_MINDEC","page":"Home","title":"SimpleSolvers.F_MINDEC","text":"minimum value by which the function has to decrease.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.F_RELTOL","page":"Home","title":"SimpleSolvers.F_RELTOL","text":"relative tolerance for the function value.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.F_SUCTOL","page":"Home","title":"SimpleSolvers.F_SUCTOL","text":"succesive tolerance for the function value\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.G_RESTOL","page":"Home","title":"SimpleSolvers.G_RESTOL","text":"tolerance for the residual (?) of the gradient.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.MAX_ITERATIONS","page":"Home","title":"SimpleSolvers.MAX_ITERATIONS","text":"The maximum number of iterations used in an alorithm, e.g. bisection and the functor for BacktrackingState.\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.X_ABSTOL","page":"Home","title":"SimpleSolvers.X_ABSTOL","text":"absolute tolerance for x (the function argument).\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.X_RELTOL","page":"Home","title":"SimpleSolvers.X_RELTOL","text":"relative tolerance for x (the function argument).\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.X_SUCTOL","page":"Home","title":"SimpleSolvers.X_SUCTOL","text":"succesive tolerance for `x\n\n\n\n\n\n","category":"constant"},{"location":"#SimpleSolvers.AbstractNewtonSolver","page":"Home","title":"SimpleSolvers.AbstractNewtonSolver","text":"AbstractNewtonSolver <: NonlinearSolver\n\nA supertype that comprises e.g. NewtonSolver.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.AbstractObjective","page":"Home","title":"SimpleSolvers.AbstractObjective","text":"AbstractObjective\n\nAn objective is a quantity to has to be made zero by a solver or minimized by an optimizer.\n\nSee AbstractUnivariateObjective and MultivariateObjective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.AbstractUnivariateObjective","page":"Home","title":"SimpleSolvers.AbstractUnivariateObjective","text":"AbstractUnivariateObjective <: AbstractObjective\n\nA subtype of AbstractObjective that only depends on one variable. See UnivariateObjective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Backtracking","page":"Home","title":"SimpleSolvers.Backtracking","text":"Backtracking <: LinesearchMethod\n\nThe backtracking method.\n\nConstructors\n\nBacktracking()\n\nExtended help\n\nThe backtracking algorithm starts by setting y_0 gets f(0) and d_0 gets nabla_0f.\n\nThe algorithm is executed by calling the functor of BacktrackingState.\n\nThe following is then repeated until the stopping criterion is satisfied or config.max_iterations (MAX_ITERATIONS by default) is reached:\n\nif value!(obj, α) ≥ y₀ + ls.ϵ * α * d₀\n    α *= ls.p\nelse\n    break\nend\n\nThe stopping criterion as an equation can be written as:\n\nf(alpha)  y_0 + epsilon alpha nabla_0f = y_0 + epsilon (alpha - 0)nabla_0f\n\nNote that if the stopping criterion is not reached, alpha is multiplied with p and the process continues.\n\nSometimes the parameters p and epsilon have different names such as tau and c.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BacktrackingCondition","page":"Home","title":"SimpleSolvers.BacktrackingCondition","text":"BacktrackingCondition\n\nAbstract type comprising the conditions that are used for checking step sizes for the backtracking line search (see BacktrackingState).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BacktrackingState","page":"Home","title":"SimpleSolvers.BacktrackingState","text":"BacktrackingState <: LinesearchState\n\nCorresponding LinesearchState to Backtracking.\n\nKeys\n\nThe keys are:\n\nconfig::Options\nα₀: \nϵ=$(DEFAULT_WOLFE_c₁): a default step size on whose basis we compute a finite difference approximation of the derivative of the objective. Also see DEFAULT_WOLFE_c₁.\np=$(DEFAULT_ARMIJO_p): a parameter with which alpha is decreased in every step until the stopping criterion is satisfied.\n\nFunctor\n\nThe functor is used the following way:\n\nls(obj, α = ls.α₀)\n\nImplementation\n\nThe algorithm starts by setting:\n\nx_0 gets 0\ny_0 gets f(x_0)\nd_0 gets f(x_0)\nalpha gets alpha_0\n\nwhere f is the univariate objective (of type AbstractUnivariateObjective) and alpha_0 is stored in ls. It then repeatedly does alpha gets alphacdotp until either (i) the maximum number of iterations is reached (the max_iterations keyword in Options) or (ii) the following holds:\n\n    f(alpha)  y_0 + epsilon cdot alpha cdot d_0\n\nwhere epsilon is stored in ls.\n\ninfo: Info\nThe algorithm allocates an instance of SufficientDecreaseCondition by calling SufficientDecreaseCondition(ls.ϵ, x₀, y₀, d₀, one(α), obj), here we take the value one for the search direction p, this is because we already have the search direction encoded into the line search objective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BierlaireQuadratic","page":"Home","title":"SimpleSolvers.BierlaireQuadratic","text":"BierlaireQuadratic <: LinesearchMethod\n\nAlgorithm taken from [2].\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BierlaireQuadraticState","page":"Home","title":"SimpleSolvers.BierlaireQuadraticState","text":"BierlaireQuadraticState <: LinesearchState\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Bisection","page":"Home","title":"SimpleSolvers.Bisection","text":"Bisection <: LinesearchMethod\n\nThe bisection method.\n\nConstructors\n\nBisection()\n\nExtended help\n\nThe bisection algorithm starts with an interval and successively bisects it into smaller intervals until a root is found. See bisection.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BisectionState","page":"Home","title":"SimpleSolvers.BisectionState","text":"BisectionState <: LinesearchState\n\nCorresponding LinesearchState to Bisection.\n\nSee bisection for the implementation of the algorithm.\n\nConstructors\n\nBisectionState(options)\nBisectionState(; options)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.BracketMinimumCriterion","page":"Home","title":"SimpleSolvers.BracketMinimumCriterion","text":"BracketMinimumCriterion <: BracketingCriterion\n\nThe criterion used for bracket_minimum.\n\nFunctor\n\nbc(yb, yc)\n\nThis checks whether yc is bigger than yb, i.e. whether c is past the minimum.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.CurvatureCondition","page":"Home","title":"SimpleSolvers.CurvatureCondition","text":"CurvatureCondition <: LinesearchCondition\n\nThe second of the Wolfe conditions [3]. The first one is the SufficientDecreaseCondition.\n\nThis encompasses the standard curvature condition and the strong curvature condition.\n\nConstructor\n\nCurvatureCondition(c, xₖ, gradₖ, pₖ, obj, grad; mode)\n\nHere grad has to be a Gradient and obj an AbstractObjective. The other inputs are either arrays or numbers.\n\nImplementation\n\nFor computational reasons CurvatureCondition also has a field gradₖ₊₁ in which the temporary new gradient is saved.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Gradient","page":"Home","title":"SimpleSolvers.Gradient","text":"Gradient\n\nAbstract type. strcuts that are derived from this need an assoicated functor that computes the gradient of a function (in-place).\n\nImplementation\n\nWhen a custom Gradient is implemented, a functor is needed:\n\nfunction (grad::Gradient)(g::AbstractVector, x::AbstractVector) end\n\nThis functor can also be called with gradient!.\n\nExamples\n\nExamples include:\n\nGradientFunction\nGradientAutodiff\nGradientFiniteDifferences\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientAutodiff","page":"Home","title":"SimpleSolvers.GradientAutodiff","text":"GradientAutodiff <: Gradient\n\nA struct that realizes Gradient by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\n∇config: result of applying ForwardDiff.GradientConfig.\n\nConstructors\n\nGradientAutodiff(F, x::AbstractVector)\nGradientAutodiff(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\ngrad(g, x) = ForwardDiff.gradient!(g, grad.F, x, grad.∇config)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientFiniteDifferences","page":"Home","title":"SimpleSolvers.GradientFiniteDifferences","text":"GradientFiniteDifferences <: Gradient\n\nA struct that realizes Gradient by using finite differences.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nϵ: small constant on whose basis the finite differences are computed.\ne: auxiliary vector used for computing finite differences. It's of the form e_1 = beginbmatrix 1  0  cdots  0 endbmatrix.\ntx: auxiliary vector used for computing finite differences. It stores the offset in the x vector.\n\nConstructor(s)\n\nGradientFiniteDifferences{T}(F, nx::Integer; ϵ)\n\nBy default for ϵ is DEFAULT_GRADIENT_ϵ.\n\nFunctor\n\nThe functor does:\n\nfor j in eachindex(x,g)\n    ϵⱼ = grad.ϵ * x[j] + grad.ϵ\n    fill!(grad.e, 0)\n    grad.e[j] = 1\n    grad.tx .= x .- ϵⱼ .* grad.e\n    f1 = grad.F(grad.tx)\n    grad.tx .= x .+ ϵⱼ .* grad.e\n    f2 = grad.F(grad.tx)\n    g[j] = (f2 - f1) / (2ϵⱼ)\nend\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.GradientFunction","page":"Home","title":"SimpleSolvers.GradientFunction","text":"GradientFunction <: Gradient\n\nA struct that realizes a Gradient by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\n∇F!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\ngrad(g, x) = grad.∇F!(g, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Hessian","page":"Home","title":"SimpleSolvers.Hessian","text":"Hessian\n\nAbstract type. structs derived from this need an associated functor that computes the Hessian of a function (in-place).\n\nAlso see Gradient.\n\nImplementation\n\nWhen a custom Hessian is implemented, a functor is needed:\n\nfunction (hessian::Hessian)(h::AbstractMatrix, x::AbstractVector) end\n\nThis functor can also be called with compute_hessian!.\n\nExamples\n\nExamples include:\n\nHessianFunction\nHessianAutodiff\nHessianBFGS\nHessianDFP\n\nThese examples can also be called with e.g. Hessian(x; mode = :autodiff).\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianAutodiff","page":"Home","title":"SimpleSolvers.HessianAutodiff","text":"HessianAutodiff <: Hessian\n\nA struct that realizes Hessian by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nH: a matrix in which the (updated) Hessian is stored. \nHconfig: result of applying ForwardDiff.HessianConfig.\n\nConstructors\n\nHessianAutodiff(F, x::AbstractVector)\nHessianAutodiff(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\nhes(g, x) = ForwardDiff.hessian!(hes.H, hes.F, x, grad.Hconfig)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianBFGS","page":"Home","title":"SimpleSolvers.HessianBFGS","text":"HessianBFGS <: Hessian\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianDFP","page":"Home","title":"SimpleSolvers.HessianDFP","text":"HessianDFP <: Hessian\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.HessianFunction","page":"Home","title":"SimpleSolvers.HessianFunction","text":"HessianFunction <: Hessian\n\nA struct that realizes a Hessian by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\nH!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\nhes(H, x) = hes.H!(H, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Jacobian","page":"Home","title":"SimpleSolvers.Jacobian","text":"Jacobian\n\nAbstract type. structs that are derived from this need an associated functor that computes the Jacobian of a function (in-place).\n\nImplementation\n\nWhen a custom Jacobian is implemented, a functor is needed:\n\nfunction (j::Jacobian)(g::AbstractMatrix, x::AbstractVector) end\n\nThis functor can also be called with compute_jacobian!.\n\nExamples\n\nExamples include:\n\nJacobianFunction\nJacobianAutodiff\nJacobianFiniteDifferences\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianAutodiff","page":"Home","title":"SimpleSolvers.JacobianAutodiff","text":"JacobianAutodiff <: Jacobian\n\nA struct that realizes Jacobian by using ForwardDiff.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nJconfig: result of applying ForwardDiff.JacobianConfig.\nty: vector that is used for evaluating ForwardDiff.jacobian!\n\nConstructors\n\nJacobianAutodiff(F, y::AbstractVector)\nJacobianAutodiff(F, nx::Integer)\n\nFunctor\n\nThe functor does:\n\njac(J, x) = ForwardDiff.jacobian!(J, jac.ty, x, grad.Jconfig)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianFiniteDifferences","page":"Home","title":"SimpleSolvers.JacobianFiniteDifferences","text":"JacobianFiniteDifferences <: Jacobian\n\nA struct that realizes Jacobian by using finite differences.\n\nKeys\n\nThe struct stores:\n\nF: a function that has to be differentiated.\nϵ: small constant on whose basis the finite differences are computed.\nf1:\nf2:\ne1: auxiliary vector used for computing finite differences. It's of the form e_1 = beginbmatrix 1  0  cdots  0 endbmatrix.\ne2:\ntx: auxiliary vector used for computing finite differences. It stores the offset in the x vector.\n\nConstructor(s)\n\nJacobianFiniteDifferences{T}(F, nx::Integer, ny::Integer; ϵ)\n\nBy default for ϵ is DEFAULT_JACOBIAN_ϵ.\n\nFunctor\n\nThe functor does:\n\nfor j in eachindex(x)\n    ϵⱼ = jac.ϵ * x[j] + jac.ϵ\n    fill!(jac.e, 0)\n    jac.e[j] = 1\n    jac.tx .= x .- ϵⱼ .* jac.e\n    f(jac.f1, jac.tx)\n    jac.tx .= x .+ ϵⱼ .* jac.e\n    f(jac.f2, jac.tx)\n    for i in eachindex(x)\n        J[i,j] = (jac.f2[i] - jac.f1[i]) / (2ϵⱼ)\n    end\nend\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.JacobianFunction","page":"Home","title":"SimpleSolvers.JacobianFunction","text":"JacobianFunction <: Jacobian\n\nA struct that realizes a Jacobian by explicitly supplying a function.\n\nKeys\n\nThe struct stores:\n\nDF!: a function that can be applied in place.\n\nFunctor\n\nThe functor does:\n\njac(g, x) = jac.DF!(g, x)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LUSolver","page":"Home","title":"SimpleSolvers.LUSolver","text":"struct LUSolver <: LinearSolver\n\nA custom implementation of an LU solver.\n\nRoutines that use LUSolver include factorize! and ldiv!. In practice the LUSolver is used by calling its constructor together with ldiv! as shown in the Example section of this docstring.\n\nExample\n\nWe use the LUSolver together with ldiv! to compute multiplication of a matrix inverse onto a vector (from the left):\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\n\nlu = LUSolver(A)\nv = rand(3)\nx = similar(v)\n\nldiv!(x, lu, v) ≈ inv(A) * v\n\n# output\n\ntrue\n\nWhen calling LUSolver on an integer alone, a matrix with all zeros is allocated:\n\nLUSolver{Float32}(2)\n\n# output\n\nLUSolver{Float32}(2, Float32[0.0 0.0; 0.0 0.0], [1, 2], [1, 2], 1)\n\nKeys\n\nn::Int\nA::Matrix{T}\npivots::Vector{Int}\nperms::Vector{Int}\ninfo::Int\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LUSolverLAPACK","page":"Home","title":"SimpleSolvers.LUSolverLAPACK","text":"LUSolverLAPACK <: LinearSolver\n\nThe LU Solver taken from LinearAlgebra.BLAS.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinearSolver","page":"Home","title":"SimpleSolvers.LinearSolver","text":"LinearSolver <: AbstractSolver\n\nA supertype that comprises e.g. LUSolver and LUSolverLAPACK.\n\nConstructor\n\nLinearSolver(x; linear_solver = :julia)\n\nThe convenience constructor allocates a specific struct derived from LinearSolver based on what is supplied to liner_solver. The default :julia calls the constructor for LUSolver. Another option would be :lapack which calls LUSolverLAPACK and uses the LinearAlgebra.BLAS package.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Linesearch","page":"Home","title":"SimpleSolvers.Linesearch","text":"Linesearch\n\nA struct that stores the LinesearchMethod, the LinesearchState and Options.\n\nKeys\n\nalgorithm::LinesearchMethod\nconfig::Options\nstate::LinesearchState\n\nConstructors\n\nThe following constructors can be used:\n\nLinesearch(alg, config, state)\nLinesearch(; algorithm, config, kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinesearchMethod","page":"Home","title":"SimpleSolvers.LinesearchMethod","text":"LinesearchMethod\n\nExamples include StaticState, Backtracking, Bisection and Quadratic. See these examples for specific information on linesearch algorithms.\n\nExtended help\n\nA LinesearchMethod always goes together with a LinesearchState and each of those LinesearchStates has a functor implemented:\n\nls(obj, α)\n\nwhere obj is a AbstractUnivariateObjective and α is an initial step size. The output of this functor is then a final step size that is used for updating the parameters.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.LinesearchState","page":"Home","title":"SimpleSolvers.LinesearchState","text":"LinesearchState\n\nAbstract type. \n\nExamples include StaticState, BacktrackingState, BisectionState and QuadraticState.\n\nImplementation\n\nA struct that is subtyped from LinesearchState needs to implement the functors:\n\nls(x; kwargs...)\nls(obj::AbstractUnivariateObjective, x; kwargs...)\n\nAdditionaly the following function needs to be extended:\n\nLinesearchState(algorithm::LinesearchMethod; kwargs...)\n\nFunctors\n\nThe following functors are auxiliary helper functions:\n\nls(f::Callable; kwargs...) = ls(TemporaryUnivariateObjective(f, missing); kwargs...)\nls(f::Callable, x::Number; kwargs...) = ls(TemporaryUnivariateObjective(f, missing), x; kwargs...)\nls(f::Callable, g::Callable; kwargs...) = ls(TemporaryUnivariateObjective(f, g); kwargs...)\nls(f::Callable, g::Callable, x::Number; kwargs...) = ls(TemporaryUnivariateObjective(f, g), x; kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.MultivariateObjective","page":"Home","title":"SimpleSolvers.MultivariateObjective","text":"MultivariateObjective <: AbstractObjective\n\nLike UnivariateObjective, but stores gradients instead of derivatives.\n\nThe type of the stored gradient has to be a subtype of Gradient.\n\nFunctor\n\nIf MultivariateObjective is called on a single function, the gradient is generated with GradientAutodiff.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonOptimizerCache","page":"Home","title":"SimpleSolvers.NewtonOptimizerCache","text":"NewtonOptimizerCache\n\nKeys\n\nx̄: the previous iterate,\nx: current iterate (this stores the guess called by the functions generated with linesearch_objective),\nδ: direction of optimization step (difference between x and x̄); this is obtained by multiplying rhs with the inverse of the Hessian,\ng: gradient value (this stores the gradient associated with x called by the derivative part of linesearch_objective),\nrhs: the right hand side used to compute the update.\n\nTo understand how these are used in practice see e.g. linesearch_objective.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonOptimizerState","page":"Home","title":"SimpleSolvers.NewtonOptimizerState","text":"NewtonOptimizerState <: OptimizationAlgorithm\n\nKeys\n\nobjective::MultivariateObjective\nhessian::Hessian\nlinesearch::LinesearchState\nls_objective\ncache::NewtonOptimizerCache\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonSolver","page":"Home","title":"SimpleSolvers.NewtonSolver","text":"NewtonSolver\n\nA struct that comprises all Newton solvers. Those typically differ in the way the Jacobian is computed.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NewtonSolverCache","page":"Home","title":"SimpleSolvers.NewtonSolverCache","text":"NewtonSolverCache\n\nStores x₀, x₁, δx, rhs, y and J.\n\nKeys\n\nδx: search direction.\n\nConstructor\n\nNewtonSolverCache(x, y)\n\nJ is allocated by calling alloc_j.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearMethod","page":"Home","title":"SimpleSolvers.NonlinearMethod","text":"A supertype collecting all nonlinear methods, including NewtonMethods.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.NonlinearSolver","page":"Home","title":"SimpleSolvers.NonlinearSolver","text":"NonlinearSolver <: AbstractSolver\n\nA supertype that comprises e.g. AbstractNewtonSolver.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizationAlgorithm","page":"Home","title":"SimpleSolvers.OptimizationAlgorithm","text":"An OptimizationAlgorithm is a data structure that is used to dispatch on different algorithms.\n\nIt needs to implement three methods,\n\ninitialize!(alg::OptimizationAlgorithm, ::AbstractVector)\nupdate!(alg::OptimizationAlgorithm, ::AbstractVector)\nsolver_step!(::AbstractVector, alg::OptimizationAlgorithm)\n\nthat initialize and update the state of the algorithm and perform an actual optimization step.\n\nFurther the following convenience methods should be implemented,\n\nobjective(alg::OptimizationAlgorithm)\ngradient(alg::OptimizationAlgorithm)\nhessian(alg::OptimizationAlgorithm)\nlinesearch(alg::OptimizationAlgorithm)\n\nwhich return the objective to optimize, its gradient and (approximate) Hessian as well as the linesearch algorithm used in conjunction with the optimization algorithm if any.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Optimizer","page":"Home","title":"SimpleSolvers.Optimizer","text":"Optimizer\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerResult","page":"Home","title":"SimpleSolvers.OptimizerResult","text":"OptimizerResult\n\nStores an OptimizerStatus as well as x, f and g (as keys). OptimizerStatus stores all other information (apart form x ,f and g); i.e. residuals etc.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.OptimizerStatus","page":"Home","title":"SimpleSolvers.OptimizerStatus","text":"OptimizerStatus\n\nStores residuals (relative and absolute) and various convergence properties.\n\nSee OptimizerResult.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Options","page":"Home","title":"SimpleSolvers.Options","text":"Configurable options with defaults (values 0 and NaN indicate unlimited):\n\nx_abstol = -Inf,\nx_reltol = 4.440892098500626e-16,\nx_suctol = 4.440892098500626e-16\nf_abstol = 1.0e-50,\nf_reltol = 4.440892098500626e-16,\nf_suctol = 4.440892098500626e-16,\nf_mindec = 0.0001,\ng_restol = 1.4901161193847656e-8,\nx_abstol_break = Inf,\nx_reltol_break = Inf,\nf_abstol_break = Inf,\nf_reltol_break = Inf,\ng_restol_break = Inf,\nf_calls_limit = 0,\ng_calls_limit = 0,\nh_calls_limit = 0,\nallow_f_increases = true,\nmin_iterations = 0,\nmax_iterations = 1000,\nwarn_iterations = 1000,\nshow_trace = false,\nstore_trace = false,\nextended_trace = false,\nshow_every = 1,\nverbosity = 1\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Quadratic","page":"Home","title":"SimpleSolvers.Quadratic","text":"Quadractic <: LinesearchMethod\n\nThe quadratic method. Compare this to BierlaireQuadratic. The algorithm is taken from [1].\n\nConstructors\n\nQuadractic()\n\nExtended help\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.QuadraticState","page":"Home","title":"SimpleSolvers.QuadraticState","text":"QuadraticState <: LinesearchState\n\nQuadratic Polynomial line search.\n\nQuadratic line search works by fitting a polynomial to a univariate objective (see AbstractUnivariateObjective) and then finding the minimum of that polynomial. Also compare this to BierlaireQuadraticState. The algorithm is taken from [1].\n\nKeywords\n\nconfig::Options\nα₀: by default DEFAULT_ARMIJO_α₀\nσ₀: by default DEFAULT_ARMIJO_σ₀\nσ₁: by default DEFAULT_ARMIJO_σ₁\nc: by default DEFAULT_WOLFE_c₁\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.Static","page":"Home","title":"SimpleSolvers.Static","text":"Static <: LinesearchMethod\n\nThe static method.\n\nConstructors\n\nStatic(α)\n\nKeys\n\nKeys include: -α: equivalent to a step size. The default is 1.\n\nExtended help\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.StaticState","page":"Home","title":"SimpleSolvers.StaticState","text":"StaticState <: LinesearchState\n\nThe state for Static.\n\nFunctors\n\nFor a Number a and an AbstractUnivariateObjective obj we have the following functors:\n\nls.(a) = ls.α\nls.(obj, a) = ls.α\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.SufficientDecreaseCondition","page":"Home","title":"SimpleSolvers.SufficientDecreaseCondition","text":"SufficientDecreaseCondition <: LinesearchCondition\n\nThe condition that determines if alpha_k is big enough.\n\nConstructor\n\nSufficientDecreaseCondition(c₁, xₖ, fₖ, gradₖ, pₖ, obj)\n\nFunctors\n\nsdc(xₖ₊₁, αₖ)\nsdc(αₖ)\n\nThe second functor is shorthand for sdc(compute_new_iterate(sdc.xₖ, αₖ, sdc.pₖ), T(αₖ)), also see compute_new_iterate.\n\nExtended help\n\nWe call the constant that pertains to the sufficient decrease condition c. This is typically called c_1 in the literature [3]. See DEFAULT_WOLFE_c₁ for the relevant constant\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.TemporaryUnivariateObjective","page":"Home","title":"SimpleSolvers.TemporaryUnivariateObjective","text":"TemporaryUnivariateObjective <: AbstractUnivariateObjective\n\nLike UnivariateObjective but doesn't store f, d, x_f and x_d as well as f_calls and d_calls.\n\n\n\n\n\n","category":"type"},{"location":"#SimpleSolvers.UnivariateObjective","page":"Home","title":"SimpleSolvers.UnivariateObjective","text":"UnivariateObjective <: AbstractUnivariateObjective\n\nKeywords\n\nIt stores the following:\n\nF: objective\nD: derivative of objective\nf: cache for function output\nd: cache for derivative output\nx_f: x used to evaluate F (stored in f)\nx_d: x used to evaluate D (stored in d)\nf_calls: number of times F has been called\nd_calls: number of times D has been called\n\nConstructor\n\nThere are several constructors, the most generic (besides the default one) is:\n\nUnivariateObjective(F, D, x; f, d)\n\nWhere no keys are inferred, except x_f and x_d (via alloc_f and alloc_d). f_calls and d_calls are set to zero.\n\nThe most general constructor (i.e. the one the needs the least specification) is:\n\nf(x::Number) = x ^ 2\nUnivariateObjective(f, 1.)\n\n# output\n\nUnivariateObjective:\n\n    f(x)              = NaN\n    d(x)              = NaN\n    x_f               = NaN\n    x_d               = NaN\n    number of f calls = 0\n    number of d calls = 0\n\nwhere ForwardDiff is used to generate the derivative of the (anonymous) function.\n\nFunctor\n\nThe functor calls value!.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricBase.value-Tuple{SimpleSolvers.AbstractObjective, Union{Number, AbstractArray{<:Number}}}","page":"Home","title":"GeometricBase.value","text":"value(obj::AbstractObjective, x)\n\nEvaluates the objective value at x (i.e. computes obj.F(x)).\n\nExamples\n\nusing SimpleSolvers\n\nobj = UnivariateObjective(x::Number -> x^2, 1.)\nvalue(obj, 2.)\nobj.f_calls\n\n# output\n\n1\n\nNote that the f_calls counter increased by one!\n\nIf value is called on obj (an AbstractObjective) without supplying x than the output of the last obj.F call is returned:\n\nusing SimpleSolvers\n\nobj = UnivariateObjective(x::Number -> x^2, 1.)\nvalue(obj)\n\n# output\n\nNaN\n\nIn this example this is NaN since the function hasn't been called yet.\n\n\n\n\n\n","category":"method"},{"location":"#LinearAlgebra.ldiv!-Union{Tuple{T}, Tuple{AbstractVector{T}, LUSolver{T}, AbstractVector{T}}} where T","page":"Home","title":"LinearAlgebra.ldiv!","text":"ldiv!(x, lu, b)\n\nCompute inv(lu.A) * b by utilizing the factorization in the LUSolver and store the result in x.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.adjust_α-Union{Tuple{T}, Tuple{SimpleSolvers.QuadraticState{T}, T, T}} where T","page":"Home","title":"SimpleSolvers.adjust_α","text":"adjust_α(ls, αₜ, α)\n\nCheck which conditions the new αₜ is in sigma_0alpha_0 simga_1alpha_0 and return the updated α accordingly (it is updated if it does not lie in the interval).\n\nWe first check the following:\n\n    alpha_t   alpha_0alpha\n\nwhere sigma_0 is stored in ls (i.e. in an instance of QuadraticState). If this is not true we check:\n\n    alpha_t  sigma_1alpha\n\nwhere sigma_1 is again stored in ls. If this second condition is also not true we simply return the unchanged alpha_t. So if \\alpha_t does not lie in the interval (sigma_0alpha sigma_1alpha) the interval is made bigger by either multiplying with sigma_0 (default DEFAULT_ARMIJO_σ₀) or sigma_1 (default DEFAULT_ARMIJO_σ₁).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.adjust_α-Union{Tuple{T}, Tuple{T, T}, Tuple{T, T, T}, NTuple{4, T}} where T","page":"Home","title":"SimpleSolvers.adjust_α","text":"adjust_α(αₜ, α)\n\nAdjust αₜ based on the previous α. Also see adjust_α(::QuadraticState{T}, ::T, ::T) where {T}.\n\nThe check that alpha in sigma_0alpha_mathrmold sigma_1alpha_mathrmold should safeguard against stagnation in the iterates as well as checking that alpha decreases at least by a factor sigma_1. The defaults for σ₀ and σ₁ are DEFAULT_ARMIJO_σ₀ and DEFAULT_ARMIJO_σ₁ respectively.\n\nImplementation\n\nWee use defaults DEFAULT_ARMIJO_σ₀ and DEFAULT_ARMIJO_σ₁.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.alloc_d","page":"Home","title":"SimpleSolvers.alloc_d","text":"alloc_d(x)\n\nAllocate NaNs of the size of the derivative of f (with respect to x).\n\nThis is used in combination with a AbstractUnivariateObjective.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_f","page":"Home","title":"SimpleSolvers.alloc_f","text":"alloc_f(x)\n\nAllocate NaNs of the size the size of f (evaluated at x).\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_g","page":"Home","title":"SimpleSolvers.alloc_g","text":"alloc_g(x)\n\nAllocate NaNs of the size of the gradient of f (with respect to x).\n\nThis is used in combination with a MultivariateObjective.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_h","page":"Home","title":"SimpleSolvers.alloc_h","text":"alloc_h(x)\n\nAllocate NaNs of the size of the Hessian of f (with respect to x).\n\nThis is used in combination with a MultivariateObjective.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_j","page":"Home","title":"SimpleSolvers.alloc_j","text":"alloc_j(x, f)\n\nAllocate NaNs of the size of the Jacobian of f (with respect to x).\n\nThis is used in combination with a MultivariateObjective.\n\nExamples\n\nx = rand(3)\nfₓ = rand(2)\nalloc_j(x, fₓ)\n\n# output\n\n2×3 Matrix{Float64}:\n NaN  NaN  NaN\n NaN  NaN  NaN\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.alloc_x","page":"Home","title":"SimpleSolvers.alloc_x","text":"alloc_x(x)\n\nAllocate NaNs of the size of x.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.assess_convergence!-Tuple{SimpleSolvers.OptimizerStatus, Options}","page":"Home","title":"SimpleSolvers.assess_convergence!","text":"assess_convergence!(status, config)\n\nChecks if the optimizer converged.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bisection-Tuple{Any, Number}","page":"Home","title":"SimpleSolvers.bisection","text":"bisection(f, x)\n\nUse bracket_minimum to find a starting interval and then do bisections.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bisection-Union{Tuple{T}, Tuple{Union{Function, Type}, T, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bisection","text":"bisection(f, xmin, xmax; config)\n\nPerform bisection of f in the interval [xmin, xmax] with Options config.\n\nThe algorithm is repeated until a root is found (up to tolerance config.f_abstol which is F_ABSTOL by default).\n\nimplementation\n\nWhen calling bisection it first checks if x_mathrmmin  x_mathrmmax and else flips the two entries.\n\nExtended help\n\nThe bisection algorithm divides an interval into equal halves until a root is found (up to a desired accuracy).\n\nWe first initialize:\n\nbeginaligned\nx_0 gets  x_mathrmmin\nx_1 gets  x_mathrmmax\nendaligned\n\nand then repeat:\n\nbeginaligned\n x gets fracx_0 + x_12 \n textif f(x_0)f(x)  0 \n qquad x_0 gets x \n textelse \n qquad x_1 gets x \n textend\nendaligned\n\nSo the algorithm checks in each step where the sign change occurred and moves the x_0 or x_1 accordingly. The loop is terminated (and errors) if config.max_iterations is reached (see MAX_ITERATIONS and the Options struct).\n\nwarning: Warning\nThe obvious danger with using bisections is that the supplied interval can have multiple roots (or no roots). One should be careful to avoid this when fixing the interval.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_minimum-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_minimum","text":"bracket_minimum(f, x)\n\nMove a bracket successively in the search direction (starting at x) and increase its size until a local minimum of f is found.  This is used for performing Bisections when only one x is given (and not an entire interval).  This bracketing algorithm is taken from [4]. Also compare it to bracket_minimum_with_fixed_point.\n\nKeyword arguments\n\ns::DEFAULT_BRACKETING_s\nk::DEFAULT_BRACKETING_k\nnmax::DEFAULT_BRACKETING_nmax\n\nExtended help\n\nFor bracketing we need two constants s and k (see DEFAULT_BRACKETING_s and DEFAULT_BRACKETING_k). \n\nBefore we start the algorithm we initialize it, i.e. we check that we indeed have a descent direction:\n\nbeginaligned\n a gets x \n b gets a + s \n mathrmif quad f(b)  f(a)\n qquadtextFlip a and b and set sgets-s\n mathrmend\nendaligned\n\nThe algorithm then successively computes:\n\nc gets b + s\n\nand then checks whether f(c)  f(b). If this is true it returns (a c) or (c a), depending on whether ac or ca respectively. If this is not satisfied a b and s are updated:\n\nbeginaligned\na gets  b \nb gets  c \ns gets  sk \nendaligned\n\nand the algorithm is continued. If we have not found a sign chance after n_mathrmmax iterations (see DEFAULT_BRACKETING_nmax) the algorithm is terminated and returns an error. The interval that is returned by bracket_minimum is then typically used as a starting point for bisection.\n\ninfo: Info\nThe function bracket_root is equivalent to bracket_minimum with the only difference that the criterion we check for is:f(c)f(b)  0i.e. that a sign change in the function occurs.\n\nSee bracket_root.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_minimum_with_fixed_point-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_minimum_with_fixed_point","text":"bracket_minimum_with_fixed_point(f, x)\n\nFind a bracket while keeping the left side (i.e. x) fixed.  The algorithm is similar to bracket_minimum (also based on DEFAULT_BRACKETING_s and DEFAULT_BRACKETING_k) with the difference that for the latter the left side is also moving.\n\nThe function bracket_minimum_with_fixed_point is used as a starting point for Quadratic (taken from [1]), as the minimum of the polynomial approximation is:\n\np_2 = fracf(b) - f(a) - f(0)bb^2\n\nwhere b = mathttbracket_minimum_with_fixed_point(a). We check that f(b)  f(a) in order to ensure that the curvature of the polynomial (i.e. p_2 is positive) and we have a minimum.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.bracket_root-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T<:Number","page":"Home","title":"SimpleSolvers.bracket_root","text":"bracket_root(f, x)\n\nMake a bracket for the function based on x (for root finding).\n\nThis is largely equivalent to bracket_minimum. See the end of that docstring for more information.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_gradient-Tuple{AbstractVector}","page":"Home","title":"SimpleSolvers.check_gradient","text":"check_gradient(g)\n\nCheck norm, maximum value and minimum value of a vector.\n\nExamples\n\nusing SimpleSolvers\n\ng = [1., 1., 1., 2., 0.9, 3.]\nSimpleSolvers.check_gradient(g; digits=3)\n\n# output\n\nnorm(Gradient):               4.1\nminimum(|Gradient|):          0.9\nmaximum(|Gradient|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_hessian-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers.check_hessian","text":"check_hessian(H)\n\nCheck the condition number, determinant, max and min value of the Hessian H.\n\nusing SimpleSolvers\n\nH = [1. √2.; √2. 3.]\nSimpleSolvers.check_hessian(H)\n\n# output\n\nCondition Number of Hessian: 13.9282\nDeterminant of Hessian:      1.0\nminimum(|Hessian|):          1.0\nmaximum(|Hessian|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.check_jacobian-Tuple{AbstractMatrix}","page":"Home","title":"SimpleSolvers.check_jacobian","text":"check_jacobian(J)\n\nCheck the condition number, determinant, max and min value of the Jacobian J.\n\nusing SimpleSolvers\n\nJ = [1. √2.; √2. 3.]\nSimpleSolvers.check_jacobian(J)\n\n# output\n\nCondition Number of Jacobian: 13.9282\nDeterminant of Jacobian:      1.0\nminimum(|Jacobian|):          1.0\nmaximum(|Jacobian|):          3.0\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Tuple{MultivariateObjective}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(obj)\n\nSimilar to initialize!, but return nothing.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Tuple{SimpleSolvers.AbstractUnivariateObjective}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(obj)\n\nSimilar to initialize!, but return nothing.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Union{Tuple{SimpleSolvers.OptimizerResult{XT, YT, VT, OST} where {VT<:(AbstractArray{XT}), OST<:SimpleSolvers.OptimizerStatus{XT, YT}}}, Tuple{YT}, Tuple{XT}} where {XT, YT}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(obj)\n\nSimilar to initialize!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.clear!-Union{Tuple{SimpleSolvers.OptimizerStatus{XT, YT}}, Tuple{YT}, Tuple{XT}} where {XT, YT}","page":"Home","title":"SimpleSolvers.clear!","text":"clear!(obj)\n\nSimilar to initialize!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_gradient!","page":"Home","title":"SimpleSolvers.compute_gradient!","text":"compute_gradient!\n\nAlias for gradient!. Will probably be deprecated.\n\n\n\n\n\n","category":"function"},{"location":"#SimpleSolvers.compute_hessian!-Tuple{AbstractMatrix, AbstractVector, Any}","page":"Home","title":"SimpleSolvers.compute_hessian!","text":"compute_hessian!(h, x, ForH)\n\nCompute the hessian of function ForH at x and store it in h.\n\nImplementation\n\nInternally this allocates a Hessian object.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_hessian!-Tuple{AbstractMatrix, AbstractVector, Hessian}","page":"Home","title":"SimpleSolvers.compute_hessian!","text":"compute_hessian!(h, x, hessian)\n\nCompute the Hessian and store it in h.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_hessian-Tuple{Any, Hessian}","page":"Home","title":"SimpleSolvers.compute_hessian","text":"compute_hessian(x, hessian)\n\nCompute the Hessian at point x and return the result.\n\nInternally this calls compute_hessian!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_hessian_ad!-Union{Tuple{FT}, Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, FT}} where {T, FT}","page":"Home","title":"SimpleSolvers.compute_hessian_ad!","text":"compute_hessian_ad!(g, x, F)\n\nBuild a HessianAutodiff object based on F and apply it to x. The result is stored in H.\n\nAlso see gradient_ad! for the Gradient version.\n\nImplementation\n\nThis is using compute_hessian! with the keyword mode set to autodiff.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_jacobian!-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, Jacobian{T}}} where T","page":"Home","title":"SimpleSolvers.compute_jacobian!","text":"compute_jacobian!(j, x, jacobian::Jacobian)\n\nApply the Jacobian and store the result in j.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_jacobian!-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, Union{Function, Type}}} where T","page":"Home","title":"SimpleSolvers.compute_jacobian!","text":"compute_jacobian!(j, x, ForJ)\n\nAllocate a Jacobian object, apply it to x, and store the result in j.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.compute_new_iterate-Union{Tuple{TVT}, Tuple{VT}, Tuple{T}, Tuple{VT, T, TVT}} where {T, VT, TVT}","page":"Home","title":"SimpleSolvers.compute_new_iterate","text":"compute_new_iterate(xₖ, αₖ, pₖ)\n\nCompute xₖ₊₁ based on a direction pₖ and a step length αₖ.\n\nExtended help\n\nIn the case of vector spaces this function simply does:\n\nxₖ + αₖ * pₖ\n\nFor manifolds we instead perform a retraction [5].\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.derivative!!-Tuple{UnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.derivative!!","text":"derivative!!(obj::AbstractObjective, x)\n\nSimilar to value!!, but fo the derivative part (see UnivariateObjective and TemporaryUnivariateObjective).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.derivative!-Tuple{UnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.derivative!","text":"derivative!(obj, x)\n\nSimilar to value!, but fo the derivative part (see UnivariateObjective).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.derivative-Tuple{UnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.derivative","text":"derivative(obj::AbstractObjective, x)\n\nSimilar to value, but for the derivative part (see UnivariateObjective).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.determine_initial_α-Union{Tuple{T}, Tuple{SimpleSolvers.AbstractUnivariateObjective, T}, Tuple{SimpleSolvers.AbstractUnivariateObjective, T, T}} where T","page":"Home","title":"SimpleSolvers.determine_initial_α","text":"determine_initial_α(y₀, obj, α₀)\n\nCheck whether α₀ satisfies the BracketMinimumCriterion for obj. If the criterion is not satisfied we call bracket_minimum_with_fixed_point. This is used as a starting point for using the functor of QuadraticState and makes sure that α describes a point past the minimum.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.direction-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.direction","text":"direction(::NewtonOptimizerCache)\n\nReturn the direction of the gradient step (i.e. δ) of an instance of NewtonOptimizerCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.factorize!-Union{Tuple{T}, Tuple{LUSolver{T}, AbstractMatrix{T}}} where T","page":"Home","title":"SimpleSolvers.factorize!","text":"factorize!(lu, A)\n\nFactorize the matrix A and store the result in lu.A.\n\nExamples\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\nlu = LUSolver{Float64}(3, similar(A), zeros(Int, 3), zeros(Int, 3), 0)\nfactorize!(lu, A)\nlu.A\n\n# output\n\n3×3 Matrix{Float64}:\n 13.0        17.0       19.0\n  0.0769231   0.692308   1.53846\n  0.384615    0.666667   2.66667\n\nHere lu.A stores the factorized result. If we want to save this factorized matrix in the same A to save memory we can write:\n\nA = [1. 2. 3.; 5. 7. 11.; 13. 17. 19.]\nlu = LUSolver{Float64}(3, A, zeros(Int, 3), zeros(Int, 3), 0)\nfactorize!(lu, A)\nA\n\n# output\n\n3×3 Matrix{Float64}:\n 13.0        17.0       19.0\n  0.0769231   0.692308   1.53846\n  0.384615    0.666667   2.66667\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.find_maximum_value-Union{Tuple{T}, Tuple{AbstractVector{T}, Integer}} where T<:Number","page":"Home","title":"SimpleSolvers.find_maximum_value","text":"find_maximum_value(v, k)\n\nFind the maximum value of vector v starting from the index k.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient!!-Tuple{MultivariateObjective, AbstractArray{<:Number}}","page":"Home","title":"SimpleSolvers.gradient!!","text":"gradient(obj::MultivariateObjective, x)\n\nLike derivative!!, but for MultivariateObjective, not UnivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient!-Tuple{AbstractVector, AbstractVector, Gradient}","page":"Home","title":"SimpleSolvers.gradient!","text":"gradient!(g, x, grad)\n\nApply the Gradient grad to x and store the result in g.\n\nImplementation\n\nThis is equivalent to doing\n\ngrad(g, x)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient!-Tuple{MultivariateObjective, AbstractArray{<:Number}}","page":"Home","title":"SimpleSolvers.gradient!","text":"gradient!(obj::MultivariateObjective, x)\n\nLike derivative!, but for MultivariateObjective, not UnivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{Any, Gradient}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(x, grad)\n\nApply grad to x and return the result. \n\nImplementation\n\nInternally this is using gradient!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{MultivariateObjective}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(x, obj::MultivariateObjective)\n\nLike derivative, but for MultivariateObjective, not UnivariateObjective.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.gradient","text":"gradient(::NewtonOptimizerCache)\n\nReturn the stored gradient (array) of an instance of NewtonOptimizerCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient_ad!-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}, Union{Function, Type}}} where T<:Number","page":"Home","title":"SimpleSolvers.gradient_ad!","text":"gradient_ad!(g, x, F)\n\nBuild a GradientAutodiff object based on F and apply it to x. The result is stored in g.\n\nAlso see gradient_fd! for the finite differences version.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.gradient_fd!-Union{Tuple{FT}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}, FT}} where {T, FT}","page":"Home","title":"SimpleSolvers.gradient_fd!","text":"gradient_fd!(g, x, F)\n\nBuild a GradientFiniteDifferences object based on F and apply it to x. The result is stored in g.\n\nAlso see gradient_ad! for the autodiff version.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.increase_iteration_number!-Tuple{SimpleSolvers.NonlinearSolverStatus}","page":"Home","title":"SimpleSolvers.increase_iteration_number!","text":"increase_iteration_number!(status)\n\nIncrease iteration number of status.\n\nExamples\n\nstatus = NonlinearSolverStatus{Float64}(5)\nincrease_iteration_number!(status)\nstatus.i\n\n# output\n\n1\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{Hessian, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(hessian, x)\n\nSee e.g. initialize!(::HessianAutodiff, ::AbstractVector).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{HessianAutodiff, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(H, x)\n\nInitialize a HessianAutodiff object H.\n\nImplementation\n\nInternally this is calling the HessianAutodiff functor and therefore also ForwardDiff.hessian!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{SimpleSolvers.NewtonSolverCache, AbstractVector}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(cache, x)\n\nInitialize the NewtonSolverCache based on x.\n\nImplementation\n\nThis calls alloc_x to do all the initialization.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.initialize!-Tuple{SimpleSolvers.NonlinearSolverStatus, AbstractVector, SimpleSolvers.AbstractObjective}","page":"Home","title":"SimpleSolvers.initialize!","text":"initialize!(status, x, f)\n\nClear status and initialize it based on x and the function f.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.isaOptimizationAlgorithm-Tuple{Any}","page":"Home","title":"SimpleSolvers.isaOptimizationAlgorithm","text":"Verifies if an object implements the OptimizationAlgorithm interface.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.jacobian-Tuple{AbstractNewtonSolver}","page":"Home","title":"SimpleSolvers.jacobian","text":"jacobian(solver::AbstractNewtonSolver)\n\nCalling jacobian on an instance of AbstractNewtonSolver produces a slight ambiguity since the cache (of type NewtonSolverCache) also stores a Jacobian, but in the latter case it is a matrix not an instance of type Jacobian. Hence we return the object of type Jacobian when calling jacobian. This is also used in solver_step!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linearsolver-Tuple{AbstractNewtonSolver}","page":"Home","title":"SimpleSolvers.linearsolver","text":"linearsolver(solver)\n\nReturn the linear part (i.e. a LinearSolver) of an AbstractNewtonSolver.\n\nExamples\n\nx = rand(3)\ny = rand(3)\nF(x) = tanh.(x)\ns = NewtonSolver(x, y; F = F)\nlinearsolver(s)\n\n# output\n\nLUSolver{Float64}(3, [0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0], [1, 2, 3], [1, 2, 3], 1)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_objective-Union{Tuple{T}, Tuple{MultivariateObjective{T, Tx, TF, TG} where {Tx<:AbstractVector{T}, TF<:Union{Function, Type}, TG<:Gradient{T}}, SimpleSolvers.NewtonOptimizerCache{T, AT} where AT<:(AbstractArray{T})}} where T","page":"Home","title":"SimpleSolvers.linesearch_objective","text":"linesearch_objective(objective, cache)\n\nCreate TemporaryUnivariateObjective for linesearch algorithm. The variable on which this objective depends is alpha.\n\nExample\n\nx = [1, 0., 0.]\nf = x -> sum(x .^ 3 / 6 + x .^ 2 / 2)\nobj = MultivariateObjective(f, x)\ngradient!(obj, x)\nvalue!(obj, x)\ncache = NewtonOptimizerCache(x)\nhess = Hessian(obj, x; mode = :autodiff)\nupdate!(hess, x)\nupdate!(cache, x, obj.g, hess)\nx₂ = [.9, 0., 0.]\ngradient!(obj, x₂)\nvalue!(obj, x₂)\nupdate!(hess, x₂)\nupdate!(cache, x₂, obj.g, hess)\nls_obj = linesearch_objective(obj, cache)\nα = .1\n(ls_obj.F(α), ls_obj.D(α))\n\n# output\n\n(0.4412947468016475, -0.8083161485821551)\n\nIn the example above we have to apply update! twice on the instance of NewtonOptimizerCache because it needs to store the current and the previous iterate.\n\nImplementation\n\nCalling the function and derivative stored in the TemporaryUnivariateObjective created with linesearch_objective does not allocate a new array, but uses the one stored in the instance of NewtonOptimizerCache.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.linesearch_objective-Union{Tuple{T}, Tuple{SimpleSolvers.AbstractObjective, Jacobian, SimpleSolvers.NewtonSolverCache{T, AT, JT} where {AT<:AbstractVector{T}, JT<:AbstractMatrix{T}}}} where T","page":"Home","title":"SimpleSolvers.linesearch_objective","text":"linesearch_objective(objective!, jacobian!, cache)\n\nMake a line search objective for a Newton solver (the cache here is an instance of NewtonSolverCache).\n\nImplementation\n\ninfo: Producing a single-valued output\nDifferent from the linesearch_objective for NewtonOptimizerCaches, we apply l2norm to the output of objective!. This is because the solver operates on an objective with multiple outputs from which we have to find roots, whereas an optimizer operates on an objective with a single output of which we should find a minimum.\n\nAlso see linesearch_objective(::MultivariateObjective{T}, ::NewtonOptimizerCache{T}) where {T}.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.meets_stopping_criteria-Tuple{SimpleSolvers.OptimizerStatus, Options}","page":"Home","title":"SimpleSolvers.meets_stopping_criteria","text":"meets_stopping_criteria(status, config)\n\nCheck if the optimizer has converged.\n\nImplementation\n\nmeets_stopping_criteria first calls assess_convergence! and then checks if one of the following is true:\n\nconverged (the output of assess_convergence!) is true and status.i geq config.min_iterations,\nif config.allow_f_increases is false: status.f_increased is true,\nstatus.i geq config.max_iterations,\nstatus.rxₐ  config.x_abstol_break\nstatus.rxᵣ  config.x_reltol_break\nstatus.rfₐ  config.f_abstol_break\nstatus.rfᵣ  config.f_reltol_break\nstatus.rg   config.g_restol_break\nstatus.x_isnan\nstatus.f_isnan\nstatus.g_isnan\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.next_iteration!-Tuple{SimpleSolvers.NonlinearSolverStatus}","page":"Home","title":"SimpleSolvers.next_iteration!","text":"next_iteration!(status)\n\nCall increase_iteration_number!, set x̄ and f̄ to x and f respectively and δ as well as γ to 0.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.print_status-Tuple{SimpleSolvers.NonlinearSolverStatus, Options}","page":"Home","title":"SimpleSolvers.print_status","text":"print_status(status, config)\n\nPrint the solver staus if:\n\nThe following three are satisfied: (i) config.verbosity geq1 (ii) assess_convergence!(status, config) is false (iii) status.i > config.max_iterations\nconfig.verbosity > 1.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.residual!-Union{Tuple{GT}, Tuple{FT}, Tuple{XT}, Tuple{OS}, Tuple{OS, XT, XT, FT, FT, GT, GT}} where {OS<:SimpleSolvers.OptimizerStatus, XT, FT, GT}","page":"Home","title":"SimpleSolvers.residual!","text":"residual!(status, x, x̄, f, f̄, g, ḡ)\n\nCompute the residual based on previous iterates (x̄, f̄, ḡ) and current iterates (x, f, g).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.residual!-Union{Tuple{OR}, Tuple{OST}, Tuple{YT}, Tuple{VT}, Tuple{XT}, Tuple{OR, VT, YT, VT}} where {XT, VT<:(AbstractArray{XT}), YT, OST<:SimpleSolvers.OptimizerStatus{XT, YT}, OR<:SimpleSolvers.OptimizerResult{XT, YT, VT, OST}}","page":"Home","title":"SimpleSolvers.residual!","text":"residual!(result, x, f, g)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.rhs-Tuple{SimpleSolvers.NewtonOptimizerCache}","page":"Home","title":"SimpleSolvers.rhs","text":"rhs(cache)\n\nReturn the right hand side of an instance of NewtonOptimizerCache\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solve!-Tuple{AbstractVector, Optimizer}","page":"Home","title":"SimpleSolvers.solve!","text":"solve!(x, opt)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solver_step!-Union{Tuple{T}, Tuple{Union{AbstractVector{T}, T}, SimpleSolvers.AbstractObjective, Any, NewtonSolver{T, AT, OT, JT, TJ, TL, TLS, TST} where {AT, OT<:SimpleSolvers.AbstractObjective, JT, TJ<:Jacobian, TL, TLS<:SimpleSolvers.LinesearchState, TST<:(SimpleSolvers.NonlinearSolverStatus{T})}}} where T","page":"Home","title":"SimpleSolvers.solver_step!","text":"solver_step!(s)\n\nCompute one Newton step for f based on the Jacobian jacobian!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.solver_step!-Union{Tuple{VT}, Tuple{VT, SimpleSolvers.NewtonOptimizerState}} where VT<:(AbstractVector)","page":"Home","title":"SimpleSolvers.solver_step!","text":"solver_step!(x, newton)\n\nCompute a full iterate for an instance of NewtonOptimizerState newton.\n\nThis also performs a line search.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.triple_point_finder-Union{Tuple{Union{Function, Type}}, Tuple{T}, Tuple{Union{Function, Type}, T}} where T","page":"Home","title":"SimpleSolvers.triple_point_finder","text":"triple_point_finder(f, x)\n\nFind three points a > b > c s.t. f(a) > f(b) and f(c) > f(b). This is used for performing a quadratic line search (see QuadraticState).\n\nImplementation\n\nFor δ we take DEFAULT_BRACKETING_s as default. For nmax we take [DEFAULTBRACKETINGnmax`](@ref) as default.\n\nExtended help\n\nThe algorithm is taken from [2, Chapter 11.2.1].\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Tuple{HessianAutodiff, AbstractVector}","page":"Home","title":"SimpleSolvers.update!","text":"update!(H, x)\n\nUpdate a HessianAutodiff object H.\n\nThis is identical to initialize!.\n\nImplementation\n\nInternally this is calling the HessianAutodiff functor and therefore also ForwardDiff.hessian!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Tuple{Optimizer, AbstractVector}","page":"Home","title":"SimpleSolvers.update!","text":"compute objective and gradient at new solution and update result\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Tuple{SimpleSolvers.NewtonOptimizerCache, AbstractVector, Union{Gradient, AbstractVector}, Hessian}","page":"Home","title":"SimpleSolvers.update!","text":"update!(cache::NewtonOptimizerCache, x, g, hes)\n\nUpdate an instance of NewtonOptimizerCache based on x.\n\nThis sets:\n\nbarx^mathttcache gets x\nx^mathttcache gets x\ng^mathttcache gets g\nmathrmrhs^mathttcache gets -g\ndelta^mathttcache gets H^-1mathrmrhs^mathttcache\n\nwhere we wrote H for the Hessian (i.e. the input argument hes). \n\nAlso see update!(::NewtonSolverCache, ::AbstractVector). \n\nImplementation\n\nThe multiplication by the inverse of H is done with LinearAlgebra.ldiv!.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Tuple{SimpleSolvers.NewtonOptimizerState, AbstractVector}","page":"Home","title":"SimpleSolvers.update!","text":"update!(newton::NewtonOptimizerState, x)\n\nUpdate an instance of NewtonOptimizerState based on x.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Tuple{SimpleSolvers.NewtonSolverCache, AbstractVector}","page":"Home","title":"SimpleSolvers.update!","text":"update!(cache, x)\n\nUpdate the NewtonSolverCache based on x, i.e.:\n\ncache.x₀ gets x,\ncache.x₁ gets x,\ncache.δx gets 0.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Tuple{SimpleSolvers.NonlinearSolverStatus, AbstractVector, Union{Function, Type}}","page":"Home","title":"SimpleSolvers.update!","text":"update!(status, x, f)\n\nUpdate status based on x and the function f.\n\nThe new x and x̄ stored in status are used to compute δ. The new f and f̄ stored in status are used to compute γ.\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Tuple{SimpleSolvers.OptimizerResult, AbstractVector, Number, AbstractVector}","page":"Home","title":"SimpleSolvers.update!","text":"update!(result, x, f, g)\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.update!-Union{Tuple{HT}, Tuple{HT, AbstractVector}} where HT<:Hessian","page":"Home","title":"SimpleSolvers.update!","text":"update!(hessian, x)\n\nUpdate the Hessian based on the vector x. For an explicit example see e.g. update!(::HessianAutodiff).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.value!!-Tuple{SimpleSolvers.AbstractUnivariateObjective, Number}","page":"Home","title":"SimpleSolvers.value!!","text":"value!!(obj::AbstractObjective, x)\n\nSet obj.x_f to x and obj.f to value(obj, x) and return value(obj).\n\n\n\n\n\n","category":"method"},{"location":"#SimpleSolvers.value!-Tuple{SimpleSolvers.AbstractObjective, Union{Number, AbstractArray{<:Number}}}","page":"Home","title":"SimpleSolvers.value!","text":"value!(obj::AbstractObjective, x)\n\nCheck if x is not equal to obj.x_f and then apply value!!. Else simply return value(obj).\n\n\n\n\n\n","category":"method"},{"location":"initialize/#Initialization","page":"Initialization","title":"Initialization","text":"","category":"section"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"Before we can use SimpleSolvers.update! we have to initialize with SimpleSolvers.initialize![1].","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"[1]: The different methods for SimpleSolvers.initialize! are however often called with the constructor of a struct (e.g. for SimpleSolvers.NewtonOptimizerCache).","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"Similar to SimpleSolvers.update!, SimpleSolvers.initialize! returns the first input argument as output. Examples include:","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"SimpleSolvers.initialize!(::Hessian, ::AbstractVector): this routine exists for most Hessians, i.e. for HessianFunction, HessianAutodiff, HessianBFGS and HessianDFP,\nSimpleSolvers.initialize!(::SimpleSolvers.NewtonSolverCache, ::AbstractVector),\nSimpleSolvers.initialize!(::SimpleSolvers.NonlinearSolverStatus, ::AbstractVector, ::Base.Callable),\nSimpleSolvers.initialize!(::SimpleSolvers.NewtonOptimizerCache, ::AbstractVector).","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"We demonstrate these examples in code. First for an instance of SimpleSolvers.Hessian:","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"using SimpleSolvers # hide\nusing SimpleSolvers: initialize! # hide\nusing LinearAlgebra: norm\nimport Random # hide\nRandom.seed!(123) # hide\n\nx = rand(3)\nobj = MultivariateObjective(x -> norm(x - vcat(0., 0., 1.)) ^ 2, x)\nbt = Backtracking() # hide\nconfig = Options() # hide\nalg = Newton() # hide\n# opt = Optimizer(x, obj; algorithm = alg, linesearch = bt, config = config) # hide\n\nhes = Hessian(obj, x; mode = :autodiff)\ninitialize!(hes, x)\nhes.H","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"For an instance of SimpleSolvers.NewtonOptimizerCache[2]:","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"[2]: Here we remark that SimpleSolvers.NewtonOptimizerCache has five keys: x, x̄, δ, g and rhs. All of them are initialized with NaNs except for x. We also remark that the constructor, which is called by providing a single vector x as input argument, internally also calls SimpleSolvers.initialize!.","category":"page"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"cache = SimpleSolvers.NewtonOptimizerCache(x)\ninitialize!(cache, x)\ncache.g","category":"page"},{"location":"initialize/#Clear-Routines","page":"Initialization","title":"Clear Routines","text":"","category":"section"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"For SimpleSolvers.OptimizerResult and SimpleSolvers.OptimizerStatus the SimpleSolvers.clear! routines are used instead of the SimpleSolvers.initialize! routines.","category":"page"},{"location":"initialize/#Reasoning-behind-Initialization-with-NaNs","page":"Initialization","title":"Reasoning behind Initialization with NaNs","text":"","category":"section"},{"location":"initialize/","page":"Initialization","title":"Initialization","text":"We initialize with NaNs instead of with zeros (or other values) as this clearly divides the initialization from the numerical operations (which are done with SimpleSolvers.update!).","category":"page"},{"location":"initialize/#Alloc-Functions","page":"Initialization","title":"Alloc Functions","text":"","category":"section"}]
}
